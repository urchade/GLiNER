<!DOCTYPE html>
<html lang="en" data-accent-color="violet" data-content_root="../">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>gliner.modeling.layers module - Home 0.2.24 documentation</title><link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="gliner.modeling.loss_functions module" href="gliner.modeling.loss_functions.html" /><link rel="prev" title="gliner.modeling.encoder module" href="gliner.modeling.encoder.html" /><script>
    function setColorMode(t){let e=document.documentElement;e.setAttribute("data-color-mode",t);let a=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,s=t;"auto"===t&&(s=a?"dark":"light"),"light"===s?(e.classList.remove("dark"),e.classList.add("light")):(e.classList.remove("light"),e.classList.add("dark"))}
    setColorMode(localStorage._theme||"auto");
  </script><link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=e1a1ceaf" />
    <link rel="stylesheet" type="text/css" href="../_static/shibuya.css?v=d140fbf8" />
    <link media="print" rel="stylesheet" type="text/css" href="../_static/print.css?v=20ff2c19" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
:root {
  --sy-f-text: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
  --sy-f-heading: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
}
</style>
    <meta property="og:type" content="website"/><meta property="og:title" content="gliner.modeling.layers module"/>
<meta name="twitter:card" content="summary"/>
  </head>
<body><div class="sy-head">
  <div class="sy-head-blur"></div>
  <div class="sy-head-inner sy-container mx-auto">
    <a class="sy-head-brand" href="../index.html">
      
      
      <strong>Home</strong>
    </a>
    <div class="sy-head-nav" id="head-nav">
      <nav class="sy-head-links"></nav>
      <div class="sy-head-extra flex items-center print:hidden"><form class="searchbox flex items-center" action="../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <kbd>/</kbd>
</form><div class="sy-head-socials"></div></div>
    </div>
    <div class="sy-head-actions flex items-center shrink-0 print:hidden"><button class="js-theme theme-switch flex items-center"
data-aria-auto="Switch to light color mode"
data-aria-light="Switch to dark color mode"
data-aria-dark="Switch to auto color mode">
<i class="i-lucide theme-icon"></i>
</button><button class="md:hidden flex items-center js-menu" aria-label="Menu" type="button" aria-controls="head-nav" aria-expanded="false">
        <div class="hamburger">
          <span class="hamburger_1"></span>
          <span class="hamburger_2"></span>
          <span class="hamburger_3"></span>
        </div>
      </button>
    </div>
  </div>
</div>
<div class="sy-page sy-container flex mx-auto">
  <aside id="lside" class="sy-lside md:w-72 md:shrink-0 print:hidden">
    <div class="sy-lside-inner md:sticky">
      <div class="sy-scrollbar p-6">
        <div class="globaltoc" data-expand-depth="0"><p class="caption" role="heading" aria-level="3"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction to ðŸ‘‘ GLiNER</a></li>
<li class="toctree-l1"><a class="reference internal" href="../instalation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Advanced Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configs.html">Components &amp; Configs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architectures.html">Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../convert_to_onnx.html">ONNX Export &amp; Deployment</a></li>
</ul>
<p class="caption" role="heading" aria-level="3"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gliner.model.html">gliner.model module</a></li>
<li class="toctree-l1"><a class="reference internal" href="gliner.config.html">gliner.config module</a></li>
<li class="toctree-l1"><a class="reference internal" href="gliner.training.html">gliner.training package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gliner.training.trainer.html">gliner.training.trainer module</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="gliner.modeling.html">gliner.modeling package</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="gliner.modeling.multitask.html">gliner.modeling.multitask package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="gliner.modeling.multitask.relations_layers.html">gliner.modeling.multitask.relations_layers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="gliner.modeling.multitask.triples_layers.html">gliner.modeling.multitask.triples_layers module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="gliner.modeling.base.html">gliner.modeling.base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.modeling.decoder.html">gliner.modeling.decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.modeling.encoder.html">gliner.modeling.encoder module</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">gliner.modeling.layers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.modeling.loss_functions.html">gliner.modeling.loss_functions module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.modeling.outputs.html">gliner.modeling.outputs module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.modeling.scorers.html">gliner.modeling.scorers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.modeling.span_rep.html">gliner.modeling.span_rep module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.modeling.utils.html">gliner.modeling.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gliner.data_processing.html">gliner.data_processing package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gliner.data_processing.collator.html">gliner.data_processing.collator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.data_processing.processor.html">gliner.data_processing.processor module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.data_processing.tokenizer.html">gliner.data_processing.tokenizer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.data_processing.utils.html">gliner.data_processing.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gliner.evaluation.html">gliner.evaluation package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gliner.evaluation.evaluate_ner.html">gliner.evaluation.evaluate_ner module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.evaluation.evaluator.html">gliner.evaluation.evaluator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.evaluation.utils.html">gliner.evaluation.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gliner.onnx.html">gliner.onnx package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gliner.onnx.model.html">gliner.onnx.model module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gliner.decoding.html">gliner.decoding package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gliner.decoding.trie.html">gliner.decoding.trie package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="gliner.decoding.trie.labels_trie.html">gliner.decoding.trie.labels_trie module</a></li>
<li class="toctree-l3"><a class="reference internal" href="gliner.decoding.trie.python_labels_trie.html">gliner.decoding.trie.python_labels_trie module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="gliner.decoding.decoder.html">gliner.decoding.decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="gliner.decoding.utils.html">gliner.decoding.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gliner.utils.html">gliner.utils module</a></li>
</ul>

        </div>
      </div>
    </div>
  </aside>
  <div class="lside-overlay js-menu" role="button" aria-label="Close left sidebar" aria-controls="lside" aria-expanded="false"></div>
  <aside id="rside" class="sy-rside pb-3 w-64 shrink-0 order-last">
    <button class="rside-close js-menu xl:hidden" aria-label="Close Table of Contents" type="button" aria-controls="rside" aria-expanded="false">
      <i class="i-lucide close"></i>
    </button>
    <div class="sy-scrollbar sy-rside-inner px-6 xl:top-16 xl:sticky xl:pl-0 pt-6 pb-4"><div class="localtoc"><h3>On this page</h3><ul>
<li><a class="reference internal" href="#gliner.modeling.layers.LstmSeq2SeqEncoder"><code class="docutils literal notranslate"><span class="pre">LstmSeq2SeqEncoder</span></code></a><ul>
<li><a class="reference internal" href="#gliner.modeling.layers.LstmSeq2SeqEncoder.lstm"><code class="docutils literal notranslate">lstm</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.LstmSeq2SeqEncoder.__init__"><code class="docutils literal notranslate">__init__()</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.LstmSeq2SeqEncoder.forward"><code class="docutils literal notranslate">forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#gliner.modeling.layers.create_projection_layer"><code class="docutils literal notranslate"><span class="pre">create_projection_layer()</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.MultiheadAttention"><code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code></a><ul>
<li><a class="reference internal" href="#gliner.modeling.layers.MultiheadAttention.hidden_size"><code class="docutils literal notranslate">hidden_size</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.MultiheadAttention.num_heads"><code class="docutils literal notranslate">num_heads</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.MultiheadAttention.attention_head_size"><code class="docutils literal notranslate">attention_head_size</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.MultiheadAttention.attention_probs_dropout_prob"><code class="docutils literal notranslate">attention_probs_dropout_prob</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.MultiheadAttention.query_layer"><code class="docutils literal notranslate">query_layer</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.MultiheadAttention.key_layer"><code class="docutils literal notranslate">key_layer</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.MultiheadAttention.value_layer"><code class="docutils literal notranslate">value_layer</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.MultiheadAttention.__init__"><code class="docutils literal notranslate">__init__()</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.MultiheadAttention.transpose_for_scores"><code class="docutils literal notranslate">transpose_for_scores()</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.MultiheadAttention.forward"><code class="docutils literal notranslate">forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#gliner.modeling.layers.SelfAttentionBlock"><code class="docutils literal notranslate"><span class="pre">SelfAttentionBlock</span></code></a><ul>
<li><a class="reference internal" href="#gliner.modeling.layers.SelfAttentionBlock.self_attn"><code class="docutils literal notranslate">self_attn</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.SelfAttentionBlock.pre_norm"><code class="docutils literal notranslate">pre_norm</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.SelfAttentionBlock.post_norm"><code class="docutils literal notranslate">post_norm</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.SelfAttentionBlock.dropout"><code class="docutils literal notranslate">dropout</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.SelfAttentionBlock.q_proj"><code class="docutils literal notranslate">q_proj</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.SelfAttentionBlock.k_proj"><code class="docutils literal notranslate">k_proj</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.SelfAttentionBlock.v_proj"><code class="docutils literal notranslate">v_proj</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.SelfAttentionBlock.__init__"><code class="docutils literal notranslate">__init__()</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.SelfAttentionBlock.forward"><code class="docutils literal notranslate">forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossAttentionBlock"><code class="docutils literal notranslate"><span class="pre">CrossAttentionBlock</span></code></a><ul>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossAttentionBlock.cross_attn"><code class="docutils literal notranslate">cross_attn</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossAttentionBlock.pre_norm"><code class="docutils literal notranslate">pre_norm</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossAttentionBlock.post_norm"><code class="docutils literal notranslate">post_norm</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossAttentionBlock.dropout"><code class="docutils literal notranslate">dropout</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossAttentionBlock.v_proj"><code class="docutils literal notranslate">v_proj</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossAttentionBlock.__init__"><code class="docutils literal notranslate">__init__()</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossAttentionBlock.forward"><code class="docutils literal notranslate">forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossFuser"><code class="docutils literal notranslate"><span class="pre">CrossFuser</span></code></a><ul>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossFuser.d_model"><code class="docutils literal notranslate">d_model</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossFuser.schema"><code class="docutils literal notranslate">schema</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossFuser.layers"><code class="docutils literal notranslate">layers</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossFuser.__init__"><code class="docutils literal notranslate">__init__()</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.CrossFuser.forward"><code class="docutils literal notranslate">forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#gliner.modeling.layers.LayersFuser"><code class="docutils literal notranslate"><span class="pre">LayersFuser</span></code></a><ul>
<li><a class="reference internal" href="#gliner.modeling.layers.LayersFuser.num_layers"><code class="docutils literal notranslate">num_layers</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.LayersFuser.hidden_size"><code class="docutils literal notranslate">hidden_size</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.LayersFuser.output_size"><code class="docutils literal notranslate">output_size</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.LayersFuser.squeeze"><code class="docutils literal notranslate">squeeze</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.LayersFuser.W1"><code class="docutils literal notranslate">W1</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.LayersFuser.W2"><code class="docutils literal notranslate">W2</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.LayersFuser.output_projection"><code class="docutils literal notranslate">output_projection</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.LayersFuser.__init__"><code class="docutils literal notranslate">__init__()</span></code></a></li>
<li><a class="reference internal" href="#gliner.modeling.layers.LayersFuser.forward"><code class="docutils literal notranslate">forward()</span></code></a></li>
</ul>
</li>
</ul>
</div><div id="ethical-ad-placement" data-ea-publisher="readthedocs"></div></div>
  </aside>
  <div class="rside-overlay js-menu" role="button" aria-label="Close Table of Contents" aria-controls="rside" aria-expanded="false"></div>
  <main class="sy-main w-full max-sm:max-w-full print:pt-6">
<div class="sy-breadcrumbs" role="navigation">
  <div class="sy-breadcrumbs-inner flex items-center">
    <div class="md:hidden mr-3">
      <button class="js-menu" aria-label="Menu" type="button" aria-controls="lside" aria-expanded="false">
        <i class="i-lucide menu"></i>
      </button>
    </div>
    <ol class="flex-1" itemscope itemtype="https://schema.org/BreadcrumbList"><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../index.html"><span itemprop="name">Home</span></a>
        <span>/</span>
        <meta itemprop="position" content="1" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="gliner.modeling.html"><span itemprop="name">gliner.modeling package</span></a>
        <span>/</span>
        <meta itemprop="position" content="2" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <strong itemprop="name">gliner.modeling.layers module</strong>
        <meta itemprop="position" content="3" />
      </li></ol>
    <div class="xl:hidden ml-1">
      <button class="js-menu" aria-label="Show table of contents" type="button" aria-controls="rside"
        aria-expanded="false">
        <i class="i-lucide outdent"></i>
      </button>
    </div>
  </div>
</div><div class="flex flex-col break-words justify-between">
      <div class="min-w-0 max-w-6xl px-6 pb-6 pt-8 xl:px-12">
        <article class="yue" role="main">
          <section id="module-gliner.modeling.layers">
<span id="gliner-modeling-layers-module"></span><h1>gliner.modeling.layers module<a class="headerlink" href="#module-gliner.modeling.layers" title="Link to this heading">Â¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="gliner.modeling.layers.LstmSeq2SeqEncoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">gliner.modeling.layers.</span></span><span class="sig-name descname"><span class="pre">LstmSeq2SeqEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#LstmSeq2SeqEncoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.LstmSeq2SeqEncoder" title="Link to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Bidirectional LSTM encoder for sequence-to-sequence models.</p>
<p>This encoder processes input sequences using a bidirectional LSTM and returns
the encoded representations. It handles variable-length sequences through packing.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.LstmSeq2SeqEncoder.lstm">
<span class="sig-name descname"><span class="pre">lstm</span></span><a class="headerlink" href="#gliner.modeling.layers.LstmSeq2SeqEncoder.lstm" title="Link to this definition">Â¶</a></dt>
<dd><p>The bidirectional LSTM layer for encoding sequences.</p>
</dd></dl>

<p>Initializes the LSTM encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> â€“ Configuration object containing model hyperparameters.
Must have a <cite>hidden_size</cite> attribute.</p></li>
<li><p><strong>num_layers</strong> (<em>int</em>) â€“ Number of recurrent layers. Defaults to 1.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) â€“ Dropout probability between LSTM layers. Defaults to 0.</p></li>
<li><p><strong>bidirectional</strong> (<em>bool</em>) â€“ If True, becomes a bidirectional LSTM. Defaults to True.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.LstmSeq2SeqEncoder.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#LstmSeq2SeqEncoder.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.LstmSeq2SeqEncoder.__init__" title="Link to this definition">Â¶</a></dt>
<dd><p>Initializes the LSTM encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> â€“ Configuration object containing model hyperparameters.
Must have a <cite>hidden_size</cite> attribute.</p></li>
<li><p><strong>num_layers</strong> (<em>int</em>) â€“ Number of recurrent layers. Defaults to 1.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) â€“ Dropout probability between LSTM layers. Defaults to 0.</p></li>
<li><p><strong>bidirectional</strong> (<em>bool</em>) â€“ If True, becomes a bidirectional LSTM. Defaults to True.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.LstmSeq2SeqEncoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#LstmSeq2SeqEncoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.LstmSeq2SeqEncoder.forward" title="Link to this definition">Â¶</a></dt>
<dd><p>Encodes input sequences through the LSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) â€“ Input tensor of shape (batch_size, seq_len, hidden_size).</p></li>
<li><p><strong>mask</strong> (<em>Tensor</em>) â€“ Binary mask tensor of shape (batch_size, seq_len) where 1 indicates
valid positions and 0 indicates padding.</p></li>
<li><p><strong>hidden</strong> (<em>Tuple</em><em>[</em><em>Tensor</em><em>, </em><em>Tensor</em><em>] </em><em>| </em><em>None</em>) â€“ Optional initial hidden state tuple (h_0, c_0). Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Encoded output tensor of shape (batch_size, seq_len, hidden_size).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Tensor</em></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="gliner.modeling.layers.create_projection_layer">
<span class="sig-prename descclassname"><span class="pre">gliner.modeling.layers.</span></span><span class="sig-name descname"><span class="pre">create_projection_layer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#create_projection_layer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.create_projection_layer" title="Link to this definition">Â¶</a></dt>
<dd><p>Creates a two-layer projection network with ReLU activation and dropout.</p>
<p>The projection layer expands the input by 4x in the hidden layer before
projecting to the output dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<em>int</em>) â€“ Size of the input hidden dimension.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) â€“ Dropout probability applied after the first layer.</p></li>
<li><p><strong>out_dim</strong> (<em>int</em><em> | </em><em>None</em>) â€“ Output dimension size. If None, uses hidden_size. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A Sequential module containing the projection layers.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Sequential</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="gliner.modeling.layers.MultiheadAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">gliner.modeling.layers.</span></span><span class="sig-name descname"><span class="pre">MultiheadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#MultiheadAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.MultiheadAttention" title="Link to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Multi-head scaled dot-product attention mechanism.</p>
<p>Implements multi-head attention where the hidden dimension is split across
multiple attention heads. Uses PyTorchâ€™s scaled_dot_product_attention for
efficient computation.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.MultiheadAttention.hidden_size">
<span class="sig-name descname"><span class="pre">hidden_size</span></span><a class="headerlink" href="#gliner.modeling.layers.MultiheadAttention.hidden_size" title="Link to this definition">Â¶</a></dt>
<dd><p>Total hidden dimension size.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.MultiheadAttention.num_heads">
<span class="sig-name descname"><span class="pre">num_heads</span></span><a class="headerlink" href="#gliner.modeling.layers.MultiheadAttention.num_heads" title="Link to this definition">Â¶</a></dt>
<dd><p>Number of attention heads.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.MultiheadAttention.attention_head_size">
<span class="sig-name descname"><span class="pre">attention_head_size</span></span><a class="headerlink" href="#gliner.modeling.layers.MultiheadAttention.attention_head_size" title="Link to this definition">Â¶</a></dt>
<dd><p>Dimension of each attention head.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.MultiheadAttention.attention_probs_dropout_prob">
<span class="sig-name descname"><span class="pre">attention_probs_dropout_prob</span></span><a class="headerlink" href="#gliner.modeling.layers.MultiheadAttention.attention_probs_dropout_prob" title="Link to this definition">Â¶</a></dt>
<dd><p>Dropout probability for attention weights.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.MultiheadAttention.query_layer">
<span class="sig-name descname"><span class="pre">query_layer</span></span><a class="headerlink" href="#gliner.modeling.layers.MultiheadAttention.query_layer" title="Link to this definition">Â¶</a></dt>
<dd><p>Linear projection for query vectors.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.MultiheadAttention.key_layer">
<span class="sig-name descname"><span class="pre">key_layer</span></span><a class="headerlink" href="#gliner.modeling.layers.MultiheadAttention.key_layer" title="Link to this definition">Â¶</a></dt>
<dd><p>Linear projection for key vectors.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.MultiheadAttention.value_layer">
<span class="sig-name descname"><span class="pre">value_layer</span></span><a class="headerlink" href="#gliner.modeling.layers.MultiheadAttention.value_layer" title="Link to this definition">Â¶</a></dt>
<dd><p>Linear projection for value vectors.</p>
</dd></dl>

<p>Initializes the multi-head attention module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<em>int</em>) â€“ Size of the hidden dimension. Must be divisible by num_heads.</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) â€“ Number of attention heads.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) â€“ Dropout probability for attention weights.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.MultiheadAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#MultiheadAttention.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.MultiheadAttention.__init__" title="Link to this definition">Â¶</a></dt>
<dd><p>Initializes the multi-head attention module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<em>int</em>) â€“ Size of the hidden dimension. Must be divisible by num_heads.</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) â€“ Number of attention heads.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) â€“ Dropout probability for attention weights.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.MultiheadAttention.transpose_for_scores">
<span class="sig-name descname"><span class="pre">transpose_for_scores</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#MultiheadAttention.transpose_for_scores"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.MultiheadAttention.transpose_for_scores" title="Link to this definition">Â¶</a></dt>
<dd><p>Reshapes tensor for multi-head attention computation.</p>
<p>Transforms from (batch, seq_len, hidden) to (batch, num_heads, seq_len, head_dim).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) â€“ Input tensor of shape (batch_size, seq_len, hidden_size).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Reshaped tensor of shape (batch_size, num_heads, seq_len, attention_head_size).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Tensor</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.MultiheadAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#MultiheadAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.MultiheadAttention.forward" title="Link to this definition">Â¶</a></dt>
<dd><p>Computes multi-head attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>Tensor</em>) â€“ Query tensor of shape (batch_size, seq_len, hidden_size).</p></li>
<li><p><strong>key</strong> (<em>Tensor</em><em> | </em><em>None</em>) â€“ Optional key tensor. If None, uses query. Defaults to None.</p></li>
<li><p><strong>value</strong> (<em>Tensor</em><em> | </em><em>None</em>) â€“ Optional value tensor. If None, uses key or query. Defaults to None.</p></li>
<li><p><strong>head_mask</strong> (<em>Tensor</em><em> | </em><em>None</em>) â€“ Optional mask for attention heads. Defaults to None.</p></li>
<li><p><strong>attn_mask</strong> (<em>Tensor</em><em> | </em><em>None</em>) â€“ Optional attention mask. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>context_layer: Attention output of shape (batch_size, seq_len, hidden_size).</p></li>
<li><p>None: Placeholder for attention weights (not returned).</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>A tuple containing</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="gliner.modeling.layers.SelfAttentionBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">gliner.modeling.layers.</span></span><span class="sig-name descname"><span class="pre">SelfAttentionBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#SelfAttentionBlock"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.SelfAttentionBlock" title="Link to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Self-attention block with pre-normalization and residual connection.</p>
<p>Implements a standard transformer-style self-attention block with layer
normalization before and after the attention operation.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.SelfAttentionBlock.self_attn">
<span class="sig-name descname"><span class="pre">self_attn</span></span><a class="headerlink" href="#gliner.modeling.layers.SelfAttentionBlock.self_attn" title="Link to this definition">Â¶</a></dt>
<dd><p>Multi-head self-attention module.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.SelfAttentionBlock.pre_norm">
<span class="sig-name descname"><span class="pre">pre_norm</span></span><a class="headerlink" href="#gliner.modeling.layers.SelfAttentionBlock.pre_norm" title="Link to this definition">Â¶</a></dt>
<dd><p>Layer normalization applied before attention.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.SelfAttentionBlock.post_norm">
<span class="sig-name descname"><span class="pre">post_norm</span></span><a class="headerlink" href="#gliner.modeling.layers.SelfAttentionBlock.post_norm" title="Link to this definition">Â¶</a></dt>
<dd><p>Layer normalization applied after residual connection.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.SelfAttentionBlock.dropout">
<span class="sig-name descname"><span class="pre">dropout</span></span><a class="headerlink" href="#gliner.modeling.layers.SelfAttentionBlock.dropout" title="Link to this definition">Â¶</a></dt>
<dd><p>Dropout layer for attention output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.SelfAttentionBlock.q_proj">
<span class="sig-name descname"><span class="pre">q_proj</span></span><a class="headerlink" href="#gliner.modeling.layers.SelfAttentionBlock.q_proj" title="Link to this definition">Â¶</a></dt>
<dd><p>Linear projection for queries.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.SelfAttentionBlock.k_proj">
<span class="sig-name descname"><span class="pre">k_proj</span></span><a class="headerlink" href="#gliner.modeling.layers.SelfAttentionBlock.k_proj" title="Link to this definition">Â¶</a></dt>
<dd><p>Linear projection for keys.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.SelfAttentionBlock.v_proj">
<span class="sig-name descname"><span class="pre">v_proj</span></span><a class="headerlink" href="#gliner.modeling.layers.SelfAttentionBlock.v_proj" title="Link to this definition">Â¶</a></dt>
<dd><p>Linear projection for values.</p>
</dd></dl>

<p>Initializes the self-attention block.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) â€“ Model dimension size.</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) â€“ Number of attention heads.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) â€“ Dropout probability. Defaults to 0.1.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.SelfAttentionBlock.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#SelfAttentionBlock.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.SelfAttentionBlock.__init__" title="Link to this definition">Â¶</a></dt>
<dd><p>Initializes the self-attention block.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) â€“ Model dimension size.</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) â€“ Number of attention heads.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) â€“ Dropout probability. Defaults to 0.1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.SelfAttentionBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#SelfAttentionBlock.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.SelfAttentionBlock.forward" title="Link to this definition">Â¶</a></dt>
<dd><p>Applies self-attention to input tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) â€“ Input tensor of shape (batch_size, seq_len, d_model).</p></li>
<li><p><strong>mask</strong> (<em>Tensor</em><em> | </em><em>None</em>) â€“ Optional attention mask. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor of shape (batch_size, seq_len, d_model).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Tensor</em></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossAttentionBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">gliner.modeling.layers.</span></span><span class="sig-name descname"><span class="pre">CrossAttentionBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#CrossAttentionBlock"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.CrossAttentionBlock" title="Link to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Cross-attention block with pre-normalization and residual connection.</p>
<p>Implements cross-attention between query and key-value pairs, typically used
for attending from one sequence to another.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossAttentionBlock.cross_attn">
<span class="sig-name descname"><span class="pre">cross_attn</span></span><a class="headerlink" href="#gliner.modeling.layers.CrossAttentionBlock.cross_attn" title="Link to this definition">Â¶</a></dt>
<dd><p>Multi-head cross-attention module.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossAttentionBlock.pre_norm">
<span class="sig-name descname"><span class="pre">pre_norm</span></span><a class="headerlink" href="#gliner.modeling.layers.CrossAttentionBlock.pre_norm" title="Link to this definition">Â¶</a></dt>
<dd><p>Layer normalization applied to query before attention.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossAttentionBlock.post_norm">
<span class="sig-name descname"><span class="pre">post_norm</span></span><a class="headerlink" href="#gliner.modeling.layers.CrossAttentionBlock.post_norm" title="Link to this definition">Â¶</a></dt>
<dd><p>Layer normalization applied after residual connection.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossAttentionBlock.dropout">
<span class="sig-name descname"><span class="pre">dropout</span></span><a class="headerlink" href="#gliner.modeling.layers.CrossAttentionBlock.dropout" title="Link to this definition">Â¶</a></dt>
<dd><p>Dropout layer for attention output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossAttentionBlock.v_proj">
<span class="sig-name descname"><span class="pre">v_proj</span></span><a class="headerlink" href="#gliner.modeling.layers.CrossAttentionBlock.v_proj" title="Link to this definition">Â¶</a></dt>
<dd><p>Linear projection for values.</p>
</dd></dl>

<p>Initializes the cross-attention block.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) â€“ Model dimension size.</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) â€“ Number of attention heads.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) â€“ Dropout probability. Defaults to 0.1.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossAttentionBlock.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#CrossAttentionBlock.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.CrossAttentionBlock.__init__" title="Link to this definition">Â¶</a></dt>
<dd><p>Initializes the cross-attention block.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) â€“ Model dimension size.</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) â€“ Number of attention heads.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) â€“ Dropout probability. Defaults to 0.1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossAttentionBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#CrossAttentionBlock.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.CrossAttentionBlock.forward" title="Link to this definition">Â¶</a></dt>
<dd><p>Applies cross-attention from query to key-value pairs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>Tensor</em>) â€“ Query tensor of shape (batch_size, query_len, d_model).</p></li>
<li><p><strong>key</strong> (<em>Tensor</em>) â€“ Key tensor of shape (batch_size, key_len, d_model).</p></li>
<li><p><strong>value</strong> (<em>Tensor</em><em> | </em><em>None</em>) â€“ Optional value tensor. If None, derived from key. Defaults to None.</p></li>
<li><p><strong>mask</strong> (<em>Tensor</em><em> | </em><em>None</em>) â€“ Optional attention mask. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor of shape (batch_size, query_len, d_model).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Tensor</em></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossFuser">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">gliner.modeling.layers.</span></span><span class="sig-name descname"><span class="pre">CrossFuser</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">query_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">schema</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l2l-l2t'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#CrossFuser"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.CrossFuser" title="Link to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Flexible cross-attention fusion module with configurable attention patterns.</p>
<p>Fuses two sequences using a configurable schema of self-attention and
cross-attention operations. The schema defines the order and type of
attention operations to apply.</p>
<dl class="simple">
<dt>Schema notation:</dt><dd><ul class="simple">
<li><p>â€˜l2lâ€™: Self-attention on label sequence</p></li>
<li><p>â€˜t2tâ€™: Self-attention on text sequence</p></li>
<li><p>â€˜l2tâ€™: Cross-attention from label to text</p></li>
<li><p>â€˜t2lâ€™: Cross-attention from text to label</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossFuser.d_model">
<span class="sig-name descname"><span class="pre">d_model</span></span><a class="headerlink" href="#gliner.modeling.layers.CrossFuser.d_model" title="Link to this definition">Â¶</a></dt>
<dd><p>Model dimension size.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossFuser.schema">
<span class="sig-name descname"><span class="pre">schema</span></span><a class="headerlink" href="#gliner.modeling.layers.CrossFuser.schema" title="Link to this definition">Â¶</a></dt>
<dd><p>List of attention operation types parsed from schema string.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossFuser.layers">
<span class="sig-name descname"><span class="pre">layers</span></span><a class="headerlink" href="#gliner.modeling.layers.CrossFuser.layers" title="Link to this definition">Â¶</a></dt>
<dd><p>ModuleList of attention layers organized by depth.</p>
</dd></dl>

<p>Initializes the cross-fusion module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) â€“ Model dimension size.</p></li>
<li><p><strong>query_dim</strong> (<em>int</em>) â€“ Dimension of query input (currently unused).</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) â€“ Number of attention heads. Defaults to 8.</p></li>
<li><p><strong>num_layers</strong> (<em>int</em>) â€“ Number of attention layers. Defaults to 1.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) â€“ Dropout probability. Defaults to 0.1.</p></li>
<li><p><strong>schema</strong> (<em>str</em>) â€“ String defining attention pattern (e.g., â€˜l2l-l2t-t2tâ€™).
Defaults to â€˜l2l-l2tâ€™.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossFuser.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">query_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">schema</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l2l-l2t'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#CrossFuser.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.CrossFuser.__init__" title="Link to this definition">Â¶</a></dt>
<dd><p>Initializes the cross-fusion module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) â€“ Model dimension size.</p></li>
<li><p><strong>query_dim</strong> (<em>int</em>) â€“ Dimension of query input (currently unused).</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) â€“ Number of attention heads. Defaults to 8.</p></li>
<li><p><strong>num_layers</strong> (<em>int</em>) â€“ Number of attention layers. Defaults to 1.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) â€“ Dropout probability. Defaults to 0.1.</p></li>
<li><p><strong>schema</strong> (<em>str</em>) â€“ String defining attention pattern (e.g., â€˜l2l-l2t-t2tâ€™).
Defaults to â€˜l2l-l2tâ€™.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.CrossFuser.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">query_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#CrossFuser.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.CrossFuser.forward" title="Link to this definition">Â¶</a></dt>
<dd><p>Applies cross-fusion between query and key sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>Tensor</em>) â€“ Query tensor of shape (batch_size, query_len, d_model).</p></li>
<li><p><strong>key</strong> (<em>Tensor</em>) â€“ Key tensor of shape (batch_size, key_len, d_model).</p></li>
<li><p><strong>query_mask</strong> (<em>Tensor</em><em> | </em><em>None</em>) â€“ Optional binary mask for query (1 = valid, 0 = padding).
Shape (batch_size, query_len). Defaults to None.</p></li>
<li><p><strong>key_mask</strong> (<em>Tensor</em><em> | </em><em>None</em>) â€“ Optional binary mask for key (1 = valid, 0 = padding).
Shape (batch_size, key_len). Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>query: Fused query tensor of shape (batch_size, query_len, d_model).</p></li>
<li><p>key: Fused key tensor of shape (batch_size, key_len, d_model).</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>A tuple containing</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="gliner.modeling.layers.LayersFuser">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">gliner.modeling.layers.</span></span><span class="sig-name descname"><span class="pre">LayersFuser</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#LayersFuser"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.LayersFuser" title="Link to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Fuses multiple encoder layer outputs using squeeze-and-excitation mechanism.</p>
<p>Combines outputs from different encoder layers by learning adaptive weights
for each layer using a squeeze-and-excitation style attention mechanism.
The first layer in encoder_outputs is skipped during fusion.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.LayersFuser.num_layers">
<span class="sig-name descname"><span class="pre">num_layers</span></span><a class="headerlink" href="#gliner.modeling.layers.LayersFuser.num_layers" title="Link to this definition">Â¶</a></dt>
<dd><p>Number of encoder layers to fuse.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.LayersFuser.hidden_size">
<span class="sig-name descname"><span class="pre">hidden_size</span></span><a class="headerlink" href="#gliner.modeling.layers.LayersFuser.hidden_size" title="Link to this definition">Â¶</a></dt>
<dd><p>Hidden dimension size of encoder outputs.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.LayersFuser.output_size">
<span class="sig-name descname"><span class="pre">output_size</span></span><a class="headerlink" href="#gliner.modeling.layers.LayersFuser.output_size" title="Link to this definition">Â¶</a></dt>
<dd><p>Size of the final output projection.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.LayersFuser.squeeze">
<span class="sig-name descname"><span class="pre">squeeze</span></span><a class="headerlink" href="#gliner.modeling.layers.LayersFuser.squeeze" title="Link to this definition">Â¶</a></dt>
<dd><p>Linear layer for squeeze operation.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.LayersFuser.W1">
<span class="sig-name descname"><span class="pre">W1</span></span><a class="headerlink" href="#gliner.modeling.layers.LayersFuser.W1" title="Link to this definition">Â¶</a></dt>
<dd><p>First linear layer of excitation network.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.LayersFuser.W2">
<span class="sig-name descname"><span class="pre">W2</span></span><a class="headerlink" href="#gliner.modeling.layers.LayersFuser.W2" title="Link to this definition">Â¶</a></dt>
<dd><p>Second linear layer of excitation network.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gliner.modeling.layers.LayersFuser.output_projection">
<span class="sig-name descname"><span class="pre">output_projection</span></span><a class="headerlink" href="#gliner.modeling.layers.LayersFuser.output_projection" title="Link to this definition">Â¶</a></dt>
<dd><p>Final projection to output dimension.</p>
</dd></dl>

<p>Initializes the layer fusion module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_layers</strong> (<em>int</em>) â€“ Number of encoder layers to fuse.</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em>) â€“ Hidden dimension size.</p></li>
<li><p><strong>output_size</strong> (<em>int</em><em> | </em><em>None</em>) â€“ Output dimension size. If None, uses hidden_size.
Defaults to None.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.LayersFuser.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#LayersFuser.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.LayersFuser.__init__" title="Link to this definition">Â¶</a></dt>
<dd><p>Initializes the layer fusion module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_layers</strong> (<em>int</em>) â€“ Number of encoder layers to fuse.</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em>) â€“ Hidden dimension size.</p></li>
<li><p><strong>output_size</strong> (<em>int</em><em> | </em><em>None</em>) â€“ Output dimension size. If None, uses hidden_size.
Defaults to None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gliner.modeling.layers.LayersFuser.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder_outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/gliner/modeling/layers.html#LayersFuser.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gliner.modeling.layers.LayersFuser.forward" title="Link to this definition">Â¶</a></dt>
<dd><p>Fuses multiple encoder layer outputs into a single representation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>encoder_outputs</strong> (<em>List</em><em>[</em><em>Tensor</em><em>]</em>) â€“ List of encoder output tensors, each of shape
(batch_size, seq_len, hidden_size). The first element is skipped.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Fused output tensor of shape (batch_size, seq_len, output_size).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Tensor</em></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>

        </article><button class="back-to-top" type="button">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
  </svg>
  <span>Back to top</span>
</button><div class="navigation flex print:hidden"><div class="navigation-prev">
    <a href="gliner.modeling.encoder.html">
      <i class="i-lucide chevron-left"></i>
      <div class="page-info">
        <span>Previous</span><div class="title">gliner.modeling.encoder module</div></div>
    </a>
  </div><div class="navigation-next">
    <a href="gliner.modeling.loss_functions.html">
      <div class="page-info">
        <span>Next</span>
        <div class="title">gliner.modeling.loss_functions module</div>
      </div>
      <i class="i-lucide chevron-right"></i>
    </a>
  </div></div></div>
    </div>
  </main>
</div>
<footer class="sy-foot">
  <div class="sy-foot-inner sy-container mx-auto">
    <div class="sy-foot-reserved md:flex justify-between items-center">
      <div class="sy-foot-copyright"><p>2025, GLiNER community</p>
  
  <p>
    Made with
    
    <a href="https://www.sphinx-doc.org/">Sphinx</a> and
    
    <a href="https://shibuya.lepture.com">Shibuya theme</a>.
  </p>
</div>
      <div class="sy-foot-socials"></div>
    </div>
  </div>
</footer>
      <script src="../_static/documentation_options.js?v=dc91f075"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/shibuya.js?v=9b0e4dde"></script></body>
</html>