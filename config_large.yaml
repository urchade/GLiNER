# Learning Rate
lr_encoder: 1e-5
lr_others: 5e-5

# Training Parameters
num_steps: 30000
warmup_ratio: 0.1
train_batch_size: 8
eval_every: 5000

# Model Configuration
max_width: 12
model_name: deberta-v3-large # hugging face model
fine_tune: true
subtoken_pooling: first
hidden_size: 768
span_mode: marker
dropout: 0.4

# Directory Paths
root_dir: ablation_backbone
train_data: "train_instruct.json"

# "none" if no pretrained model 
prev_path: "none"


# Training Specifics
max_neg_type_ratio: 1

name: "large"


