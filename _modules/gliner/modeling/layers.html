<!DOCTYPE html>
<html lang="en" data-accent-color="violet" data-content_root="../../../">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>gliner.modeling.layers - Home 0.2.24 documentation</title><link rel="index" title="Index" href="../../../genindex.html" /><link rel="search" title="Search" href="../../../search.html" /><script>
    function setColorMode(t){let e=document.documentElement;e.setAttribute("data-color-mode",t);let a=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,s=t;"auto"===t&&(s=a?"dark":"light"),"light"===s?(e.classList.remove("dark"),e.classList.add("light")):(e.classList.remove("light"),e.classList.add("dark"))}
    setColorMode(localStorage._theme||"auto");
  </script><link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=e1a1ceaf" />
    <link rel="stylesheet" type="text/css" href="../../../_static/shibuya.css?v=d140fbf8" />
    <link media="print" rel="stylesheet" type="text/css" href="../../../_static/print.css?v=20ff2c19" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
:root {
  --sy-f-text: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
  --sy-f-heading: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
}
</style>
    <meta property="og:type" content="website"/><meta property="og:title" content="gliner.modeling.layers"/>
<meta name="twitter:card" content="summary"/>
  </head>
<body><div class="sy-head">
  <div class="sy-head-blur"></div>
  <div class="sy-head-inner sy-container mx-auto">
    <a class="sy-head-brand" href="../../../index.html">
      
      
      <strong>Home</strong>
    </a>
    <div class="sy-head-nav" id="head-nav">
      <nav class="sy-head-links"></nav>
      <div class="sy-head-extra flex items-center print:hidden"><form class="searchbox flex items-center" action="../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <kbd>/</kbd>
</form><div class="sy-head-socials"></div></div>
    </div>
    <div class="sy-head-actions flex items-center shrink-0 print:hidden"><button class="js-theme theme-switch flex items-center"
data-aria-auto="Switch to light color mode"
data-aria-light="Switch to dark color mode"
data-aria-dark="Switch to auto color mode">
<i class="i-lucide theme-icon"></i>
</button><button class="md:hidden flex items-center js-menu" aria-label="Menu" type="button" aria-controls="head-nav" aria-expanded="false">
        <div class="hamburger">
          <span class="hamburger_1"></span>
          <span class="hamburger_2"></span>
          <span class="hamburger_3"></span>
        </div>
      </button>
    </div>
  </div>
</div>
<div class="sy-page sy-container flex mx-auto">
  <aside id="lside" class="sy-lside md:w-72 md:shrink-0 print:hidden">
    <div class="sy-lside-inner md:sticky">
      <div class="sy-scrollbar p-6">
        <div class="globaltoc" data-expand-depth="0"><p class="caption" role="heading" aria-level="3"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../intro.html">Introduction to ðŸ‘‘ GLiNER</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../instalation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../usage.html">Advanced Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../configs.html">Components &amp; Configs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../training.html">Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architectures.html">Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../convert_to_onnx.html">ONNX Export &amp; Deployment</a></li>
</ul>
<p class="caption" role="heading" aria-level="3"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.model.html">gliner.model module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.config.html">gliner.config module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.training.html">gliner.training package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.training.trainer.html">gliner.training.trainer module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.modeling.html">gliner.modeling package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.multitask.html">gliner.modeling.multitask package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.modeling.multitask.relations_layers.html">gliner.modeling.multitask.relations_layers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.modeling.multitask.triples_layers.html">gliner.modeling.multitask.triples_layers module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.base.html">gliner.modeling.base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.decoder.html">gliner.modeling.decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.encoder.html">gliner.modeling.encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.layers.html">gliner.modeling.layers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.loss_functions.html">gliner.modeling.loss_functions module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.outputs.html">gliner.modeling.outputs module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.scorers.html">gliner.modeling.scorers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.span_rep.html">gliner.modeling.span_rep module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.utils.html">gliner.modeling.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.data_processing.html">gliner.data_processing package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.collator.html">gliner.data_processing.collator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.processor.html">gliner.data_processing.processor module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.tokenizer.html">gliner.data_processing.tokenizer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.utils.html">gliner.data_processing.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.evaluation.html">gliner.evaluation package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.evaluate_ner.html">gliner.evaluation.evaluate_ner module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.evaluator.html">gliner.evaluation.evaluator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.utils.html">gliner.evaluation.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.onnx.html">gliner.onnx package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.onnx.model.html">gliner.onnx.model module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.decoding.html">gliner.decoding package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.trie.html">gliner.decoding.trie package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.decoding.trie.labels_trie.html">gliner.decoding.trie.labels_trie module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.decoding.trie.python_labels_trie.html">gliner.decoding.trie.python_labels_trie module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.decoder.html">gliner.decoding.decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.utils.html">gliner.decoding.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.utils.html">gliner.utils module</a></li>
</ul>

        </div>
      </div>
    </div>
  </aside>
  <div class="lside-overlay js-menu" role="button" aria-label="Close left sidebar" aria-controls="lside" aria-expanded="false"></div>
  <aside id="rside" class="sy-rside pb-3 w-64 shrink-0 order-last">
    <button class="rside-close js-menu xl:hidden" aria-label="Close Table of Contents" type="button" aria-controls="rside" aria-expanded="false">
      <i class="i-lucide close"></i>
    </button>
    <div class="sy-scrollbar sy-rside-inner px-6 xl:top-16 xl:sticky xl:pl-0 pt-6 pb-4"><div id="ethical-ad-placement" data-ea-publisher="readthedocs"></div></div>
  </aside>
  <div class="rside-overlay js-menu" role="button" aria-label="Close Table of Contents" aria-controls="rside" aria-expanded="false"></div>
  <main class="sy-main w-full max-sm:max-w-full print:pt-6">
<div class="sy-breadcrumbs" role="navigation">
  <div class="sy-breadcrumbs-inner flex items-center">
    <div class="md:hidden mr-3">
      <button class="js-menu" aria-label="Menu" type="button" aria-controls="lside" aria-expanded="false">
        <i class="i-lucide menu"></i>
      </button>
    </div>
    <ol class="flex-1" itemscope itemtype="https://schema.org/BreadcrumbList"><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../../../index.html"><span itemprop="name">Home</span></a>
        <span>/</span>
        <meta itemprop="position" content="1" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../../index.html"><span itemprop="name">Module code</span></a>
        <span>/</span>
        <meta itemprop="position" content="2" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <strong itemprop="name">gliner.modeling.layers</strong>
        <meta itemprop="position" content="3" />
      </li></ol>
    <div class="xl:hidden ml-1">
      <button class="js-menu" aria-label="Show table of contents" type="button" aria-controls="rside"
        aria-expanded="false">
        <i class="i-lucide outdent"></i>
      </button>
    </div>
  </div>
</div><div class="flex flex-col break-words justify-between">
      <div class="min-w-0 max-w-6xl px-6 pb-6 pt-8 xl:px-12">
        <article class="yue" role="main">
          <h1>Source code for gliner.modeling.layers</h1><div class="highlight"><pre>
<span></span><span data-line="1"><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Optional</span>
</span><span data-line="2">
</span><span data-line="3"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span data-line="4"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
</span><span data-line="5"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span data-line="6"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils.rnn</span><span class="w"> </span><span class="kn">import</span> <span class="n">pad_packed_sequence</span><span class="p">,</span> <span class="n">pack_padded_sequence</span>
</span><span data-line="7">
</span><span data-line="8">
<div class="viewcode-block" id="LstmSeq2SeqEncoder">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.LstmSeq2SeqEncoder">[docs]</a>
</span><span data-line="9"><span class="k">class</span><span class="w"> </span><span class="nc">LstmSeq2SeqEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span data-line="10"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Bidirectional LSTM encoder for sequence-to-sequence models.</span>
</span><span data-line="11">
</span><span data-line="12"><span class="sd">    This encoder processes input sequences using a bidirectional LSTM and returns</span>
</span><span data-line="13"><span class="sd">    the encoded representations. It handles variable-length sequences through packing.</span>
</span><span data-line="14">
</span><span data-line="15"><span class="sd">    Attributes:</span>
</span><span data-line="16"><span class="sd">        lstm: The bidirectional LSTM layer for encoding sequences.</span>
</span><span data-line="17"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="18">
<div class="viewcode-block" id="LstmSeq2SeqEncoder.__init__">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.LstmSeq2SeqEncoder.__init__">[docs]</a>
</span><span data-line="19">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="20"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the LSTM encoder.</span>
</span><span data-line="21">
</span><span data-line="22"><span class="sd">        Args:</span>
</span><span data-line="23"><span class="sd">            config: Configuration object containing model hyperparameters.</span>
</span><span data-line="24"><span class="sd">                Must have a `hidden_size` attribute.</span>
</span><span data-line="25"><span class="sd">            num_layers: Number of recurrent layers. Defaults to 1.</span>
</span><span data-line="26"><span class="sd">            dropout: Dropout probability between LSTM layers. Defaults to 0.</span>
</span><span data-line="27"><span class="sd">            bidirectional: If True, becomes a bidirectional LSTM. Defaults to True.</span>
</span><span data-line="28"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="29">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span data-line="30">        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span>
</span><span data-line="31">            <span class="n">input_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span data-line="32">            <span class="n">hidden_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
</span><span data-line="33">            <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
</span><span data-line="34">            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
</span><span data-line="35">            <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
</span><span data-line="36">            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span data-line="37">        <span class="p">)</span></div>

</span><span data-line="38">
<div class="viewcode-block" id="LstmSeq2SeqEncoder.forward">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.LstmSeq2SeqEncoder.forward">[docs]</a>
</span><span data-line="39">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span data-line="40">        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">hidden</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
</span><span data-line="41">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span data-line="42"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Encodes input sequences through the LSTM.</span>
</span><span data-line="43">
</span><span data-line="44"><span class="sd">        Args:</span>
</span><span data-line="45"><span class="sd">            x: Input tensor of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="46"><span class="sd">            mask: Binary mask tensor of shape (batch_size, seq_len) where 1 indicates</span>
</span><span data-line="47"><span class="sd">                valid positions and 0 indicates padding.</span>
</span><span data-line="48"><span class="sd">            hidden: Optional initial hidden state tuple (h_0, c_0). Defaults to None.</span>
</span><span data-line="49">
</span><span data-line="50"><span class="sd">        Returns:</span>
</span><span data-line="51"><span class="sd">            Encoded output tensor of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="52"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="53">        <span class="c1"># Packing the input sequence</span>
</span><span data-line="54">        <span class="n">lengths</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span><span data-line="55">        <span class="n">packed_x</span> <span class="o">=</span> <span class="n">pack_padded_sequence</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">enforce_sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span data-line="56">
</span><span data-line="57">        <span class="c1"># Passing packed sequence through LSTM</span>
</span><span data-line="58">        <span class="n">packed_output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">packed_x</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
</span><span data-line="59">
</span><span data-line="60">        <span class="c1"># Unpacking the output sequence</span>
</span><span data-line="61">        <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pad_packed_sequence</span><span class="p">(</span><span class="n">packed_output</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span data-line="62">
</span><span data-line="63">        <span class="k">return</span> <span class="n">output</span></div>
</div>

</span><span data-line="64">
</span><span data-line="65">
<div class="viewcode-block" id="create_projection_layer">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.create_projection_layer">[docs]</a>
</span><span data-line="66"><span class="k">def</span><span class="w"> </span><span class="nf">create_projection_layer</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
</span><span data-line="67"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a two-layer projection network with ReLU activation and dropout.</span>
</span><span data-line="68">
</span><span data-line="69"><span class="sd">    The projection layer expands the input by 4x in the hidden layer before</span>
</span><span data-line="70"><span class="sd">    projecting to the output dimension.</span>
</span><span data-line="71">
</span><span data-line="72"><span class="sd">    Args:</span>
</span><span data-line="73"><span class="sd">        hidden_size: Size of the input hidden dimension.</span>
</span><span data-line="74"><span class="sd">        dropout: Dropout probability applied after the first layer.</span>
</span><span data-line="75"><span class="sd">        out_dim: Output dimension size. If None, uses hidden_size. Defaults to None.</span>
</span><span data-line="76">
</span><span data-line="77"><span class="sd">    Returns:</span>
</span><span data-line="78"><span class="sd">        A Sequential module containing the projection layers.</span>
</span><span data-line="79"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="80">    <span class="k">if</span> <span class="n">out_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="81">        <span class="n">out_dim</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span data-line="82">
</span><span data-line="83">    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span data-line="84">        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">out_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>
</span><span data-line="85">    <span class="p">)</span></div>

</span><span data-line="86">
</span><span data-line="87">
<div class="viewcode-block" id="MultiheadAttention">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.MultiheadAttention">[docs]</a>
</span><span data-line="88"><span class="k">class</span><span class="w"> </span><span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span data-line="89"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Multi-head scaled dot-product attention mechanism.</span>
</span><span data-line="90">
</span><span data-line="91"><span class="sd">    Implements multi-head attention where the hidden dimension is split across</span>
</span><span data-line="92"><span class="sd">    multiple attention heads. Uses PyTorch&#39;s scaled_dot_product_attention for</span>
</span><span data-line="93"><span class="sd">    efficient computation.</span>
</span><span data-line="94">
</span><span data-line="95"><span class="sd">    Attributes:</span>
</span><span data-line="96"><span class="sd">        hidden_size: Total hidden dimension size.</span>
</span><span data-line="97"><span class="sd">        num_heads: Number of attention heads.</span>
</span><span data-line="98"><span class="sd">        attention_head_size: Dimension of each attention head.</span>
</span><span data-line="99"><span class="sd">        attention_probs_dropout_prob: Dropout probability for attention weights.</span>
</span><span data-line="100"><span class="sd">        query_layer: Linear projection for query vectors.</span>
</span><span data-line="101"><span class="sd">        key_layer: Linear projection for key vectors.</span>
</span><span data-line="102"><span class="sd">        value_layer: Linear projection for value vectors.</span>
</span><span data-line="103"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="104">
<div class="viewcode-block" id="MultiheadAttention.__init__">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.MultiheadAttention.__init__">[docs]</a>
</span><span data-line="105">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="106"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the multi-head attention module.</span>
</span><span data-line="107">
</span><span data-line="108"><span class="sd">        Args:</span>
</span><span data-line="109"><span class="sd">            hidden_size: Size of the hidden dimension. Must be divisible by num_heads.</span>
</span><span data-line="110"><span class="sd">            num_heads: Number of attention heads.</span>
</span><span data-line="111"><span class="sd">            dropout: Dropout probability for attention weights.</span>
</span><span data-line="112"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="113">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span data-line="114">        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span data-line="115">        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span><span data-line="116">        <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="n">num_heads</span>
</span><span data-line="117">        <span class="bp">self</span><span class="o">.</span><span class="n">attention_probs_dropout_prob</span> <span class="o">=</span> <span class="n">dropout</span>
</span><span data-line="118">        <span class="bp">self</span><span class="o">.</span><span class="n">query_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span><span data-line="119">        <span class="bp">self</span><span class="o">.</span><span class="n">key_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span><span data-line="120">        <span class="bp">self</span><span class="o">.</span><span class="n">value_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span></div>

</span><span data-line="121">
<div class="viewcode-block" id="MultiheadAttention.transpose_for_scores">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.MultiheadAttention.transpose_for_scores">[docs]</a>
</span><span data-line="122">    <span class="k">def</span><span class="w"> </span><span class="nf">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span data-line="123"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reshapes tensor for multi-head attention computation.</span>
</span><span data-line="124">
</span><span data-line="125"><span class="sd">        Transforms from (batch, seq_len, hidden) to (batch, num_heads, seq_len, head_dim).</span>
</span><span data-line="126">
</span><span data-line="127"><span class="sd">        Args:</span>
</span><span data-line="128"><span class="sd">            x: Input tensor of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="129">
</span><span data-line="130"><span class="sd">        Returns:</span>
</span><span data-line="131"><span class="sd">            Reshaped tensor of shape (batch_size, num_heads, seq_len, attention_head_size).</span>
</span><span data-line="132"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="133">        <span class="n">new_x_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">)</span>
</span><span data-line="134">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">new_x_shape</span><span class="p">)</span>
</span><span data-line="135">        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span></div>

</span><span data-line="136">
<div class="viewcode-block" id="MultiheadAttention.forward">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.MultiheadAttention.forward">[docs]</a>
</span><span data-line="137">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span data-line="138">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="139">        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="140">        <span class="n">key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="141">        <span class="n">value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="142">        <span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="143">        <span class="n">attn_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="144">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
</span><span data-line="145"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes multi-head attention.</span>
</span><span data-line="146">
</span><span data-line="147"><span class="sd">        Args:</span>
</span><span data-line="148"><span class="sd">            query: Query tensor of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="149"><span class="sd">            key: Optional key tensor. If None, uses query. Defaults to None.</span>
</span><span data-line="150"><span class="sd">            value: Optional value tensor. If None, uses key or query. Defaults to None.</span>
</span><span data-line="151"><span class="sd">            head_mask: Optional mask for attention heads. Defaults to None.</span>
</span><span data-line="152"><span class="sd">            attn_mask: Optional attention mask. Defaults to None.</span>
</span><span data-line="153">
</span><span data-line="154"><span class="sd">        Returns:</span>
</span><span data-line="155"><span class="sd">            A tuple containing:</span>
</span><span data-line="156"><span class="sd">                - context_layer: Attention output of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="157"><span class="sd">                - None: Placeholder for attention weights (not returned).</span>
</span><span data-line="158"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="159">        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">query_layer</span><span class="p">(</span><span class="n">query</span><span class="p">))</span>
</span><span data-line="160">        <span class="k">if</span> <span class="n">key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="161">            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_layer</span><span class="p">(</span><span class="n">query</span><span class="p">))</span>
</span><span data-line="162">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="163">            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_layer</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
</span><span data-line="164">        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="165">            <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_layer</span><span class="p">(</span><span class="n">query</span><span class="p">))</span>
</span><span data-line="166">        <span class="k">elif</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="167">            <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_layer</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
</span><span data-line="168">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="169">            <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_layer</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
</span><span data-line="170">
</span><span data-line="171">        <span class="n">context_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
</span><span data-line="172">            <span class="n">query</span><span class="p">,</span>
</span><span data-line="173">            <span class="n">key</span><span class="p">,</span>
</span><span data-line="174">            <span class="n">value</span><span class="p">,</span>
</span><span data-line="175">            <span class="n">head_mask</span><span class="p">,</span>
</span><span data-line="176">            <span class="bp">self</span><span class="o">.</span><span class="n">attention_probs_dropout_prob</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span data-line="177">            <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span data-line="178">            <span class="n">scale</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span data-line="179">        <span class="p">)</span>
</span><span data-line="180">
</span><span data-line="181">        <span class="n">context_layer</span> <span class="o">=</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span><span data-line="182">        <span class="n">new_context_layer_shape</span> <span class="o">=</span> <span class="p">(</span>
</span><span data-line="183">            <span class="o">*</span><span class="n">context_layer</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
</span><span data-line="184">            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span data-line="185">        <span class="p">)</span>
</span><span data-line="186">        <span class="n">context_layer</span> <span class="o">=</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">new_context_layer_shape</span><span class="p">)</span>
</span><span data-line="187">
</span><span data-line="188">        <span class="k">return</span> <span class="n">context_layer</span><span class="p">,</span> <span class="kc">None</span></div>
</div>

</span><span data-line="189">
</span><span data-line="190">
<div class="viewcode-block" id="SelfAttentionBlock">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.SelfAttentionBlock">[docs]</a>
</span><span data-line="191"><span class="k">class</span><span class="w"> </span><span class="nc">SelfAttentionBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span data-line="192"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Self-attention block with pre-normalization and residual connection.</span>
</span><span data-line="193">
</span><span data-line="194"><span class="sd">    Implements a standard transformer-style self-attention block with layer</span>
</span><span data-line="195"><span class="sd">    normalization before and after the attention operation.</span>
</span><span data-line="196">
</span><span data-line="197"><span class="sd">    Attributes:</span>
</span><span data-line="198"><span class="sd">        self_attn: Multi-head self-attention module.</span>
</span><span data-line="199"><span class="sd">        pre_norm: Layer normalization applied before attention.</span>
</span><span data-line="200"><span class="sd">        post_norm: Layer normalization applied after residual connection.</span>
</span><span data-line="201"><span class="sd">        dropout: Dropout layer for attention output.</span>
</span><span data-line="202"><span class="sd">        q_proj: Linear projection for queries.</span>
</span><span data-line="203"><span class="sd">        k_proj: Linear projection for keys.</span>
</span><span data-line="204"><span class="sd">        v_proj: Linear projection for values.</span>
</span><span data-line="205"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="206">
<div class="viewcode-block" id="SelfAttentionBlock.__init__">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.SelfAttentionBlock.__init__">[docs]</a>
</span><span data-line="207">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="208"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the self-attention block.</span>
</span><span data-line="209">
</span><span data-line="210"><span class="sd">        Args:</span>
</span><span data-line="211"><span class="sd">            d_model: Model dimension size.</span>
</span><span data-line="212"><span class="sd">            num_heads: Number of attention heads.</span>
</span><span data-line="213"><span class="sd">            dropout: Dropout probability. Defaults to 0.1.</span>
</span><span data-line="214"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="215">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span data-line="216">        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
</span><span data-line="217">        <span class="bp">self</span><span class="o">.</span><span class="n">pre_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
</span><span data-line="218">        <span class="bp">self</span><span class="o">.</span><span class="n">post_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
</span><span data-line="219">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span><span data-line="220">        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span><span data-line="221">        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span><span data-line="222">        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span></div>

</span><span data-line="223">
<div class="viewcode-block" id="SelfAttentionBlock.forward">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.SelfAttentionBlock.forward">[docs]</a>
</span><span data-line="224">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span data-line="225"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Applies self-attention to input tensor.</span>
</span><span data-line="226">
</span><span data-line="227"><span class="sd">        Args:</span>
</span><span data-line="228"><span class="sd">            x: Input tensor of shape (batch_size, seq_len, d_model).</span>
</span><span data-line="229"><span class="sd">            mask: Optional attention mask. Defaults to None.</span>
</span><span data-line="230">
</span><span data-line="231"><span class="sd">        Returns:</span>
</span><span data-line="232"><span class="sd">            Output tensor of shape (batch_size, seq_len, d_model).</span>
</span><span data-line="233"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="234">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span data-line="235">        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span data-line="236">        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span data-line="237">        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span data-line="238">        <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</span><span data-line="239">        <span class="n">output</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
</span><span data-line="240">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_norm</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></div>
</div>

</span><span data-line="241">
</span><span data-line="242">
<div class="viewcode-block" id="CrossAttentionBlock">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.CrossAttentionBlock">[docs]</a>
</span><span data-line="243"><span class="k">class</span><span class="w"> </span><span class="nc">CrossAttentionBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span data-line="244"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Cross-attention block with pre-normalization and residual connection.</span>
</span><span data-line="245">
</span><span data-line="246"><span class="sd">    Implements cross-attention between query and key-value pairs, typically used</span>
</span><span data-line="247"><span class="sd">    for attending from one sequence to another.</span>
</span><span data-line="248">
</span><span data-line="249"><span class="sd">    Attributes:</span>
</span><span data-line="250"><span class="sd">        cross_attn: Multi-head cross-attention module.</span>
</span><span data-line="251"><span class="sd">        pre_norm: Layer normalization applied to query before attention.</span>
</span><span data-line="252"><span class="sd">        post_norm: Layer normalization applied after residual connection.</span>
</span><span data-line="253"><span class="sd">        dropout: Dropout layer for attention output.</span>
</span><span data-line="254"><span class="sd">        v_proj: Linear projection for values.</span>
</span><span data-line="255"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="256">
<div class="viewcode-block" id="CrossAttentionBlock.__init__">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.CrossAttentionBlock.__init__">[docs]</a>
</span><span data-line="257">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="258"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the cross-attention block.</span>
</span><span data-line="259">
</span><span data-line="260"><span class="sd">        Args:</span>
</span><span data-line="261"><span class="sd">            d_model: Model dimension size.</span>
</span><span data-line="262"><span class="sd">            num_heads: Number of attention heads.</span>
</span><span data-line="263"><span class="sd">            dropout: Dropout probability. Defaults to 0.1.</span>
</span><span data-line="264"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="265">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span data-line="266">        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
</span><span data-line="267">        <span class="bp">self</span><span class="o">.</span><span class="n">pre_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
</span><span data-line="268">        <span class="bp">self</span><span class="o">.</span><span class="n">post_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
</span><span data-line="269">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span><span data-line="270">        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span></div>

</span><span data-line="271">
<div class="viewcode-block" id="CrossAttentionBlock.forward">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.CrossAttentionBlock.forward">[docs]</a>
</span><span data-line="272">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span data-line="273">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="274">        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="275">        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="276">        <span class="n">value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="277">        <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="278">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span data-line="279"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Applies cross-attention from query to key-value pairs.</span>
</span><span data-line="280">
</span><span data-line="281"><span class="sd">        Args:</span>
</span><span data-line="282"><span class="sd">            query: Query tensor of shape (batch_size, query_len, d_model).</span>
</span><span data-line="283"><span class="sd">            key: Key tensor of shape (batch_size, key_len, d_model).</span>
</span><span data-line="284"><span class="sd">            value: Optional value tensor. If None, derived from key. Defaults to None.</span>
</span><span data-line="285"><span class="sd">            mask: Optional attention mask. Defaults to None.</span>
</span><span data-line="286">
</span><span data-line="287"><span class="sd">        Returns:</span>
</span><span data-line="288"><span class="sd">            Output tensor of shape (batch_size, query_len, d_model).</span>
</span><span data-line="289"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="290">        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_norm</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</span><span data-line="291">        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="292">            <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
</span><span data-line="293">        <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</span><span data-line="294">        <span class="n">output</span> <span class="o">=</span> <span class="n">query</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
</span><span data-line="295">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_norm</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></div>
</div>

</span><span data-line="296">
</span><span data-line="297">
<div class="viewcode-block" id="CrossFuser">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.CrossFuser">[docs]</a>
</span><span data-line="298"><span class="k">class</span><span class="w"> </span><span class="nc">CrossFuser</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span data-line="299"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Flexible cross-attention fusion module with configurable attention patterns.</span>
</span><span data-line="300">
</span><span data-line="301"><span class="sd">    Fuses two sequences using a configurable schema of self-attention and</span>
</span><span data-line="302"><span class="sd">    cross-attention operations. The schema defines the order and type of</span>
</span><span data-line="303"><span class="sd">    attention operations to apply.</span>
</span><span data-line="304">
</span><span data-line="305"><span class="sd">    Schema notation:</span>
</span><span data-line="306"><span class="sd">        - &#39;l2l&#39;: Self-attention on label sequence</span>
</span><span data-line="307"><span class="sd">        - &#39;t2t&#39;: Self-attention on text sequence</span>
</span><span data-line="308"><span class="sd">        - &#39;l2t&#39;: Cross-attention from label to text</span>
</span><span data-line="309"><span class="sd">        - &#39;t2l&#39;: Cross-attention from text to label</span>
</span><span data-line="310">
</span><span data-line="311"><span class="sd">    Attributes:</span>
</span><span data-line="312"><span class="sd">        d_model: Model dimension size.</span>
</span><span data-line="313"><span class="sd">        schema: List of attention operation types parsed from schema string.</span>
</span><span data-line="314"><span class="sd">        layers: ModuleList of attention layers organized by depth.</span>
</span><span data-line="315"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="316">
<div class="viewcode-block" id="CrossFuser.__init__">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.CrossFuser.__init__">[docs]</a>
</span><span data-line="317">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span data-line="318">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="319">        <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span data-line="320">        <span class="n">query_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span data-line="321">        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
</span><span data-line="322">        <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span data-line="323">        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span data-line="324">        <span class="n">schema</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;l2l-l2t&quot;</span><span class="p">,</span>
</span><span data-line="325">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="326"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the cross-fusion module.</span>
</span><span data-line="327">
</span><span data-line="328"><span class="sd">        Args:</span>
</span><span data-line="329"><span class="sd">            d_model: Model dimension size.</span>
</span><span data-line="330"><span class="sd">            query_dim: Dimension of query input (currently unused).</span>
</span><span data-line="331"><span class="sd">            num_heads: Number of attention heads. Defaults to 8.</span>
</span><span data-line="332"><span class="sd">            num_layers: Number of attention layers. Defaults to 1.</span>
</span><span data-line="333"><span class="sd">            dropout: Dropout probability. Defaults to 0.1.</span>
</span><span data-line="334"><span class="sd">            schema: String defining attention pattern (e.g., &#39;l2l-l2t-t2t&#39;).</span>
</span><span data-line="335"><span class="sd">                Defaults to &#39;l2l-l2t&#39;.</span>
</span><span data-line="336"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="337">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span data-line="338">        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</span><span data-line="339">        <span class="bp">self</span><span class="o">.</span><span class="n">schema</span> <span class="o">=</span> <span class="n">schema</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
</span><span data-line="340">        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
</span><span data-line="341">        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
</span><span data-line="342">            <span class="n">layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span data-line="343">            <span class="k">for</span> <span class="n">attn_type</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span>
</span><span data-line="344">                <span class="k">if</span> <span class="n">attn_type</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;l2l&quot;</span><span class="p">,</span> <span class="s2">&quot;t2t&quot;</span><span class="p">}:</span>
</span><span data-line="345">                    <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SelfAttentionBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">))</span>
</span><span data-line="346">                <span class="k">else</span><span class="p">:</span>
</span><span data-line="347">                    <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">CrossAttentionBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">))</span>
</span><span data-line="348">            <span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
</span><span data-line="349">            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
</span><span data-line="350">
</span><span data-line="351">        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span></div>

</span><span data-line="352">
<div class="viewcode-block" id="CrossFuser.forward">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.CrossFuser.forward">[docs]</a>
</span><span data-line="353">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span data-line="354">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="355">        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="356">        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="357">        <span class="n">query_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="358">        <span class="n">key_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="359">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span data-line="360"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Applies cross-fusion between query and key sequences.</span>
</span><span data-line="361">
</span><span data-line="362"><span class="sd">        Args:</span>
</span><span data-line="363"><span class="sd">            query: Query tensor of shape (batch_size, query_len, d_model).</span>
</span><span data-line="364"><span class="sd">            key: Key tensor of shape (batch_size, key_len, d_model).</span>
</span><span data-line="365"><span class="sd">            query_mask: Optional binary mask for query (1 = valid, 0 = padding).</span>
</span><span data-line="366"><span class="sd">                Shape (batch_size, query_len). Defaults to None.</span>
</span><span data-line="367"><span class="sd">            key_mask: Optional binary mask for key (1 = valid, 0 = padding).</span>
</span><span data-line="368"><span class="sd">                Shape (batch_size, key_len). Defaults to None.</span>
</span><span data-line="369">
</span><span data-line="370"><span class="sd">        Returns:</span>
</span><span data-line="371"><span class="sd">            A tuple containing:</span>
</span><span data-line="372"><span class="sd">                - query: Fused query tensor of shape (batch_size, query_len, d_model).</span>
</span><span data-line="373"><span class="sd">                - key: Fused key tensor of shape (batch_size, key_len, d_model).</span>
</span><span data-line="374"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="375">        <span class="k">for</span> <span class="n">sublayers</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
</span><span data-line="376">            <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sublayers</span><span class="p">):</span>
</span><span data-line="377">                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;l2l&quot;</span><span class="p">:</span>
</span><span data-line="378">                    <span class="k">if</span> <span class="n">query_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="379">                        <span class="n">self_attn_mask</span> <span class="o">=</span> <span class="n">query_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">query_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span><span data-line="380">                    <span class="k">else</span><span class="p">:</span>
</span><span data-line="381">                        <span class="n">self_attn_mask</span> <span class="o">=</span> <span class="kc">None</span>
</span><span data-line="382">                    <span class="n">query</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">self_attn_mask</span><span class="p">)</span>
</span><span data-line="383">                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;t2t&quot;</span><span class="p">:</span>
</span><span data-line="384">                    <span class="k">if</span> <span class="n">key_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="385">                        <span class="n">self_attn_mask</span> <span class="o">=</span> <span class="n">key_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">key_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span><span data-line="386">                    <span class="k">else</span><span class="p">:</span>
</span><span data-line="387">                        <span class="n">self_attn_mask</span> <span class="o">=</span> <span class="kc">None</span>
</span><span data-line="388">                    <span class="n">key</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">self_attn_mask</span><span class="p">)</span>
</span><span data-line="389">                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;l2t&quot;</span><span class="p">:</span>
</span><span data-line="390">                    <span class="k">if</span> <span class="n">query_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">key_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="391">                        <span class="n">cross_attn_mask</span> <span class="o">=</span> <span class="n">query_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">key_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span data-line="392">                    <span class="k">else</span><span class="p">:</span>
</span><span data-line="393">                        <span class="n">cross_attn_mask</span> <span class="o">=</span> <span class="kc">None</span>
</span><span data-line="394">                    <span class="n">query</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">cross_attn_mask</span><span class="p">)</span>
</span><span data-line="395">                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;t2l&quot;</span><span class="p">:</span>
</span><span data-line="396">                    <span class="k">if</span> <span class="n">query_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">key_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="397">                        <span class="n">cross_attn_mask</span> <span class="o">=</span> <span class="n">key_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">query_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span data-line="398">                    <span class="k">else</span><span class="p">:</span>
</span><span data-line="399">                        <span class="n">cross_attn_mask</span> <span class="o">=</span> <span class="kc">None</span>
</span><span data-line="400">                    <span class="n">key</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">cross_attn_mask</span><span class="p">)</span>
</span><span data-line="401">
</span><span data-line="402">        <span class="k">return</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span></div>
</div>

</span><span data-line="403">
</span><span data-line="404">
<div class="viewcode-block" id="LayersFuser">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.LayersFuser">[docs]</a>
</span><span data-line="405"><span class="k">class</span><span class="w"> </span><span class="nc">LayersFuser</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span data-line="406"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Fuses multiple encoder layer outputs using squeeze-and-excitation mechanism.</span>
</span><span data-line="407">
</span><span data-line="408"><span class="sd">    Combines outputs from different encoder layers by learning adaptive weights</span>
</span><span data-line="409"><span class="sd">    for each layer using a squeeze-and-excitation style attention mechanism.</span>
</span><span data-line="410"><span class="sd">    The first layer in encoder_outputs is skipped during fusion.</span>
</span><span data-line="411">
</span><span data-line="412"><span class="sd">    Attributes:</span>
</span><span data-line="413"><span class="sd">        num_layers: Number of encoder layers to fuse.</span>
</span><span data-line="414"><span class="sd">        hidden_size: Hidden dimension size of encoder outputs.</span>
</span><span data-line="415"><span class="sd">        output_size: Size of the final output projection.</span>
</span><span data-line="416"><span class="sd">        squeeze: Linear layer for squeeze operation.</span>
</span><span data-line="417"><span class="sd">        W1: First linear layer of excitation network.</span>
</span><span data-line="418"><span class="sd">        W2: Second linear layer of excitation network.</span>
</span><span data-line="419"><span class="sd">        output_projection: Final projection to output dimension.</span>
</span><span data-line="420"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="421">
<div class="viewcode-block" id="LayersFuser.__init__">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.LayersFuser.__init__">[docs]</a>
</span><span data-line="422">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="423"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the layer fusion module.</span>
</span><span data-line="424">
</span><span data-line="425"><span class="sd">        Args:</span>
</span><span data-line="426"><span class="sd">            num_layers: Number of encoder layers to fuse.</span>
</span><span data-line="427"><span class="sd">            hidden_size: Hidden dimension size.</span>
</span><span data-line="428"><span class="sd">            output_size: Output dimension size. If None, uses hidden_size.</span>
</span><span data-line="429"><span class="sd">                Defaults to None.</span>
</span><span data-line="430"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="431">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span data-line="432">        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
</span><span data-line="433">        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span data-line="434">        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span> <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">hidden_size</span>
</span><span data-line="435">
</span><span data-line="436">        <span class="c1"># Squeeze operation</span>
</span><span data-line="437">        <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span data-line="438">
</span><span data-line="439">        <span class="c1"># Excitation operation</span>
</span><span data-line="440">        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">num_layers</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
</span><span data-line="441">        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">)</span>
</span><span data-line="442">
</span><span data-line="443">        <span class="c1"># Final projection</span>
</span><span data-line="444">        <span class="bp">self</span><span class="o">.</span><span class="n">output_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span></div>

</span><span data-line="445">
<div class="viewcode-block" id="LayersFuser.forward">
<a class="viewcode-back" href="../../../api/gliner.modeling.layers.html#gliner.modeling.layers.LayersFuser.forward">[docs]</a>
</span><span data-line="446">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span data-line="447"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Fuses multiple encoder layer outputs into a single representation.</span>
</span><span data-line="448">
</span><span data-line="449"><span class="sd">        Args:</span>
</span><span data-line="450"><span class="sd">            encoder_outputs: List of encoder output tensors, each of shape</span>
</span><span data-line="451"><span class="sd">                (batch_size, seq_len, hidden_size). The first element is skipped.</span>
</span><span data-line="452">
</span><span data-line="453"><span class="sd">        Returns:</span>
</span><span data-line="454"><span class="sd">            Fused output tensor of shape (batch_size, seq_len, output_size).</span>
</span><span data-line="455"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="456">        <span class="c1"># Concatenate all layers (skip first layer)</span>
</span><span data-line="457">        <span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [B, K, L, D]</span>
</span><span data-line="458">
</span><span data-line="459">        <span class="c1"># Squeeze operation</span>
</span><span data-line="460">        <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">U</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [B, K, L]</span>
</span><span data-line="461">        <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># [B, K]</span>
</span><span data-line="462">
</span><span data-line="463">        <span class="c1"># Excitation operation</span>
</span><span data-line="464">        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">(</span><span class="n">Z</span><span class="p">)))</span>  <span class="c1"># [B, K]</span>
</span><span data-line="465">        <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>  <span class="c1"># [B, K]</span>
</span><span data-line="466">
</span><span data-line="467">        <span class="c1"># Apply attention weights</span>
</span><span data-line="468">        <span class="n">U_weighted</span> <span class="o">=</span> <span class="n">U</span> <span class="o">*</span> <span class="n">s</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [B, K, L, D]</span>
</span><span data-line="469">
</span><span data-line="470">        <span class="c1"># Sum across layers</span>
</span><span data-line="471">        <span class="n">U_sum</span> <span class="o">=</span> <span class="n">U_weighted</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [B, L, D]</span>
</span><span data-line="472">
</span><span data-line="473">        <span class="c1"># Final projection</span>
</span><span data-line="474">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_projection</span><span class="p">(</span><span class="n">U_sum</span><span class="p">)</span>  <span class="c1"># [B, L, output_size]</span>
</span><span data-line="475">
</span><span data-line="476">        <span class="k">return</span> <span class="n">output</span></div>
</div>

</span></pre></div>
        </article><button class="back-to-top" type="button">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
  </svg>
  <span>Back to top</span>
</button><div class="navigation flex print:hidden"></div></div>
    </div>
  </main>
</div>
<footer class="sy-foot">
  <div class="sy-foot-inner sy-container mx-auto">
    <div class="sy-foot-reserved md:flex justify-between items-center">
      <div class="sy-foot-copyright"><p>2025, GLiNER community</p>
  
  <p>
    Made with
    
    <a href="https://www.sphinx-doc.org/">Sphinx</a> and
    
    <a href="https://shibuya.lepture.com">Shibuya theme</a>.
  </p>
</div>
      <div class="sy-foot-socials"></div>
    </div>
  </div>
</footer>
      <script src="../../../_static/documentation_options.js?v=dc91f075"></script>
      <script src="../../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../_static/shibuya.js?v=9b0e4dde"></script></body>
</html>