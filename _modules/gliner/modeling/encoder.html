<!DOCTYPE html>
<html lang="en" data-accent-color="violet" data-content_root="../../../">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>gliner.modeling.encoder - Home 0.2.24 documentation</title><link rel="index" title="Index" href="../../../genindex.html" /><link rel="search" title="Search" href="../../../search.html" /><script>
    function setColorMode(t){let e=document.documentElement;e.setAttribute("data-color-mode",t);let a=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,s=t;"auto"===t&&(s=a?"dark":"light"),"light"===s?(e.classList.remove("dark"),e.classList.add("light")):(e.classList.remove("light"),e.classList.add("dark"))}
    setColorMode(localStorage._theme||"auto");
  </script><link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=e1a1ceaf" />
    <link rel="stylesheet" type="text/css" href="../../../_static/shibuya.css?v=d140fbf8" />
    <link media="print" rel="stylesheet" type="text/css" href="../../../_static/print.css?v=20ff2c19" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
:root {
  --sy-f-text: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
  --sy-f-heading: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
}
</style>
    <meta property="og:type" content="website"/><meta property="og:title" content="gliner.modeling.encoder"/>
<meta name="twitter:card" content="summary"/>
  </head>
<body><div class="sy-head">
  <div class="sy-head-blur"></div>
  <div class="sy-head-inner sy-container mx-auto">
    <a class="sy-head-brand" href="../../../index.html">
      
      
      <strong>Home</strong>
    </a>
    <div class="sy-head-nav" id="head-nav">
      <nav class="sy-head-links"></nav>
      <div class="sy-head-extra flex items-center print:hidden"><form class="searchbox flex items-center" action="../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <kbd>/</kbd>
</form><div class="sy-head-socials"></div></div>
    </div>
    <div class="sy-head-actions flex items-center shrink-0 print:hidden"><button class="js-theme theme-switch flex items-center"
data-aria-auto="Switch to light color mode"
data-aria-light="Switch to dark color mode"
data-aria-dark="Switch to auto color mode">
<i class="i-lucide theme-icon"></i>
</button><button class="md:hidden flex items-center js-menu" aria-label="Menu" type="button" aria-controls="head-nav" aria-expanded="false">
        <div class="hamburger">
          <span class="hamburger_1"></span>
          <span class="hamburger_2"></span>
          <span class="hamburger_3"></span>
        </div>
      </button>
    </div>
  </div>
</div>
<div class="sy-page sy-container flex mx-auto">
  <aside id="lside" class="sy-lside md:w-72 md:shrink-0 print:hidden">
    <div class="sy-lside-inner md:sticky">
      <div class="sy-scrollbar p-6">
        <div class="globaltoc" data-expand-depth="0"><p class="caption" role="heading" aria-level="3"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../intro.html">Introduction to ðŸ‘‘ GLiNER</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../instalation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../usage.html">Advanced Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../configs.html">Components &amp; Configs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../training.html">Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architectures.html">Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../convert_to_onnx.html">ONNX Export &amp; Deployment</a></li>
</ul>
<p class="caption" role="heading" aria-level="3"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.model.html">gliner.model module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.config.html">gliner.config module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.training.html">gliner.training package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.training.trainer.html">gliner.training.trainer module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.modeling.html">gliner.modeling package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.multitask.html">gliner.modeling.multitask package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.modeling.multitask.relations_layers.html">gliner.modeling.multitask.relations_layers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.modeling.multitask.triples_layers.html">gliner.modeling.multitask.triples_layers module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.base.html">gliner.modeling.base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.decoder.html">gliner.modeling.decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.encoder.html">gliner.modeling.encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.layers.html">gliner.modeling.layers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.loss_functions.html">gliner.modeling.loss_functions module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.outputs.html">gliner.modeling.outputs module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.scorers.html">gliner.modeling.scorers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.span_rep.html">gliner.modeling.span_rep module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.utils.html">gliner.modeling.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.data_processing.html">gliner.data_processing package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.collator.html">gliner.data_processing.collator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.processor.html">gliner.data_processing.processor module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.tokenizer.html">gliner.data_processing.tokenizer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.utils.html">gliner.data_processing.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.evaluation.html">gliner.evaluation package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.evaluate_ner.html">gliner.evaluation.evaluate_ner module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.evaluator.html">gliner.evaluation.evaluator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.utils.html">gliner.evaluation.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.onnx.html">gliner.onnx package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.onnx.model.html">gliner.onnx.model module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.decoding.html">gliner.decoding package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.trie.html">gliner.decoding.trie package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.decoding.trie.labels_trie.html">gliner.decoding.trie.labels_trie module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.decoding.trie.python_labels_trie.html">gliner.decoding.trie.python_labels_trie module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.decoder.html">gliner.decoding.decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.utils.html">gliner.decoding.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.utils.html">gliner.utils module</a></li>
</ul>

        </div>
      </div>
    </div>
  </aside>
  <div class="lside-overlay js-menu" role="button" aria-label="Close left sidebar" aria-controls="lside" aria-expanded="false"></div>
  <aside id="rside" class="sy-rside pb-3 w-64 shrink-0 order-last">
    <button class="rside-close js-menu xl:hidden" aria-label="Close Table of Contents" type="button" aria-controls="rside" aria-expanded="false">
      <i class="i-lucide close"></i>
    </button>
    <div class="sy-scrollbar sy-rside-inner px-6 xl:top-16 xl:sticky xl:pl-0 pt-6 pb-4"><div id="ethical-ad-placement" data-ea-publisher="readthedocs"></div></div>
  </aside>
  <div class="rside-overlay js-menu" role="button" aria-label="Close Table of Contents" aria-controls="rside" aria-expanded="false"></div>
  <main class="sy-main w-full max-sm:max-w-full print:pt-6">
<div class="sy-breadcrumbs" role="navigation">
  <div class="sy-breadcrumbs-inner flex items-center">
    <div class="md:hidden mr-3">
      <button class="js-menu" aria-label="Menu" type="button" aria-controls="lside" aria-expanded="false">
        <i class="i-lucide menu"></i>
      </button>
    </div>
    <ol class="flex-1" itemscope itemtype="https://schema.org/BreadcrumbList"><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../../../index.html"><span itemprop="name">Home</span></a>
        <span>/</span>
        <meta itemprop="position" content="1" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../../index.html"><span itemprop="name">Module code</span></a>
        <span>/</span>
        <meta itemprop="position" content="2" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <strong itemprop="name">gliner.modeling.encoder</strong>
        <meta itemprop="position" content="3" />
      </li></ol>
    <div class="xl:hidden ml-1">
      <button class="js-menu" aria-label="Show table of contents" type="button" aria-controls="rside"
        aria-expanded="false">
        <i class="i-lucide outdent"></i>
      </button>
    </div>
  </div>
</div><div class="flex flex-col break-words justify-between">
      <div class="min-w-0 max-w-6xl px-6 pb-6 pt-8 xl:px-12">
        <article class="yue" role="main">
          <h1>Source code for gliner.modeling.encoder</h1><div class="highlight"><pre>
<span></span><span data-line="1"><span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
</span><span data-line="2"><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span>
</span><span data-line="3"><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
</span><span data-line="4">
</span><span data-line="5"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span data-line="6"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span data-line="7"><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoConfig</span>
</span><span data-line="8"><span class="kn">from</span><span class="w"> </span><span class="nn">transformers.modeling_outputs</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModelOutput</span>
</span><span data-line="9">
</span><span data-line="10"><span class="kn">from</span><span class="w"> </span><span class="nn">..utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">MissedPackageException</span><span class="p">,</span> <span class="n">is_module_available</span>
</span><span data-line="11"><span class="kn">from</span><span class="w"> </span><span class="nn">.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">LayersFuser</span>
</span><span data-line="12"><span class="kn">from</span><span class="w"> </span><span class="nn">..infer_packing</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferencePackingConfig</span><span class="p">,</span> <span class="n">unpack_spans</span><span class="p">,</span> <span class="n">pack_requests</span>
</span><span data-line="13">
</span><span data-line="14"><span class="c1"># Check for optional dependencies</span>
</span><span data-line="15"><span class="n">IS_LLM2VEC</span> <span class="o">=</span> <span class="n">is_module_available</span><span class="p">(</span><span class="s2">&quot;llm2vec&quot;</span><span class="p">)</span>
</span><span data-line="16"><span class="n">IS_PEFT</span> <span class="o">=</span> <span class="n">is_module_available</span><span class="p">(</span><span class="s2">&quot;peft&quot;</span><span class="p">)</span>
</span><span data-line="17"><span class="n">IS_TURBOT5</span> <span class="o">=</span> <span class="n">is_module_available</span><span class="p">(</span><span class="s2">&quot;turbot5&quot;</span><span class="p">)</span>
</span><span data-line="18"><span class="n">IS_FLASHDEBERTA</span> <span class="o">=</span> <span class="n">is_module_available</span><span class="p">(</span><span class="s2">&quot;flashdeberta&quot;</span><span class="p">)</span>
</span><span data-line="19">
</span><span data-line="20"><span class="k">if</span> <span class="n">IS_LLM2VEC</span><span class="p">:</span>
</span><span data-line="21">    <span class="kn">from</span><span class="w"> </span><span class="nn">llm2vec.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">GemmaBiModel</span><span class="p">,</span> <span class="n">LlamaBiModel</span><span class="p">,</span> <span class="n">Qwen2BiModel</span><span class="p">,</span> <span class="n">MistralBiModel</span>
</span><span data-line="22">
</span><span data-line="23">    <span class="n">DECODER_MODEL_MAPPING</span> <span class="o">=</span> <span class="p">{</span>
</span><span data-line="24">        <span class="s2">&quot;MistralConfig&quot;</span><span class="p">:</span> <span class="n">MistralBiModel</span><span class="p">,</span>
</span><span data-line="25">        <span class="s2">&quot;LlamaConfig&quot;</span><span class="p">:</span> <span class="n">LlamaBiModel</span><span class="p">,</span>
</span><span data-line="26">        <span class="s2">&quot;GemmaConfig&quot;</span><span class="p">:</span> <span class="n">GemmaBiModel</span><span class="p">,</span>
</span><span data-line="27">        <span class="s2">&quot;Qwen2Config&quot;</span><span class="p">:</span> <span class="n">Qwen2BiModel</span><span class="p">,</span>
</span><span data-line="28">    <span class="p">}</span>
</span><span data-line="29"><span class="k">else</span><span class="p">:</span>
</span><span data-line="30">    <span class="n">DECODER_MODEL_MAPPING</span> <span class="o">=</span> <span class="p">{}</span>
</span><span data-line="31">
</span><span data-line="32"><span class="k">if</span> <span class="n">IS_TURBOT5</span><span class="p">:</span>
</span><span data-line="33">    <span class="kn">from</span><span class="w"> </span><span class="nn">turbot5.model.modeling</span><span class="w"> </span><span class="kn">import</span> <span class="n">T5EncoderModel</span>
</span><span data-line="34"><span class="k">else</span><span class="p">:</span>
</span><span data-line="35">    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">T5EncoderModel</span>
</span><span data-line="36">
</span><span data-line="37"><span class="k">if</span> <span class="n">IS_FLASHDEBERTA</span><span class="p">:</span>
</span><span data-line="38">    <span class="kn">from</span><span class="w"> </span><span class="nn">flashdeberta</span><span class="w"> </span><span class="kn">import</span> <span class="n">FlashDebertaV2Model</span> <span class="k">as</span> <span class="n">DebertaV2Model</span>
</span><span data-line="39"><span class="k">else</span><span class="p">:</span>
</span><span data-line="40">    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DebertaV2Model</span>
</span><span data-line="41">
</span><span data-line="42"><span class="k">if</span> <span class="n">IS_PEFT</span><span class="p">:</span>
</span><span data-line="43">    <span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span>
</span><span data-line="44">
</span><span data-line="45">
<div class="viewcode-block" id="Transformer">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.Transformer">[docs]</a>
</span><span data-line="46"><span class="k">class</span><span class="w"> </span><span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span data-line="47"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Flexible transformer wrapper supporting multiple architectures and configurations.</span>
</span><span data-line="48">
</span><span data-line="49"><span class="sd">    This class provides a unified interface for various transformer models including</span>
</span><span data-line="50"><span class="sd">    encoder-only (BERT, DeBERTa), encoder-decoder (T5), and decoder-only models</span>
</span><span data-line="51"><span class="sd">    (LLaMA, Mistral) with bidirectional adaptations. It handles model initialization,</span>
</span><span data-line="52"><span class="sd">    adapter loading, and specialized forward passes for different architectures.</span>
</span><span data-line="53">
</span><span data-line="54"><span class="sd">    Attributes:</span>
</span><span data-line="55"><span class="sd">        model: The underlying transformer model instance.</span>
</span><span data-line="56"><span class="sd">        layers_fuser: Optional layer fusion module when config.fuse_layers is True.</span>
</span><span data-line="57"><span class="sd">        config: Configuration object containing model hyperparameters.</span>
</span><span data-line="58"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="59">
<div class="viewcode-block" id="Transformer.__init__">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.Transformer.__init__">[docs]</a>
</span><span data-line="60">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span data-line="61">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="62">        <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span data-line="63">        <span class="n">config</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
</span><span data-line="64">        <span class="n">from_pretrained</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span data-line="65">        <span class="n">labels_encoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span data-line="66">        <span class="n">cache_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="67">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="68"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the transformer wrapper.</span>
</span><span data-line="69">
</span><span data-line="70"><span class="sd">        Args:</span>
</span><span data-line="71"><span class="sd">            model_name: Name or path of the pretrained model to load.</span>
</span><span data-line="72"><span class="sd">            config: Configuration object containing model hyperparameters. Must have</span>
</span><span data-line="73"><span class="sd">                attributes like `encoder_config`, `labels_encoder_config`, `vocab_size`,</span>
</span><span data-line="74"><span class="sd">                `_attn_implementation`, and `fuse_layers`.</span>
</span><span data-line="75"><span class="sd">            from_pretrained: If True, loads pretrained weights. If False, initializes</span>
</span><span data-line="76"><span class="sd">                from config only. Defaults to False.</span>
</span><span data-line="77"><span class="sd">            labels_encoder: If True, initializes as a labels encoder using</span>
</span><span data-line="78"><span class="sd">                `config.labels_encoder_config`. Defaults to False.</span>
</span><span data-line="79"><span class="sd">            cache_dir: Optional directory for caching downloaded models. Defaults to None.</span>
</span><span data-line="80">
</span><span data-line="81"><span class="sd">        Raises:</span>
</span><span data-line="82"><span class="sd">            MissedPackageException: If required packages (llm2vec, peft) are not installed</span>
</span><span data-line="83"><span class="sd">                when needed for specific model types.</span>
</span><span data-line="84"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="85">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span data-line="86">        <span class="k">if</span> <span class="n">labels_encoder</span><span class="p">:</span>
</span><span data-line="87">            <span class="n">encoder_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">labels_encoder_config</span>
</span><span data-line="88">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="89">            <span class="n">encoder_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">encoder_config</span>
</span><span data-line="90">        <span class="k">if</span> <span class="n">encoder_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="91">            <span class="n">encoder_config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">)</span>
</span><span data-line="92">            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
</span><span data-line="93">                <span class="n">encoder_config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
</span><span data-line="94">
</span><span data-line="95">        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">labels_encoder</span><span class="p">:</span>
</span><span data-line="96">            <span class="n">encoder_config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span>
</span><span data-line="97">
</span><span data-line="98">        <span class="n">config_name</span> <span class="o">=</span> <span class="n">encoder_config</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
</span><span data-line="99">
</span><span data-line="100">        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
</span><span data-line="101">        <span class="k">if</span> <span class="n">config_name</span> <span class="ow">in</span> <span class="n">DECODER_MODEL_MAPPING</span><span class="p">:</span>
</span><span data-line="102">            <span class="k">if</span> <span class="ow">not</span> <span class="n">IS_LLM2VEC</span><span class="p">:</span>
</span><span data-line="103">                <span class="k">raise</span> <span class="n">MissedPackageException</span><span class="p">(</span>
</span><span data-line="104">                    <span class="sa">f</span><span class="s2">&quot;The llm2vec package must be installed to use this decoder model: </span><span class="si">{</span><span class="n">config_name</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span data-line="105">                <span class="p">)</span>
</span><span data-line="106">            <span class="k">else</span><span class="p">:</span>
</span><span data-line="107">                <span class="n">ModelClass</span> <span class="o">=</span> <span class="n">DECODER_MODEL_MAPPING</span><span class="p">[</span><span class="n">config_name</span><span class="p">]</span>
</span><span data-line="108">            <span class="n">custom</span> <span class="o">=</span> <span class="kc">True</span>
</span><span data-line="109">        <span class="k">elif</span> <span class="n">config_name</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;T5Config&quot;</span><span class="p">,</span> <span class="s2">&quot;MT5Config&quot;</span><span class="p">}:</span>
</span><span data-line="110">            <span class="n">custom</span> <span class="o">=</span> <span class="kc">True</span>
</span><span data-line="111">            <span class="n">ModelClass</span> <span class="o">=</span> <span class="n">T5EncoderModel</span>
</span><span data-line="112">            <span class="k">if</span> <span class="n">IS_TURBOT5</span><span class="p">:</span>
</span><span data-line="113">                <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;attention_type&quot;</span><span class="p">:</span> <span class="s2">&quot;flash&quot;</span><span class="p">}</span>
</span><span data-line="114">        <span class="k">elif</span> <span class="n">config_name</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;DebertaV2Config&quot;</span><span class="p">}:</span>
</span><span data-line="115">            <span class="n">custom</span> <span class="o">=</span> <span class="kc">True</span>
</span><span data-line="116">            <span class="n">ModelClass</span> <span class="o">=</span> <span class="n">DebertaV2Model</span>
</span><span data-line="117">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="118">            <span class="n">custom</span> <span class="o">=</span> <span class="kc">False</span>
</span><span data-line="119">            <span class="n">ModelClass</span> <span class="o">=</span> <span class="n">AutoModel</span>
</span><span data-line="120">
</span><span data-line="121">        <span class="k">if</span> <span class="n">from_pretrained</span><span class="p">:</span>
</span><span data-line="122">            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">ModelClass</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span data-line="123">        <span class="k">elif</span> <span class="ow">not</span> <span class="n">custom</span><span class="p">:</span>
</span><span data-line="124">            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">ModelClass</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">encoder_config</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span data-line="125">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="126">            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">ModelClass</span><span class="p">(</span><span class="n">encoder_config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span data-line="127">
</span><span data-line="128">        <span class="n">adapter_config_file</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;adapter_config.json&quot;</span>
</span><span data-line="129">
</span><span data-line="130">        <span class="k">if</span> <span class="n">adapter_config_file</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
</span><span data-line="131">            <span class="k">if</span> <span class="ow">not</span> <span class="n">IS_PEFT</span><span class="p">:</span>
</span><span data-line="132">                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
</span><span data-line="133">                    <span class="s2">&quot;Adapter configs were detected, if you want to apply them you need to install peft package.&quot;</span><span class="p">,</span>
</span><span data-line="134">                    <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span><span data-line="135">                <span class="p">)</span>
</span><span data-line="136">            <span class="k">else</span><span class="p">:</span>
</span><span data-line="137">                <span class="n">adapter_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span><span data-line="138">                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">adapter_config</span><span class="p">)</span>
</span><span data-line="139">
</span><span data-line="140">        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">fuse_layers</span><span class="p">:</span>
</span><span data-line="141">            <span class="bp">self</span><span class="o">.</span><span class="n">layers_fuser</span> <span class="o">=</span> <span class="n">LayersFuser</span><span class="p">(</span><span class="n">encoder_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">,</span> <span class="n">encoder_config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
</span><span data-line="142">
</span><span data-line="143">        <span class="k">if</span> <span class="n">labels_encoder</span><span class="p">:</span>
</span><span data-line="144">            <span class="n">config</span><span class="o">.</span><span class="n">labels_encoder_config</span> <span class="o">=</span> <span class="n">encoder_config</span>
</span><span data-line="145">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="146">            <span class="n">config</span><span class="o">.</span><span class="n">encoder_config</span> <span class="o">=</span> <span class="n">encoder_config</span>
</span><span data-line="147">
</span><span data-line="148">        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span></div>

</span><span data-line="149">
<div class="viewcode-block" id="Transformer.forward">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.Transformer.forward">[docs]</a>
</span><span data-line="150">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span data-line="151"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass through the transformer model.</span>
</span><span data-line="152">
</span><span data-line="153"><span class="sd">        Handles different attention mask configurations and model architectures,</span>
</span><span data-line="154"><span class="sd">        including support for pair attention masks for packed sequences.</span>
</span><span data-line="155">
</span><span data-line="156"><span class="sd">        Args:</span>
</span><span data-line="157"><span class="sd">            *args: Variable positional arguments passed to the model.</span>
</span><span data-line="158"><span class="sd">            **kwargs: Variable keyword arguments. Special arguments include:</span>
</span><span data-line="159"><span class="sd">                - pair_attention_mask: Optional pairwise attention mask of shape</span>
</span><span data-line="160"><span class="sd">                    (batch_size, seq_len, seq_len) for packed sequences.</span>
</span><span data-line="161"><span class="sd">                - attention_mask: Standard attention mask of shape (batch_size, seq_len).</span>
</span><span data-line="162"><span class="sd">                - input_ids: Input token IDs of shape (batch_size, seq_len).</span>
</span><span data-line="163"><span class="sd">                - Other model-specific arguments.</span>
</span><span data-line="164">
</span><span data-line="165"><span class="sd">        Returns:</span>
</span><span data-line="166"><span class="sd">            Encoded representations of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="167"><span class="sd">            If config.fuse_layers is True, returns fused layer outputs, otherwise</span>
</span><span data-line="168"><span class="sd">            returns the last hidden state.</span>
</span><span data-line="169"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="170">        <span class="n">pair_attention_mask</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;pair_attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="171">        <span class="n">base_attention_mask</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="172">        <span class="c1"># Extract input_ids if present</span>
</span><span data-line="173">        <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</span><span data-line="174">        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="175">        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">args</span><span class="p">:</span>
</span><span data-line="176">            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span data-line="177">            <span class="n">args</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
</span><span data-line="178">        <span class="n">args</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</span><span data-line="179">
</span><span data-line="180">        <span class="c1"># Set default kwargs</span>
</span><span data-line="181">        <span class="n">kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span><span data-line="182">        <span class="n">kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;return_dict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</span><span data-line="183">
</span><span data-line="184">        <span class="c1"># Handle output_hidden_states based on fuse_layers config</span>
</span><span data-line="185">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">fuse_layers</span><span class="p">:</span>
</span><span data-line="186">            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
</span><span data-line="187">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="188">            <span class="n">kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span><span data-line="189">
</span><span data-line="190">        <span class="k">if</span> <span class="n">pair_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="191">            <span class="n">mask_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_pair_attention_masks</span><span class="p">(</span>
</span><span data-line="192">                <span class="n">pair_attention_mask</span><span class="p">,</span>
</span><span data-line="193">                <span class="n">base_attention_mask</span><span class="p">,</span>
</span><span data-line="194">                <span class="n">input_ids</span><span class="p">,</span>
</span><span data-line="195">                <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">),</span>
</span><span data-line="196">            <span class="p">)</span>
</span><span data-line="197">
</span><span data-line="198">            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
</span><span data-line="199">            <span class="n">model_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
</span><span data-line="200">
</span><span data-line="201">            <span class="k">if</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;DebertaV2Model&quot;</span><span class="p">,</span> <span class="s2">&quot;DebertaModel&quot;</span><span class="p">}:</span>
</span><span data-line="202">                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_deberta</span><span class="p">(</span>
</span><span data-line="203">                    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
</span><span data-line="204">                    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
</span><span data-line="205">                    <span class="n">mask_info</span><span class="o">=</span><span class="n">mask_info</span><span class="p">,</span>
</span><span data-line="206">                <span class="p">)</span>
</span><span data-line="207">            <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s2">&quot;ModernBertModel&quot;</span><span class="p">:</span>
</span><span data-line="208">                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_modernbert</span><span class="p">(</span>
</span><span data-line="209">                    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
</span><span data-line="210">                    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
</span><span data-line="211">                    <span class="n">mask_info</span><span class="o">=</span><span class="n">mask_info</span><span class="p">,</span>
</span><span data-line="212">                <span class="p">)</span>
</span><span data-line="213">            <span class="k">elif</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;T5EncoderModel&quot;</span><span class="p">,</span> <span class="s2">&quot;MT5EncoderModel&quot;</span><span class="p">,</span> <span class="s2">&quot;T5Model&quot;</span><span class="p">}:</span>
</span><span data-line="214">                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_t5</span><span class="p">(</span>
</span><span data-line="215">                    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
</span><span data-line="216">                    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
</span><span data-line="217">                    <span class="n">mask_info</span><span class="o">=</span><span class="n">mask_info</span><span class="p">,</span>
</span><span data-line="218">                <span class="p">)</span>
</span><span data-line="219">            <span class="k">else</span><span class="p">:</span>
</span><span data-line="220">                <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;packing_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="221">                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_info</span><span class="p">[</span><span class="s2">&quot;extended_mask&quot;</span><span class="p">]</span>
</span><span data-line="222">                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
</span><span data-line="223">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="224">            <span class="k">if</span> <span class="n">base_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="225">                <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">base_attention_mask</span>
</span><span data-line="226">            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span data-line="227">
</span><span data-line="228">        <span class="c1"># Common logic for both paths</span>
</span><span data-line="229">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">fuse_layers</span><span class="p">:</span>
</span><span data-line="230">            <span class="n">encoder_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_fuser</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">)</span>
</span><span data-line="231">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="232">            <span class="n">encoder_layer</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span data-line="233">
</span><span data-line="234">        <span class="k">return</span> <span class="n">encoder_layer</span></div>

</span><span data-line="235">
</span><span data-line="236">    <span class="k">def</span><span class="w"> </span><span class="nf">_get_model_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
</span><span data-line="237"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Gets the data type of the model parameters.</span>
</span><span data-line="238">
</span><span data-line="239"><span class="sd">        Returns:</span>
</span><span data-line="240"><span class="sd">            The dtype of the model&#39;s parameters, or torch.float32 if no parameters exist.</span>
</span><span data-line="241"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="242">        <span class="k">try</span><span class="p">:</span>
</span><span data-line="243">            <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">dtype</span>
</span><span data-line="244">        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
</span><span data-line="245">            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
</span><span data-line="246">
</span><span data-line="247">    <span class="k">def</span><span class="w"> </span><span class="nf">_prepare_pair_attention_masks</span><span class="p">(</span>
</span><span data-line="248">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="249">        <span class="n">pair_attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="250">        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span data-line="251">        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span data-line="252">        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span data-line="253">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span data-line="254"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Prepares attention masks for packed sequence processing.</span>
</span><span data-line="255">
</span><span data-line="256"><span class="sd">        Converts pair attention masks (which specify token-to-token visibility) into</span>
</span><span data-line="257"><span class="sd">        various mask formats required by different transformer architectures. Ensures</span>
</span><span data-line="258"><span class="sd">        diagonal elements are attended to and inactive tokens are properly masked.</span>
</span><span data-line="259">
</span><span data-line="260"><span class="sd">        Args:</span>
</span><span data-line="261"><span class="sd">            pair_attention_mask: Pairwise attention mask of shape (batch_size, seq_len, seq_len)</span>
</span><span data-line="262"><span class="sd">                where 1 indicates attention is allowed.</span>
</span><span data-line="263"><span class="sd">            attention_mask: Optional standard attention mask of shape (batch_size, seq_len).</span>
</span><span data-line="264"><span class="sd">            input_ids: Optional input token IDs for device detection.</span>
</span><span data-line="265"><span class="sd">            inputs_embeds: Optional input embeddings for device detection.</span>
</span><span data-line="266">
</span><span data-line="267"><span class="sd">        Returns:</span>
</span><span data-line="268"><span class="sd">            Dictionary containing:</span>
</span><span data-line="269"><span class="sd">                - token_mask: Per-token mask of shape (batch_size, seq_len).</span>
</span><span data-line="270"><span class="sd">                - token_mask_bool: Boolean version of token_mask.</span>
</span><span data-line="271"><span class="sd">                - extended_mask: 4D attention mask of shape (batch_size, 1, seq_len, seq_len)</span>
</span><span data-line="272"><span class="sd">                    with -inf for masked positions.</span>
</span><span data-line="273"><span class="sd">                - block_mask: Boolean 3D mask of shape (batch_size, seq_len, seq_len).</span>
</span><span data-line="274"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="275">        <span class="n">device</span> <span class="o">=</span> <span class="n">pair_attention_mask</span><span class="o">.</span><span class="n">device</span>
</span><span data-line="276">        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="277">            <span class="n">device</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">device</span>
</span><span data-line="278">        <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="279">            <span class="n">device</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">device</span>
</span><span data-line="280">
</span><span data-line="281">        <span class="n">pair_mask_bool</span> <span class="o">=</span> <span class="n">pair_attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</span><span data-line="282">
</span><span data-line="283">        <span class="n">token_mask_bool</span> <span class="o">=</span> <span class="n">pair_mask_bool</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span data-line="284">        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="285">            <span class="n">token_mask_bool</span> <span class="o">=</span> <span class="n">token_mask_bool</span> <span class="o">&amp;</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</span><span data-line="286">
</span><span data-line="287">        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">pair_mask_bool</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span data-line="288">        <span class="k">if</span> <span class="n">seq_len</span><span class="p">:</span>
</span><span data-line="289">            <span class="n">identity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span data-line="290">            <span class="n">token_diag</span> <span class="o">=</span> <span class="n">token_mask_bool</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span data-line="291">            <span class="n">pair_mask_bool</span> <span class="o">=</span> <span class="n">pair_mask_bool</span> <span class="o">|</span> <span class="p">(</span><span class="n">identity</span> <span class="o">&amp;</span> <span class="n">token_diag</span><span class="p">)</span>
</span><span data-line="292">
</span><span data-line="293">        <span class="n">active</span> <span class="o">=</span> <span class="n">token_mask_bool</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">token_mask_bool</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</span><span data-line="294">        <span class="n">pair_mask_bool</span> <span class="o">=</span> <span class="n">pair_mask_bool</span> <span class="o">&amp;</span> <span class="n">active</span>
</span><span data-line="295">
</span><span data-line="296">        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="297">            <span class="n">token_mask</span> <span class="o">=</span> <span class="n">token_mask_bool</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span data-line="298">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="299">            <span class="n">token_mask</span> <span class="o">=</span> <span class="n">token_mask_bool</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span><span data-line="300">
</span><span data-line="301">        <span class="n">mask_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_model_dtype</span><span class="p">()</span>
</span><span data-line="302">        <span class="n">neg_inf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">mask_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
</span><span data-line="303">        <span class="n">extended_mask</span> <span class="o">=</span> <span class="p">(</span>
</span><span data-line="304">            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">pair_mask_bool</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mask_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="305">            <span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">pair_mask_bool</span><span class="p">,</span> <span class="n">neg_inf</span><span class="p">)</span>
</span><span data-line="306">            <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span data-line="307">        <span class="p">)</span>
</span><span data-line="308">
</span><span data-line="309">        <span class="n">inactive</span> <span class="o">=</span> <span class="o">~</span><span class="n">token_mask_bool</span>
</span><span data-line="310">        <span class="k">if</span> <span class="n">inactive</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
</span><span data-line="311">            <span class="n">extended_mask</span> <span class="o">=</span> <span class="n">extended_mask</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
</span><span data-line="312">                <span class="n">inactive</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
</span><span data-line="313">                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mask_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
</span><span data-line="314">            <span class="p">)</span>
</span><span data-line="315">
</span><span data-line="316">        <span class="k">return</span> <span class="p">{</span>
</span><span data-line="317">            <span class="s2">&quot;token_mask&quot;</span><span class="p">:</span> <span class="n">token_mask</span><span class="p">,</span>
</span><span data-line="318">            <span class="s2">&quot;token_mask_bool&quot;</span><span class="p">:</span> <span class="n">token_mask_bool</span><span class="p">,</span>
</span><span data-line="319">            <span class="s2">&quot;extended_mask&quot;</span><span class="p">:</span> <span class="n">extended_mask</span><span class="p">,</span>
</span><span data-line="320">            <span class="s2">&quot;block_mask&quot;</span><span class="p">:</span> <span class="n">pair_mask_bool</span><span class="p">,</span>
</span><span data-line="321">        <span class="p">}</span>
</span><span data-line="322">
</span><span data-line="323">    <span class="k">def</span><span class="w"> </span><span class="nf">_forward_deberta</span><span class="p">(</span>
</span><span data-line="324">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="325">        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span data-line="326">        <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
</span><span data-line="327">        <span class="n">mask_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span data-line="328">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseModelOutput</span><span class="p">:</span>
</span><span data-line="329"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass through DeBERTa models with packed attention support.</span>
</span><span data-line="330">
</span><span data-line="331"><span class="sd">        Handles the specific requirements of DeBERTa architecture including embeddings,</span>
</span><span data-line="332"><span class="sd">        relative position encodings, and optional enhanced mask tuning (z_steps).</span>
</span><span data-line="333">
</span><span data-line="334"><span class="sd">        Args:</span>
</span><span data-line="335"><span class="sd">            input_ids: Input token IDs of shape (batch_size, seq_len), or None if</span>
</span><span data-line="336"><span class="sd">                inputs_embeds is provided.</span>
</span><span data-line="337"><span class="sd">            model_kwargs: Dictionary of model-specific keyword arguments including</span>
</span><span data-line="338"><span class="sd">                inputs_embeds, token_type_ids, position_ids, output_attentions,</span>
</span><span data-line="339"><span class="sd">                output_hidden_states, and return_dict.</span>
</span><span data-line="340"><span class="sd">            mask_info: Dictionary containing prepared attention masks from</span>
</span><span data-line="341"><span class="sd">                _prepare_pair_attention_masks.</span>
</span><span data-line="342">
</span><span data-line="343"><span class="sd">        Returns:</span>
</span><span data-line="344"><span class="sd">            BaseModelOutput containing:</span>
</span><span data-line="345"><span class="sd">                - last_hidden_state: Final layer output of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="346"><span class="sd">                - hidden_states: Tuple of all layer outputs if requested.</span>
</span><span data-line="347"><span class="sd">                - attentions: Tuple of attention weights if requested.</span>
</span><span data-line="348">
</span><span data-line="349"><span class="sd">        Raises:</span>
</span><span data-line="350"><span class="sd">            ValueError: If neither or both input_ids and inputs_embeds are provided.</span>
</span><span data-line="351"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="352">        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="353">        <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="354">        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;position_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="355">        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">)</span>
</span><span data-line="356">        <span class="n">produce_hidden</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">)</span>
</span><span data-line="357">        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_dict&quot;</span><span class="p">)</span>
</span><span data-line="358">
</span><span data-line="359">        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="360">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Either input_ids or inputs_embeds must be provided for packed attention&quot;</span><span class="p">)</span>
</span><span data-line="361">        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="362">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot supply both input_ids and inputs_embeds&quot;</span><span class="p">)</span>
</span><span data-line="363">
</span><span data-line="364">        <span class="k">if</span> <span class="n">token_type_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="365">            <span class="n">ref</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">input_ids</span>
</span><span data-line="366">            <span class="n">shape</span> <span class="o">=</span> <span class="n">ref</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">ref</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span><span data-line="367">            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">ref</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="368">
</span><span data-line="369">        <span class="n">embedding_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span>
</span><span data-line="370">            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
</span><span data-line="371">            <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span>
</span><span data-line="372">            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
</span><span data-line="373">            <span class="n">mask</span><span class="o">=</span><span class="n">mask_info</span><span class="p">[</span><span class="s2">&quot;token_mask&quot;</span><span class="p">],</span>
</span><span data-line="374">            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
</span><span data-line="375">        <span class="p">)</span>
</span><span data-line="376">
</span><span data-line="377">        <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
</span><span data-line="378">            <span class="n">embedding_output</span><span class="p">,</span>
</span><span data-line="379">            <span class="n">mask_info</span><span class="p">[</span><span class="s2">&quot;block_mask&quot;</span><span class="p">],</span>
</span><span data-line="380">            <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span data-line="381">            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
</span><span data-line="382">            <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span data-line="383">        <span class="p">)</span>
</span><span data-line="384">
</span><span data-line="385">        <span class="n">encoded_layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">)</span>
</span><span data-line="386">
</span><span data-line="387">        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;z_steps&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span data-line="388">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">encoded_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
</span><span data-line="389">            <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">z_steps</span><span class="p">)]</span>
</span><span data-line="390">            <span class="n">query_states</span> <span class="o">=</span> <span class="n">encoded_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span data-line="391">            <span class="n">rel_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">get_rel_embedding</span><span class="p">()</span>
</span><span data-line="392">            <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">get_attention_mask</span><span class="p">(</span><span class="n">mask_info</span><span class="p">[</span><span class="s2">&quot;block_mask&quot;</span><span class="p">])</span>
</span><span data-line="393">            <span class="n">rel_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">get_rel_pos</span><span class="p">(</span><span class="n">embedding_output</span><span class="p">)</span>
</span><span data-line="394">            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
</span><span data-line="395">                <span class="n">query_states</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span>
</span><span data-line="396">                    <span class="n">hidden_states</span><span class="p">,</span>
</span><span data-line="397">                    <span class="n">attention_mask</span><span class="p">,</span>
</span><span data-line="398">                    <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span data-line="399">                    <span class="n">query_states</span><span class="o">=</span><span class="n">query_states</span><span class="p">,</span>
</span><span data-line="400">                    <span class="n">relative_pos</span><span class="o">=</span><span class="n">rel_pos</span><span class="p">,</span>
</span><span data-line="401">                    <span class="n">rel_embeddings</span><span class="o">=</span><span class="n">rel_embeddings</span><span class="p">,</span>
</span><span data-line="402">                <span class="p">)</span>
</span><span data-line="403">                <span class="n">encoded_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">query_states</span><span class="p">)</span>
</span><span data-line="404">
</span><span data-line="405">        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">encoded_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span data-line="406">        <span class="n">hidden_states_tuple</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">encoded_layers</span><span class="p">)</span> <span class="k">if</span> <span class="n">produce_hidden</span> <span class="k">else</span> <span class="kc">None</span>
</span><span data-line="407">        <span class="n">attentions</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
</span><span data-line="408">
</span><span data-line="409">        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
</span><span data-line="410">            <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequence_output</span><span class="p">,)</span>
</span><span data-line="411">            <span class="k">if</span> <span class="n">hidden_states_tuple</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="412">                <span class="n">result</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states_tuple</span><span class="p">,)</span>
</span><span data-line="413">            <span class="k">if</span> <span class="n">attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="414">                <span class="n">result</span> <span class="o">+=</span> <span class="p">(</span><span class="n">attentions</span><span class="p">,)</span>
</span><span data-line="415">            <span class="k">return</span> <span class="n">result</span>
</span><span data-line="416">
</span><span data-line="417">        <span class="k">return</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
</span><span data-line="418">            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">sequence_output</span><span class="p">,</span>
</span><span data-line="419">            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states_tuple</span><span class="p">,</span>
</span><span data-line="420">            <span class="n">attentions</span><span class="o">=</span><span class="n">attentions</span><span class="p">,</span>
</span><span data-line="421">        <span class="p">)</span>
</span><span data-line="422">
</span><span data-line="423">    <span class="k">def</span><span class="w"> </span><span class="nf">_forward_modernbert</span><span class="p">(</span>
</span><span data-line="424">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="425">        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span data-line="426">        <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
</span><span data-line="427">        <span class="n">mask_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span data-line="428">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseModelOutput</span><span class="p">:</span>
</span><span data-line="429"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass through ModernBERT models with packed attention support.</span>
</span><span data-line="430">
</span><span data-line="431"><span class="sd">        Handles ModernBERT-specific features including global and sliding window</span>
</span><span data-line="432"><span class="sd">        attention patterns, and temporarily switches to eager attention mode</span>
</span><span data-line="433"><span class="sd">        when using packed attention masks.</span>
</span><span data-line="434">
</span><span data-line="435"><span class="sd">        Args:</span>
</span><span data-line="436"><span class="sd">            input_ids: Input token IDs of shape (batch_size, seq_len), or None if</span>
</span><span data-line="437"><span class="sd">                inputs_embeds is provided.</span>
</span><span data-line="438"><span class="sd">            model_kwargs: Dictionary of model-specific keyword arguments including</span>
</span><span data-line="439"><span class="sd">                inputs_embeds, position_ids, indices, cu_seqlens, max_seqlen,</span>
</span><span data-line="440"><span class="sd">                batch_size, seq_len, output_attentions, output_hidden_states, return_dict.</span>
</span><span data-line="441"><span class="sd">            mask_info: Dictionary containing prepared attention masks from</span>
</span><span data-line="442"><span class="sd">                _prepare_pair_attention_masks.</span>
</span><span data-line="443">
</span><span data-line="444"><span class="sd">        Returns:</span>
</span><span data-line="445"><span class="sd">            BaseModelOutput containing:</span>
</span><span data-line="446"><span class="sd">                - last_hidden_state: Final layer output of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="447"><span class="sd">                - hidden_states: Tuple of all layer outputs if requested.</span>
</span><span data-line="448"><span class="sd">                - attentions: Tuple of attention weights if requested.</span>
</span><span data-line="449">
</span><span data-line="450"><span class="sd">        Raises:</span>
</span><span data-line="451"><span class="sd">            ValueError: If both or neither input_ids and inputs_embeds are provided.</span>
</span><span data-line="452"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="453">        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="454">        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;position_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="455">        <span class="n">cu_seqlens</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cu_seqlens&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="456">        <span class="n">max_seqlen</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;max_seqlen&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="457">        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="458">        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;seq_len&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="459">        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">)</span>
</span><span data-line="460">        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">)</span>
</span><span data-line="461">        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_dict&quot;</span><span class="p">)</span>
</span><span data-line="462">
</span><span data-line="463">        <span class="k">if</span> <span class="p">(</span><span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
</span><span data-line="464">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;ModernBERT requires exactly one of input_ids or inputs_embeds&quot;</span><span class="p">)</span>
</span><span data-line="465">
</span><span data-line="466">        <span class="n">token_mask_bool</span> <span class="o">=</span> <span class="n">mask_info</span><span class="p">[</span><span class="s2">&quot;token_mask_bool&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</span><span data-line="467">        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">seq_len</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="468">            <span class="n">ref</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">input_ids</span>
</span><span data-line="469">            <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">ref</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</span><span data-line="470">        <span class="n">device</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">device</span>
</span><span data-line="471">
</span><span data-line="472">        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="473">            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span data-line="474">
</span><span data-line="475">        <span class="n">base_attention_mask</span> <span class="o">=</span> <span class="n">token_mask_bool</span>
</span><span data-line="476">
</span><span data-line="477">        <span class="n">original_impl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span>
</span><span data-line="478">        <span class="k">if</span> <span class="n">original_impl</span> <span class="o">==</span> <span class="s2">&quot;flash_attention_2&quot;</span><span class="p">:</span>
</span><span data-line="479">            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="s2">&quot;eager&quot;</span>
</span><span data-line="480">
</span><span data-line="481">        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_maybe_set_compile</span><span class="p">()</span>
</span><span data-line="482">
</span><span data-line="483">        <span class="n">global_attention_mask</span><span class="p">,</span> <span class="n">sliding_window_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_update_attention_mask</span><span class="p">(</span>
</span><span data-line="484">            <span class="n">base_attention_mask</span><span class="p">,</span>
</span><span data-line="485">            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
</span><span data-line="486">        <span class="p">)</span>
</span><span data-line="487">
</span><span data-line="488">        <span class="n">block</span> <span class="o">=</span> <span class="n">mask_info</span><span class="p">[</span><span class="s2">&quot;block_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span data-line="489">        <span class="n">neg_inf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">global_attention_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
</span><span data-line="490">        <span class="n">global_attention_mask</span> <span class="o">=</span> <span class="n">global_attention_mask</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">block</span><span class="p">,</span> <span class="n">neg_inf</span><span class="p">)</span>
</span><span data-line="491">        <span class="n">sliding_window_mask</span> <span class="o">=</span> <span class="n">sliding_window_mask</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">block</span><span class="p">,</span> <span class="n">neg_inf</span><span class="p">)</span>
</span><span data-line="492">
</span><span data-line="493">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">)</span>
</span><span data-line="494">
</span><span data-line="495">        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
</span><span data-line="496">        <span class="n">all_self_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
</span><span data-line="497">
</span><span data-line="498">        <span class="k">for</span> <span class="n">encoder_layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
</span><span data-line="499">            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
</span><span data-line="500">                <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">)</span>
</span><span data-line="501">
</span><span data-line="502">            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">encoder_layer</span><span class="p">(</span>
</span><span data-line="503">                <span class="n">hidden_states</span><span class="p">,</span>
</span><span data-line="504">                <span class="n">attention_mask</span><span class="o">=</span><span class="n">global_attention_mask</span><span class="p">,</span>
</span><span data-line="505">                <span class="n">sliding_window_mask</span><span class="o">=</span><span class="n">sliding_window_mask</span><span class="p">,</span>
</span><span data-line="506">                <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
</span><span data-line="507">                <span class="n">cu_seqlens</span><span class="o">=</span><span class="n">cu_seqlens</span><span class="p">,</span>
</span><span data-line="508">                <span class="n">max_seqlen</span><span class="o">=</span><span class="n">max_seqlen</span><span class="p">,</span>
</span><span data-line="509">                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
</span><span data-line="510">            <span class="p">)</span>
</span><span data-line="511">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span data-line="512">            <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span data-line="513">                <span class="n">all_self_attentions</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">all_self_attentions</span><span class="p">,</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span data-line="514">
</span><span data-line="515">        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
</span><span data-line="516">            <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">)</span>
</span><span data-line="517">
</span><span data-line="518">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">final_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span><span data-line="519">
</span><span data-line="520">        <span class="k">if</span> <span class="n">original_impl</span> <span class="o">==</span> <span class="s2">&quot;flash_attention_2&quot;</span><span class="p">:</span>
</span><span data-line="521">            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="n">original_impl</span>
</span><span data-line="522">
</span><span data-line="523">        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
</span><span data-line="524">            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="525">
</span><span data-line="526">        <span class="k">return</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
</span><span data-line="527">            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
</span><span data-line="528">            <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
</span><span data-line="529">            <span class="n">attentions</span><span class="o">=</span><span class="n">all_self_attentions</span><span class="p">,</span>
</span><span data-line="530">        <span class="p">)</span>
</span><span data-line="531">
</span><span data-line="532">    <span class="k">def</span><span class="w"> </span><span class="nf">_forward_t5</span><span class="p">(</span>
</span><span data-line="533">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="534">        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span data-line="535">        <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
</span><span data-line="536">        <span class="n">mask_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span data-line="537">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseModelOutput</span><span class="p">:</span>
</span><span data-line="538"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass through T5 encoder models with packed attention support.</span>
</span><span data-line="539">
</span><span data-line="540"><span class="sd">        Handles T5/MT5-specific architecture requirements including relative position</span>
</span><span data-line="541"><span class="sd">        bias and proper attention mask formatting for the encoder stack.</span>
</span><span data-line="542">
</span><span data-line="543"><span class="sd">        Args:</span>
</span><span data-line="544"><span class="sd">            input_ids: Input token IDs of shape (batch_size, seq_len), or None if</span>
</span><span data-line="545"><span class="sd">                inputs_embeds is provided.</span>
</span><span data-line="546"><span class="sd">            model_kwargs: Dictionary of model-specific keyword arguments including</span>
</span><span data-line="547"><span class="sd">                input_ids (can override parameter), inputs_embeds, head_mask,</span>
</span><span data-line="548"><span class="sd">                past_key_values, use_cache, output_attentions, output_hidden_states,</span>
</span><span data-line="549"><span class="sd">                return_dict, cache_position.</span>
</span><span data-line="550"><span class="sd">            mask_info: Dictionary containing prepared attention masks from</span>
</span><span data-line="551"><span class="sd">                _prepare_pair_attention_masks.</span>
</span><span data-line="552">
</span><span data-line="553"><span class="sd">        Returns:</span>
</span><span data-line="554"><span class="sd">            BaseModelOutput containing:</span>
</span><span data-line="555"><span class="sd">                - last_hidden_state: Final layer output of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="556"><span class="sd">                - hidden_states: Tuple of all layer outputs if requested.</span>
</span><span data-line="557"><span class="sd">                - attentions: Tuple of attention weights if requested.</span>
</span><span data-line="558">
</span><span data-line="559"><span class="sd">        Raises:</span>
</span><span data-line="560"><span class="sd">            ValueError: If neither input_ids nor inputs_embeds is provided, or if</span>
</span><span data-line="561"><span class="sd">                unsupported kwargs are passed.</span>
</span><span data-line="562"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="563">        <span class="n">stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span>
</span><span data-line="564">
</span><span data-line="565">        <span class="n">kw_input_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="566">        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">kw_input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="567">            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">kw_input_ids</span>
</span><span data-line="568">
</span><span data-line="569">        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="570">        <span class="n">head_mask</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;head_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="571">        <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="572">        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_cache&quot;</span><span class="p">,</span> <span class="n">stack</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span><span class="p">)</span>
</span><span data-line="573">        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">)</span>
</span><span data-line="574">        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">)</span>
</span><span data-line="575">        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_dict&quot;</span><span class="p">)</span>
</span><span data-line="576">        <span class="n">cache_position</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_position&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="577">
</span><span data-line="578">        <span class="k">if</span> <span class="n">model_kwargs</span><span class="p">:</span>
</span><span data-line="579">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported kwargs for T5 forward: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span data-line="580">
</span><span data-line="581">        <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="582">            <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="583">                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Either input_ids or inputs_embeds must be provided&quot;</span><span class="p">)</span>
</span><span data-line="584">            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
</span><span data-line="585">            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span><span data-line="586">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="587">            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span data-line="588">
</span><span data-line="589">        <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span data-line="590">        <span class="n">device</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">device</span>
</span><span data-line="591">
</span><span data-line="592">        <span class="k">if</span> <span class="n">cache_position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="593">            <span class="n">cache_position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="594">
</span><span data-line="595">        <span class="n">block_mask</span> <span class="o">=</span> <span class="n">mask_info</span><span class="p">[</span><span class="s2">&quot;block_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</span><span data-line="596">
</span><span data-line="597">        <span class="n">dtype</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span>
</span><span data-line="598">        <span class="n">neg_inf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
</span><span data-line="599">        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">block_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="600">        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">block_mask</span><span class="p">,</span> <span class="n">neg_inf</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span data-line="601">
</span><span data-line="602">        <span class="n">head_mask</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">get_head_mask</span><span class="p">(</span><span class="n">head_mask</span><span class="p">,</span> <span class="n">stack</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span>
</span><span data-line="603">
</span><span data-line="604">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">)</span>
</span><span data-line="605">
</span><span data-line="606">        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
</span><span data-line="607">        <span class="n">all_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
</span><span data-line="608">        <span class="n">position_bias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span data-line="609">
</span><span data-line="610">        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">layer_module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">stack</span><span class="o">.</span><span class="n">block</span><span class="p">):</span>
</span><span data-line="611">            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
</span><span data-line="612">                <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
</span><span data-line="613">                    <span class="o">*</span><span class="n">all_hidden_states</span><span class="p">,</span>
</span><span data-line="614">                    <span class="n">hidden_states</span><span class="p">,</span>
</span><span data-line="615">                <span class="p">)</span>
</span><span data-line="616">
</span><span data-line="617">            <span class="n">layer_head_mask</span> <span class="o">=</span> <span class="n">head_mask</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">head_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span data-line="618">
</span><span data-line="619">            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">layer_module</span><span class="p">(</span>
</span><span data-line="620">                <span class="n">hidden_states</span><span class="p">,</span>
</span><span data-line="621">                <span class="n">attention_mask</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">,</span>
</span><span data-line="622">                <span class="n">position_bias</span><span class="o">=</span><span class="n">position_bias</span><span class="p">,</span>
</span><span data-line="623">                <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span data-line="624">                <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span data-line="625">                <span class="n">encoder_decoder_position_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span data-line="626">                <span class="n">layer_head_mask</span><span class="o">=</span><span class="n">layer_head_mask</span><span class="p">,</span>
</span><span data-line="627">                <span class="n">cross_attn_layer_head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span data-line="628">                <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="n">past_key_values</span><span class="p">,</span>
</span><span data-line="629">                <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span data-line="630">                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
</span><span data-line="631">                <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span data-line="632">                <span class="n">cache_position</span><span class="o">=</span><span class="n">cache_position</span><span class="p">,</span>
</span><span data-line="633">            <span class="p">)</span>
</span><span data-line="634">
</span><span data-line="635">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span data-line="636">            <span class="n">position_bias</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span data-line="637">
</span><span data-line="638">            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
</span><span data-line="639">                <span class="n">all_attentions</span> <span class="o">=</span> <span class="p">(</span>
</span><span data-line="640">                    <span class="o">*</span><span class="n">all_attentions</span><span class="p">,</span>
</span><span data-line="641">                    <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
</span><span data-line="642">                <span class="p">)</span>
</span><span data-line="643">
</span><span data-line="644">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span><span data-line="645">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span><span data-line="646">
</span><span data-line="647">        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
</span><span data-line="648">            <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">)</span>
</span><span data-line="649">
</span><span data-line="650">        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
</span><span data-line="651">            <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
</span><span data-line="652">            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
</span><span data-line="653">                <span class="n">result</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_hidden_states</span><span class="p">,)</span>
</span><span data-line="654">            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
</span><span data-line="655">                <span class="n">result</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_attentions</span><span class="p">,)</span>
</span><span data-line="656">            <span class="k">return</span> <span class="n">result</span>
</span><span data-line="657">
</span><span data-line="658">        <span class="k">return</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
</span><span data-line="659">            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
</span><span data-line="660">            <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
</span><span data-line="661">            <span class="n">attentions</span><span class="o">=</span><span class="n">all_attentions</span><span class="p">,</span>
</span><span data-line="662">        <span class="p">)</span></div>

</span><span data-line="663">
</span><span data-line="664">
<div class="viewcode-block" id="Encoder">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.Encoder">[docs]</a>
</span><span data-line="665"><span class="k">class</span><span class="w"> </span><span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span data-line="666"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Standard encoder module wrapping a transformer model with optional projection.</span>
</span><span data-line="667">
</span><span data-line="668"><span class="sd">    This class provides a high-level interface for encoding text sequences, including</span>
</span><span data-line="669"><span class="sd">    support for inference-time packing to improve throughput. It handles embedding</span>
</span><span data-line="670"><span class="sd">    extraction and optional projection to a different hidden size.</span>
</span><span data-line="671">
</span><span data-line="672"><span class="sd">    Attributes:</span>
</span><span data-line="673"><span class="sd">        bert_layer: The underlying Transformer instance.</span>
</span><span data-line="674"><span class="sd">        projection: Optional linear projection layer when config.hidden_size differs</span>
</span><span data-line="675"><span class="sd">            from the model&#39;s native hidden size.</span>
</span><span data-line="676"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="677">
<div class="viewcode-block" id="Encoder.__init__">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.Encoder.__init__">[docs]</a>
</span><span data-line="678">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span data-line="679">        <span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">from_pretrained</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">cache_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
</span><span data-line="680">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="681"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the encoder.</span>
</span><span data-line="682">
</span><span data-line="683"><span class="sd">        Args:</span>
</span><span data-line="684"><span class="sd">            config: Configuration object containing model hyperparameters including</span>
</span><span data-line="685"><span class="sd">                `model_name`, `hidden_size`, and transformer-specific settings.</span>
</span><span data-line="686"><span class="sd">            from_pretrained: If True, loads pretrained weights for the transformer.</span>
</span><span data-line="687"><span class="sd">                Defaults to False.</span>
</span><span data-line="688"><span class="sd">            cache_dir: Optional directory for caching downloaded models. Defaults to None.</span>
</span><span data-line="689"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="690">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span data-line="691">
</span><span data-line="692">        <span class="bp">self</span><span class="o">.</span><span class="n">bert_layer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">from_pretrained</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">)</span>
</span><span data-line="693">
</span><span data-line="694">        <span class="n">bert_hidden_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert_layer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
</span><span data-line="695">
</span><span data-line="696">        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">!=</span> <span class="n">bert_hidden_size</span><span class="p">:</span>
</span><span data-line="697">            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">bert_hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span></div>

</span><span data-line="698">
<div class="viewcode-block" id="Encoder.resize_token_embeddings">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.Encoder.resize_token_embeddings">[docs]</a>
</span><span data-line="699">    <span class="k">def</span><span class="w"> </span><span class="nf">resize_token_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">:</span>
</span><span data-line="700"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Resizes token embeddings to accommodate new vocabulary size.</span>
</span><span data-line="701">
</span><span data-line="702"><span class="sd">        Args:</span>
</span><span data-line="703"><span class="sd">            new_num_tokens: New vocabulary size.</span>
</span><span data-line="704"><span class="sd">            pad_to_multiple_of: Optional value to pad vocabulary size to a multiple.</span>
</span><span data-line="705"><span class="sd">                Defaults to None.</span>
</span><span data-line="706">
</span><span data-line="707"><span class="sd">        Returns:</span>
</span><span data-line="708"><span class="sd">            The resized embedding layer.</span>
</span><span data-line="709"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="710">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert_layer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="n">new_num_tokens</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="p">)</span></div>

</span><span data-line="711">
<div class="viewcode-block" id="Encoder.get_input_embeddings">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.Encoder.get_input_embeddings">[docs]</a>
</span><span data-line="712">    <span class="k">def</span><span class="w"> </span><span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">:</span>
</span><span data-line="713"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Gets the input embedding layer.</span>
</span><span data-line="714">
</span><span data-line="715"><span class="sd">        Returns:</span>
</span><span data-line="716"><span class="sd">            The model&#39;s input embedding layer.</span>
</span><span data-line="717"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="718">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert_layer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span></div>

</span><span data-line="719">
<div class="viewcode-block" id="Encoder.encode_text">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.Encoder.encode_text">[docs]</a>
</span><span data-line="720">    <span class="k">def</span><span class="w"> </span><span class="nf">encode_text</span><span class="p">(</span>
</span><span data-line="721">        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
</span><span data-line="722">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span data-line="723"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Encodes input text sequences into contextualized embeddings.</span>
</span><span data-line="724">
</span><span data-line="725"><span class="sd">        Supports inference-time packing to batch multiple variable-length sequences</span>
</span><span data-line="726"><span class="sd">        efficiently when packing_config is provided and not in training mode.</span>
</span><span data-line="727">
</span><span data-line="728"><span class="sd">        Args:</span>
</span><span data-line="729"><span class="sd">            input_ids: Input token IDs of shape (batch_size, seq_len).</span>
</span><span data-line="730"><span class="sd">            attention_mask: Attention mask of shape (batch_size, seq_len) where 1</span>
</span><span data-line="731"><span class="sd">                indicates valid tokens and 0 indicates padding.</span>
</span><span data-line="732"><span class="sd">            *args: Additional positional arguments passed to the transformer.</span>
</span><span data-line="733"><span class="sd">            **kwargs: Additional keyword arguments including:</span>
</span><span data-line="734"><span class="sd">                - packing_config: Optional InferencePackingConfig for efficient batching.</span>
</span><span data-line="735"><span class="sd">                - pair_attention_mask: Optional pairwise attention mask for packed sequences.</span>
</span><span data-line="736">
</span><span data-line="737"><span class="sd">        Returns:</span>
</span><span data-line="738"><span class="sd">            Token embeddings of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="739"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="740">        <span class="n">packing_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">InferencePackingConfig</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;packing_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="741">        <span class="n">pair_attention_mask</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;pair_attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="742">
</span><span data-line="743">        <span class="k">if</span> <span class="p">(</span>
</span><span data-line="744">            <span class="n">packing_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span data-line="745">            <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span>
</span><span data-line="746">            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span data-line="747">            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span data-line="748">            <span class="ow">and</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span>
</span><span data-line="749">        <span class="p">):</span>
</span><span data-line="750">            <span class="n">token_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_with_packing</span><span class="p">(</span>
</span><span data-line="751">                <span class="n">input_ids</span><span class="p">,</span>
</span><span data-line="752">                <span class="n">attention_mask</span><span class="p">,</span>
</span><span data-line="753">                <span class="n">packing_config</span><span class="p">,</span>
</span><span data-line="754">                <span class="n">pair_attention_mask</span><span class="p">,</span>
</span><span data-line="755">                <span class="o">*</span><span class="n">args</span><span class="p">,</span>
</span><span data-line="756">                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span data-line="757">            <span class="p">)</span>
</span><span data-line="758">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="759">            <span class="n">bert_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
</span><span data-line="760">            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="761">                <span class="n">bert_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">attention_mask</span>
</span><span data-line="762">            <span class="k">if</span> <span class="n">pair_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="763">                <span class="n">bert_kwargs</span><span class="p">[</span><span class="s2">&quot;pair_attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pair_attention_mask</span>
</span><span data-line="764">            <span class="n">token_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert_layer</span><span class="p">(</span>
</span><span data-line="765">                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
</span><span data-line="766">                <span class="o">**</span><span class="n">bert_kwargs</span><span class="p">,</span>
</span><span data-line="767">            <span class="p">)</span>
</span><span data-line="768">
</span><span data-line="769">        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;projection&quot;</span><span class="p">):</span>
</span><span data-line="770">            <span class="n">token_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">token_embeddings</span><span class="p">)</span>
</span><span data-line="771">        <span class="k">return</span> <span class="n">token_embeddings</span></div>

</span><span data-line="772">
</span><span data-line="773">    <span class="k">def</span><span class="w"> </span><span class="nf">_encode_with_packing</span><span class="p">(</span>
</span><span data-line="774">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="775">        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="776">        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="777">        <span class="n">packing_config</span><span class="p">:</span> <span class="n">InferencePackingConfig</span><span class="p">,</span>
</span><span data-line="778">        <span class="n">pair_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span data-line="779">        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
</span><span data-line="780">        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
</span><span data-line="781">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span data-line="782"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Encodes sequences using inference-time packing for efficiency.</span>
</span><span data-line="783">
</span><span data-line="784"><span class="sd">        Packs multiple variable-length sequences into fewer, more efficient batches</span>
</span><span data-line="785"><span class="sd">        to maximize GPU utilization during inference. Short sequences are combined</span>
</span><span data-line="786"><span class="sd">        into single packed sequences.</span>
</span><span data-line="787">
</span><span data-line="788"><span class="sd">        Args:</span>
</span><span data-line="789"><span class="sd">            input_ids: Input token IDs of shape (batch_size, seq_len).</span>
</span><span data-line="790"><span class="sd">            attention_mask: Attention mask of shape (batch_size, seq_len).</span>
</span><span data-line="791"><span class="sd">            packing_config: Configuration for packing behavior.</span>
</span><span data-line="792"><span class="sd">            pair_attention_mask: Optional pairwise attention mask.</span>
</span><span data-line="793"><span class="sd">            *args: Additional positional arguments.</span>
</span><span data-line="794"><span class="sd">            **kwargs: Additional keyword arguments.</span>
</span><span data-line="795">
</span><span data-line="796"><span class="sd">        Returns:</span>
</span><span data-line="797"><span class="sd">            Token embeddings of shape (batch_size, seq_len, hidden_size) with</span>
</span><span data-line="798"><span class="sd">            proper unpacking to restore original batch structure.</span>
</span><span data-line="799"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="800">        <span class="n">lengths</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</span><span data-line="801">        <span class="n">seq_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span><span data-line="802">        <span class="k">if</span> <span class="ow">not</span> <span class="n">lengths</span> <span class="ow">or</span> <span class="nb">all</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">ln</span><span class="p">)</span> <span class="o">==</span> <span class="n">seq_len</span> <span class="k">for</span> <span class="n">ln</span> <span class="ow">in</span> <span class="n">lengths</span><span class="p">):</span>
</span><span data-line="803">            <span class="n">bert_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
</span><span data-line="804">            <span class="n">bert_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">attention_mask</span>
</span><span data-line="805">            <span class="k">if</span> <span class="n">pair_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="806">                <span class="n">bert_kwargs</span><span class="p">[</span><span class="s2">&quot;pair_attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pair_attention_mask</span>
</span><span data-line="807">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert_layer</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">bert_kwargs</span><span class="p">)</span>
</span><span data-line="808">
</span><span data-line="809">        <span class="n">requests</span> <span class="o">=</span> <span class="p">[]</span>
</span><span data-line="810">        <span class="k">for</span> <span class="n">row</span><span class="p">,</span> <span class="n">length</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
</span><span data-line="811">            <span class="k">if</span> <span class="n">length</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span data-line="812">                <span class="n">requests</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="p">[]})</span>
</span><span data-line="813">            <span class="k">else</span><span class="p">:</span>
</span><span data-line="814">                <span class="n">requests</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">row</span><span class="p">[:</span><span class="n">length</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()})</span>
</span><span data-line="815">
</span><span data-line="816">        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert_layer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
</span><span data-line="817">        <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="818">            <span class="n">pad_token_id</span> <span class="o">=</span> <span class="mi">0</span>
</span><span data-line="819">
</span><span data-line="820">        <span class="n">packed</span> <span class="o">=</span> <span class="n">pack_requests</span><span class="p">(</span><span class="n">requests</span><span class="p">,</span> <span class="n">packing_config</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">)</span>
</span><span data-line="821">
</span><span data-line="822">        <span class="n">device</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">device</span>
</span><span data-line="823">        <span class="n">packed_ids</span> <span class="o">=</span> <span class="n">packed</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="824">        <span class="n">packed_mask</span> <span class="o">=</span> <span class="n">packed</span><span class="o">.</span><span class="n">pair_attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="825">        <span class="n">packed_fallback</span> <span class="o">=</span> <span class="n">packed</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="826">
</span><span data-line="827">        <span class="n">attn_to_use</span> <span class="o">=</span> <span class="n">packed_mask</span> <span class="k">if</span> <span class="n">packed_mask</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">else</span> <span class="n">packed_fallback</span>
</span><span data-line="828">        <span class="n">bert_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
</span><span data-line="829">
</span><span data-line="830">        <span class="k">if</span> <span class="n">packed_mask</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>
</span><span data-line="831">            <span class="n">bert_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">packed_fallback</span>
</span><span data-line="832">            <span class="n">bert_kwargs</span><span class="p">[</span><span class="s2">&quot;pair_attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">packed_mask</span>
</span><span data-line="833">        <span class="k">else</span><span class="p">:</span>
</span><span data-line="834">            <span class="n">bert_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">attn_to_use</span>
</span><span data-line="835">
</span><span data-line="836">        <span class="n">token_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert_layer</span><span class="p">(</span>
</span><span data-line="837">            <span class="n">input_ids</span><span class="o">=</span><span class="n">packed_ids</span><span class="p">,</span>
</span><span data-line="838">            <span class="o">**</span><span class="n">bert_kwargs</span><span class="p">,</span>
</span><span data-line="839">        <span class="p">)</span>
</span><span data-line="840">
</span><span data-line="841">        <span class="n">unpacked</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="n">unpack_spans</span><span class="p">(</span><span class="n">token_embeddings</span><span class="p">,</span> <span class="n">packed</span><span class="p">)</span>
</span><span data-line="842">        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">token_embeddings</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span data-line="843">        <span class="n">batch</span><span class="p">,</span> <span class="n">seq</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span><span data-line="844">        <span class="n">output</span> <span class="o">=</span> <span class="n">token_embeddings</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span><span data-line="845">        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">unpacked</span><span class="p">):</span>
</span><span data-line="846">            <span class="n">tgt_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span><span data-line="847">            <span class="k">if</span> <span class="n">tgt_len</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span data-line="848">                <span class="k">continue</span>
</span><span data-line="849">            <span class="n">output</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="n">tgt_len</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span>
</span><span data-line="850">        <span class="k">return</span> <span class="n">output</span>
</span><span data-line="851">
<div class="viewcode-block" id="Encoder.forward">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.Encoder.forward">[docs]</a>
</span><span data-line="852">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span data-line="853"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass through the encoder.</span>
</span><span data-line="854">
</span><span data-line="855"><span class="sd">        Args:</span>
</span><span data-line="856"><span class="sd">            *args: Positional arguments passed to encode_text.</span>
</span><span data-line="857"><span class="sd">            **kwargs: Keyword arguments passed to encode_text.</span>
</span><span data-line="858">
</span><span data-line="859"><span class="sd">        Returns:</span>
</span><span data-line="860"><span class="sd">            Token embeddings of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="861"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="862">        <span class="n">token_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_text</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span data-line="863">        <span class="k">return</span> <span class="n">token_embeddings</span></div>
</div>

</span><span data-line="864">
</span><span data-line="865">
<div class="viewcode-block" id="BiEncoder">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.BiEncoder">[docs]</a>
</span><span data-line="866"><span class="k">class</span><span class="w"> </span><span class="nc">BiEncoder</span><span class="p">(</span><span class="n">Encoder</span><span class="p">):</span>
</span><span data-line="867"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Bi-encoder architecture with separate encoders for text and labels.</span>
</span><span data-line="868">
</span><span data-line="869"><span class="sd">    This encoder processes text sequences and label sequences through potentially</span>
</span><span data-line="870"><span class="sd">    different transformer models, producing aligned representations for both. The</span>
</span><span data-line="871"><span class="sd">    label representations are mean-pooled to create fixed-size embeddings.</span>
</span><span data-line="872">
</span><span data-line="873"><span class="sd">    Attributes:</span>
</span><span data-line="874"><span class="sd">        bert_layer: Inherited text encoder from Encoder.</span>
</span><span data-line="875"><span class="sd">        projection: Inherited optional projection from Encoder.</span>
</span><span data-line="876"><span class="sd">        labels_encoder: Separate Transformer instance for encoding labels.</span>
</span><span data-line="877"><span class="sd">        labels_projection: Optional projection for label embeddings when label</span>
</span><span data-line="878"><span class="sd">            encoder hidden size differs from config.hidden_size.</span>
</span><span data-line="879"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="880">
<div class="viewcode-block" id="BiEncoder.__init__">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.BiEncoder.__init__">[docs]</a>
</span><span data-line="881">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span data-line="882">        <span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">from_pretrained</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">cache_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
</span><span data-line="883">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="884"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the bi-encoder.</span>
</span><span data-line="885">
</span><span data-line="886"><span class="sd">        Args:</span>
</span><span data-line="887"><span class="sd">            config: Configuration object containing model hyperparameters including</span>
</span><span data-line="888"><span class="sd">                `labels_encoder` (model name for label encoder) and `hidden_size`.</span>
</span><span data-line="889"><span class="sd">            from_pretrained: If True, loads pretrained weights for both encoders.</span>
</span><span data-line="890"><span class="sd">                Defaults to False.</span>
</span><span data-line="891"><span class="sd">            cache_dir: Optional directory for caching downloaded models. Defaults to None.</span>
</span><span data-line="892"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="893">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">from_pretrained</span><span class="p">)</span>
</span><span data-line="894">        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">labels_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="895">            <span class="bp">self</span><span class="o">.</span><span class="n">labels_encoder</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">labels_encoder</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">from_pretrained</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">)</span>
</span><span data-line="896">            <span class="n">le_hidden_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels_encoder</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
</span><span data-line="897">
</span><span data-line="898">            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">!=</span> <span class="n">le_hidden_size</span><span class="p">:</span>
</span><span data-line="899">                <span class="bp">self</span><span class="o">.</span><span class="n">labels_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">le_hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span></div>

</span><span data-line="900">
<div class="viewcode-block" id="BiEncoder.mean_pooling">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.BiEncoder.mean_pooling">[docs]</a>
</span><span data-line="901">    <span class="k">def</span><span class="w"> </span><span class="nf">mean_pooling</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_embeddings</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span data-line="902"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Applies mean pooling over token embeddings using attention mask.</span>
</span><span data-line="903">
</span><span data-line="904"><span class="sd">        Computes the average of token embeddings weighted by the attention mask,</span>
</span><span data-line="905"><span class="sd">        ignoring padded positions.</span>
</span><span data-line="906">
</span><span data-line="907"><span class="sd">        Args:</span>
</span><span data-line="908"><span class="sd">            token_embeddings: Token-level embeddings of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="909"><span class="sd">            attention_mask: Binary mask of shape (batch_size, seq_len) where 1 indicates</span>
</span><span data-line="910"><span class="sd">                valid tokens and 0 indicates padding.</span>
</span><span data-line="911">
</span><span data-line="912"><span class="sd">        Returns:</span>
</span><span data-line="913"><span class="sd">            Pooled embeddings of shape (batch_size, hidden_size).</span>
</span><span data-line="914"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="915">        <span class="n">input_mask_expanded</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">token_embeddings</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span data-line="916">        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">token_embeddings</span> <span class="o">*</span> <span class="n">input_mask_expanded</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">input_mask_expanded</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span></div>

</span><span data-line="917">
<div class="viewcode-block" id="BiEncoder.encode_labels">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.BiEncoder.encode_labels">[docs]</a>
</span><span data-line="918">    <span class="k">def</span><span class="w"> </span><span class="nf">encode_labels</span><span class="p">(</span>
</span><span data-line="919">        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
</span><span data-line="920">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span data-line="921"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Encodes label sequences into fixed-size embeddings.</span>
</span><span data-line="922">
</span><span data-line="923"><span class="sd">        Processes labels through the dedicated labels encoder and applies mean pooling</span>
</span><span data-line="924"><span class="sd">        to produce sentence-level representations.</span>
</span><span data-line="925">
</span><span data-line="926"><span class="sd">        Args:</span>
</span><span data-line="927"><span class="sd">            input_ids: Label token IDs of shape (batch_size, seq_len).</span>
</span><span data-line="928"><span class="sd">            attention_mask: Attention mask of shape (batch_size, seq_len).</span>
</span><span data-line="929"><span class="sd">            *args: Additional positional arguments.</span>
</span><span data-line="930"><span class="sd">            **kwargs: Additional keyword arguments (packing_config and pair_attention_mask</span>
</span><span data-line="931"><span class="sd">                are removed as they&#39;re not supported for labels).</span>
</span><span data-line="932">
</span><span data-line="933"><span class="sd">        Returns:</span>
</span><span data-line="934"><span class="sd">            Pooled label embeddings of shape (batch_size, hidden_size).</span>
</span><span data-line="935"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="936">        <span class="n">label_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
</span><span data-line="937">        <span class="n">label_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;packing_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="938">        <span class="n">label_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;pair_attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="939">        <span class="n">label_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">attention_mask</span>
</span><span data-line="940">        <span class="n">labels_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels_encoder</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">label_kwargs</span><span class="p">)</span>
</span><span data-line="941">        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;labels_projection&quot;</span><span class="p">):</span>
</span><span data-line="942">            <span class="n">labels_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels_projection</span><span class="p">(</span><span class="n">labels_embeddings</span><span class="p">)</span>
</span><span data-line="943">        <span class="n">labels_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_pooling</span><span class="p">(</span><span class="n">labels_embeddings</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
</span><span data-line="944">        <span class="k">return</span> <span class="n">labels_embeddings</span></div>

</span><span data-line="945">
<div class="viewcode-block" id="BiEncoder.forward">
<a class="viewcode-back" href="../../../api/gliner.modeling.encoder.html#gliner.modeling.encoder.BiEncoder.forward">[docs]</a>
</span><span data-line="946">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span data-line="947">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="948">        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="949">        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="950">        <span class="n">labels_input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="951">        <span class="n">labels_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="952">        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
</span><span data-line="953">        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
</span><span data-line="954">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span data-line="955"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass through the bi-encoder.</span>
</span><span data-line="956">
</span><span data-line="957"><span class="sd">        Encodes both text sequences (token-level) and label sequences (pooled) to</span>
</span><span data-line="958"><span class="sd">        produce aligned representations.</span>
</span><span data-line="959">
</span><span data-line="960"><span class="sd">        Args:</span>
</span><span data-line="961"><span class="sd">            input_ids: Text token IDs of shape (batch_size, seq_len).</span>
</span><span data-line="962"><span class="sd">            attention_mask: Text attention mask of shape (batch_size, seq_len).</span>
</span><span data-line="963"><span class="sd">            labels_input_ids: Label token IDs of shape (batch_size, label_seq_len).</span>
</span><span data-line="964"><span class="sd">            labels_attention_mask: Label attention mask of shape (batch_size, label_seq_len).</span>
</span><span data-line="965"><span class="sd">            *args: Additional positional arguments.</span>
</span><span data-line="966"><span class="sd">            **kwargs: Additional keyword arguments.</span>
</span><span data-line="967">
</span><span data-line="968"><span class="sd">        Returns:</span>
</span><span data-line="969"><span class="sd">            A tuple containing:</span>
</span><span data-line="970"><span class="sd">                - token_embeddings: Text embeddings of shape (batch_size, seq_len, hidden_size).</span>
</span><span data-line="971"><span class="sd">                - labels_embeddings: Pooled label embeddings of shape (batch_size, hidden_size).</span>
</span><span data-line="972"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="973">        <span class="n">token_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_text</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span data-line="974">
</span><span data-line="975">        <span class="n">labels_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_labels</span><span class="p">(</span><span class="n">labels_input_ids</span><span class="p">,</span> <span class="n">labels_attention_mask</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span data-line="976">        <span class="k">return</span> <span class="n">token_embeddings</span><span class="p">,</span> <span class="n">labels_embeddings</span></div>
</div>

</span></pre></div>
        </article><button class="back-to-top" type="button">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
  </svg>
  <span>Back to top</span>
</button><div class="navigation flex print:hidden"></div></div>
    </div>
  </main>
</div>
<footer class="sy-foot">
  <div class="sy-foot-inner sy-container mx-auto">
    <div class="sy-foot-reserved md:flex justify-between items-center">
      <div class="sy-foot-copyright"><p>2025, GLiNER community</p>
  
  <p>
    Made with
    
    <a href="https://www.sphinx-doc.org/">Sphinx</a> and
    
    <a href="https://shibuya.lepture.com">Shibuya theme</a>.
  </p>
</div>
      <div class="sy-foot-socials"></div>
    </div>
  </div>
</footer>
      <script src="../../../_static/documentation_options.js?v=dc91f075"></script>
      <script src="../../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../_static/shibuya.js?v=9b0e4dde"></script></body>
</html>