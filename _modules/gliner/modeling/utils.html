<!DOCTYPE html>
<html lang="en" data-accent-color="violet" data-content_root="../../../">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>gliner.modeling.utils - Home 0.2.24 documentation</title><link rel="index" title="Index" href="../../../genindex.html" /><link rel="search" title="Search" href="../../../search.html" /><script>
    function setColorMode(t){let e=document.documentElement;e.setAttribute("data-color-mode",t);let a=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,s=t;"auto"===t&&(s=a?"dark":"light"),"light"===s?(e.classList.remove("dark"),e.classList.add("light")):(e.classList.remove("light"),e.classList.add("dark"))}
    setColorMode(localStorage._theme||"auto");
  </script><link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=e1a1ceaf" />
    <link rel="stylesheet" type="text/css" href="../../../_static/shibuya.css?v=d140fbf8" />
    <link media="print" rel="stylesheet" type="text/css" href="../../../_static/print.css?v=20ff2c19" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
:root {
  --sy-f-text: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
  --sy-f-heading: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
}
</style>
    <meta property="og:type" content="website"/><meta property="og:title" content="gliner.modeling.utils"/>
<meta name="twitter:card" content="summary"/>
  </head>
<body><div class="sy-head">
  <div class="sy-head-blur"></div>
  <div class="sy-head-inner sy-container mx-auto">
    <a class="sy-head-brand" href="../../../index.html">
      
      
      <strong>Home</strong>
    </a>
    <div class="sy-head-nav" id="head-nav">
      <nav class="sy-head-links"></nav>
      <div class="sy-head-extra flex items-center print:hidden"><form class="searchbox flex items-center" action="../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <kbd>/</kbd>
</form><div class="sy-head-socials"></div></div>
    </div>
    <div class="sy-head-actions flex items-center shrink-0 print:hidden"><button class="js-theme theme-switch flex items-center"
data-aria-auto="Switch to light color mode"
data-aria-light="Switch to dark color mode"
data-aria-dark="Switch to auto color mode">
<i class="i-lucide theme-icon"></i>
</button><button class="md:hidden flex items-center js-menu" aria-label="Menu" type="button" aria-controls="head-nav" aria-expanded="false">
        <div class="hamburger">
          <span class="hamburger_1"></span>
          <span class="hamburger_2"></span>
          <span class="hamburger_3"></span>
        </div>
      </button>
    </div>
  </div>
</div>
<div class="sy-page sy-container flex mx-auto">
  <aside id="lside" class="sy-lside md:w-72 md:shrink-0 print:hidden">
    <div class="sy-lside-inner md:sticky">
      <div class="sy-scrollbar p-6">
        <div class="globaltoc" data-expand-depth="0"><p class="caption" role="heading" aria-level="3"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../intro.html">Introduction to ðŸ‘‘ GLiNER</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../instalation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../usage.html">Advanced Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../configs.html">Components &amp; Configs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../training.html">Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architectures.html">Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../convert_to_onnx.html">ONNX Export &amp; Deployment</a></li>
</ul>
<p class="caption" role="heading" aria-level="3"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.model.html">gliner.model module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.config.html">gliner.config module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.training.html">gliner.training package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.training.trainer.html">gliner.training.trainer module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.modeling.html">gliner.modeling package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.multitask.html">gliner.modeling.multitask package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.modeling.multitask.relations_layers.html">gliner.modeling.multitask.relations_layers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.modeling.multitask.triples_layers.html">gliner.modeling.multitask.triples_layers module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.base.html">gliner.modeling.base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.decoder.html">gliner.modeling.decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.encoder.html">gliner.modeling.encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.layers.html">gliner.modeling.layers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.loss_functions.html">gliner.modeling.loss_functions module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.outputs.html">gliner.modeling.outputs module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.scorers.html">gliner.modeling.scorers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.span_rep.html">gliner.modeling.span_rep module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.utils.html">gliner.modeling.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.data_processing.html">gliner.data_processing package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.collator.html">gliner.data_processing.collator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.processor.html">gliner.data_processing.processor module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.tokenizer.html">gliner.data_processing.tokenizer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.utils.html">gliner.data_processing.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.evaluation.html">gliner.evaluation package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.evaluate_ner.html">gliner.evaluation.evaluate_ner module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.evaluator.html">gliner.evaluation.evaluator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.utils.html">gliner.evaluation.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.onnx.html">gliner.onnx package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.onnx.model.html">gliner.onnx.model module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.decoding.html">gliner.decoding package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.trie.html">gliner.decoding.trie package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.decoding.trie.labels_trie.html">gliner.decoding.trie.labels_trie module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.decoding.trie.python_labels_trie.html">gliner.decoding.trie.python_labels_trie module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.decoder.html">gliner.decoding.decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.utils.html">gliner.decoding.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.utils.html">gliner.utils module</a></li>
</ul>

        </div>
      </div>
    </div>
  </aside>
  <div class="lside-overlay js-menu" role="button" aria-label="Close left sidebar" aria-controls="lside" aria-expanded="false"></div>
  <aside id="rside" class="sy-rside pb-3 w-64 shrink-0 order-last">
    <button class="rside-close js-menu xl:hidden" aria-label="Close Table of Contents" type="button" aria-controls="rside" aria-expanded="false">
      <i class="i-lucide close"></i>
    </button>
    <div class="sy-scrollbar sy-rside-inner px-6 xl:top-16 xl:sticky xl:pl-0 pt-6 pb-4"><div id="ethical-ad-placement" data-ea-publisher="readthedocs"></div></div>
  </aside>
  <div class="rside-overlay js-menu" role="button" aria-label="Close Table of Contents" aria-controls="rside" aria-expanded="false"></div>
  <main class="sy-main w-full max-sm:max-w-full print:pt-6">
<div class="sy-breadcrumbs" role="navigation">
  <div class="sy-breadcrumbs-inner flex items-center">
    <div class="md:hidden mr-3">
      <button class="js-menu" aria-label="Menu" type="button" aria-controls="lside" aria-expanded="false">
        <i class="i-lucide menu"></i>
      </button>
    </div>
    <ol class="flex-1" itemscope itemtype="https://schema.org/BreadcrumbList"><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../../../index.html"><span itemprop="name">Home</span></a>
        <span>/</span>
        <meta itemprop="position" content="1" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../../index.html"><span itemprop="name">Module code</span></a>
        <span>/</span>
        <meta itemprop="position" content="2" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <strong itemprop="name">gliner.modeling.utils</strong>
        <meta itemprop="position" content="3" />
      </li></ol>
    <div class="xl:hidden ml-1">
      <button class="js-menu" aria-label="Show table of contents" type="button" aria-controls="rside"
        aria-expanded="false">
        <i class="i-lucide outdent"></i>
      </button>
    </div>
  </div>
</div><div class="flex flex-col break-words justify-between">
      <div class="min-w-0 max-w-6xl px-6 pb-6 pt-8 xl:px-12">
        <article class="yue" role="main">
          <h1>Source code for gliner.modeling.utils</h1><div class="highlight"><pre>
<span></span><span data-line="1"><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tuple</span>
</span><span data-line="2">
</span><span data-line="3"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span data-line="4">
</span><span data-line="5">
<div class="viewcode-block" id="extract_word_embeddings">
<a class="viewcode-back" href="../../../api/gliner.modeling.utils.html#gliner.modeling.utils.extract_word_embeddings">[docs]</a>
</span><span data-line="6"><span class="k">def</span><span class="w"> </span><span class="nf">extract_word_embeddings</span><span class="p">(</span>
</span><span data-line="7">    <span class="n">token_embeds</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="8">    <span class="n">words_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="9">    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="10">    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span data-line="11">    <span class="n">max_text_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span data-line="12">    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span data-line="13">    <span class="n">text_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="14"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span data-line="15"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Extract word-level embeddings from subword token embeddings.</span>
</span><span data-line="16">
</span><span data-line="17"><span class="sd">    Maps subword token embeddings back to word-level embeddings using a word mask</span>
</span><span data-line="18"><span class="sd">    that indicates which subword token corresponds to which word. Only the first</span>
</span><span data-line="19"><span class="sd">    subword token of each word is typically used for the word representation.</span>
</span><span data-line="20">
</span><span data-line="21"><span class="sd">    This is essential for span-based NER where predictions are made at the word</span>
</span><span data-line="22"><span class="sd">    level but the transformer operates on subword tokens.</span>
</span><span data-line="23">
</span><span data-line="24"><span class="sd">    Args:</span>
</span><span data-line="25"><span class="sd">        token_embeds: Subword token embeddings from transformer.</span>
</span><span data-line="26"><span class="sd">            Shape: (batch_size, seq_len, embed_dim)</span>
</span><span data-line="27"><span class="sd">        words_mask: Mask mapping subword positions to word indices. Non-zero values</span>
</span><span data-line="28"><span class="sd">            indicate the word index (1-indexed). Zero values are special tokens or</span>
</span><span data-line="29"><span class="sd">            continuation subwords to ignore.</span>
</span><span data-line="30"><span class="sd">            Shape: (batch_size, seq_len)</span>
</span><span data-line="31"><span class="sd">        attention_mask: Standard attention mask from tokenizer.</span>
</span><span data-line="32"><span class="sd">            Shape: (batch_size, seq_len)</span>
</span><span data-line="33"><span class="sd">        batch_size: Size of the batch.</span>
</span><span data-line="34"><span class="sd">        max_text_length: Maximum number of words across all examples in batch.</span>
</span><span data-line="35"><span class="sd">        embed_dim: Embedding dimension size.</span>
</span><span data-line="36"><span class="sd">        text_lengths: Number of words in each example.</span>
</span><span data-line="37"><span class="sd">            Shape: (batch_size, 1) or (batch_size,)</span>
</span><span data-line="38">
</span><span data-line="39"><span class="sd">    Returns:</span>
</span><span data-line="40"><span class="sd">        Tuple containing:</span>
</span><span data-line="41"><span class="sd">            - words_embedding: Word-level embeddings extracted from token embeddings.</span>
</span><span data-line="42"><span class="sd">              Shape: (batch_size, max_text_length, embed_dim)</span>
</span><span data-line="43"><span class="sd">            - mask: Boolean mask indicating valid word positions (True) vs padding (False).</span>
</span><span data-line="44"><span class="sd">              Shape: (batch_size, max_text_length)</span>
</span><span data-line="45"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="46">    <span class="n">words_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span data-line="47">        <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_text_length</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">token_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">token_embeds</span><span class="o">.</span><span class="n">device</span>
</span><span data-line="48">    <span class="p">)</span>
</span><span data-line="49">
</span><span data-line="50">    <span class="c1"># Find positions where words_mask &gt; 0 (actual word positions)</span>
</span><span data-line="51">    <span class="n">batch_indices</span><span class="p">,</span> <span class="n">word_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">words_mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span><span data-line="52">
</span><span data-line="53">    <span class="c1"># Convert 1-indexed word mask to 0-indexed positions</span>
</span><span data-line="54">    <span class="n">target_word_idx</span> <span class="o">=</span> <span class="n">words_mask</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">word_idx</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
</span><span data-line="55">
</span><span data-line="56">    <span class="c1"># Copy token embeddings to word positions</span>
</span><span data-line="57">    <span class="n">words_embedding</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">target_word_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_embeds</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">word_idx</span><span class="p">]</span>
</span><span data-line="58">
</span><span data-line="59">    <span class="c1"># Create mask for valid word positions</span>
</span><span data-line="60">    <span class="n">aranged_word_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_text_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">token_embeds</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
</span><span data-line="61">        <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
</span><span data-line="62">    <span class="p">)</span>
</span><span data-line="63">
</span><span data-line="64">    <span class="n">mask</span> <span class="o">=</span> <span class="n">aranged_word_idx</span> <span class="o">&lt;</span> <span class="n">text_lengths</span>
</span><span data-line="65">    <span class="k">return</span> <span class="n">words_embedding</span><span class="p">,</span> <span class="n">mask</span></div>

</span><span data-line="66">
</span><span data-line="67">
<div class="viewcode-block" id="extract_prompt_features">
<a class="viewcode-back" href="../../../api/gliner.modeling.utils.html#gliner.modeling.utils.extract_prompt_features">[docs]</a>
</span><span data-line="68"><span class="k">def</span><span class="w"> </span><span class="nf">extract_prompt_features</span><span class="p">(</span>
</span><span data-line="69">    <span class="n">class_token_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span data-line="70">    <span class="n">token_embeds</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="71">    <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="72">    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="73">    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span data-line="74">    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span data-line="75">    <span class="n">embed_ent_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span data-line="76"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span data-line="77"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Extract prompt/entity type embeddings from special class tokens.</span>
</span><span data-line="78">
</span><span data-line="79"><span class="sd">    Extracts embeddings for entity types or other prompt elements that are marked</span>
</span><span data-line="80"><span class="sd">    with special class tokens (e.g., [ENT] tokens). These embeddings represent</span>
</span><span data-line="81"><span class="sd">    the entity types that the model should extract.</span>
</span><span data-line="82">
</span><span data-line="83"><span class="sd">    In prompt-based NER, the input is typically:</span>
</span><span data-line="84"><span class="sd">        [ENT] Person [ENT] Organization [SEP] John works at Google</span>
</span><span data-line="85">
</span><span data-line="86"><span class="sd">    This function extracts the embeddings corresponding to the [ENT] tokens</span>
</span><span data-line="87"><span class="sd">    (or the tokens immediately after them if embed_ent_token=False).</span>
</span><span data-line="88">
</span><span data-line="89"><span class="sd">    Args:</span>
</span><span data-line="90"><span class="sd">        class_token_index: Token ID of the special class token to extract</span>
</span><span data-line="91"><span class="sd">            (e.g., token ID for [ENT]).</span>
</span><span data-line="92"><span class="sd">        token_embeds: Token embeddings from transformer.</span>
</span><span data-line="93"><span class="sd">            Shape: (batch_size, seq_len, embed_dim)</span>
</span><span data-line="94"><span class="sd">        input_ids: Token IDs from tokenizer.</span>
</span><span data-line="95"><span class="sd">            Shape: (batch_size, seq_len)</span>
</span><span data-line="96"><span class="sd">        attention_mask: Standard attention mask from tokenizer.</span>
</span><span data-line="97"><span class="sd">            Shape: (batch_size, seq_len)</span>
</span><span data-line="98"><span class="sd">        batch_size: Size of the batch.</span>
</span><span data-line="99"><span class="sd">        embed_dim: Embedding dimension size.</span>
</span><span data-line="100"><span class="sd">        embed_ent_token: If True, use the [ENT] token embedding itself.</span>
</span><span data-line="101"><span class="sd">            If False, use the embedding of the token immediately after [ENT]</span>
</span><span data-line="102"><span class="sd">            (i.e., the entity type name token). Default: True.</span>
</span><span data-line="103">
</span><span data-line="104"><span class="sd">    Returns:</span>
</span><span data-line="105"><span class="sd">        Tuple containing:</span>
</span><span data-line="106"><span class="sd">            - prompts_embedding: Embeddings for each prompt/entity type.</span>
</span><span data-line="107"><span class="sd">              Shape: (batch_size, max_num_types, embed_dim)</span>
</span><span data-line="108"><span class="sd">              where max_num_types is the maximum number of entity types</span>
</span><span data-line="109"><span class="sd">              across examples in the batch.</span>
</span><span data-line="110"><span class="sd">            - prompts_embedding_mask: Mask indicating valid prompt positions</span>
</span><span data-line="111"><span class="sd">              (True) vs padding (False).</span>
</span><span data-line="112"><span class="sd">              Shape: (batch_size, max_num_types)</span>
</span><span data-line="113"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="114">    <span class="c1"># Find all positions with the class token</span>
</span><span data-line="115">    <span class="n">class_token_mask</span> <span class="o">=</span> <span class="n">input_ids</span> <span class="o">==</span> <span class="n">class_token_index</span>
</span><span data-line="116">    <span class="n">num_class_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_token_mask</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span data-line="117">
</span><span data-line="118">    <span class="c1"># Maximum number of class tokens across batch</span>
</span><span data-line="119">    <span class="n">max_embed_dim</span> <span class="o">=</span> <span class="n">num_class_tokens</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</span><span data-line="120">    <span class="n">aranged_class_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_embed_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">token_embeds</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
</span><span data-line="121">        <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
</span><span data-line="122">    <span class="p">)</span>
</span><span data-line="123">
</span><span data-line="124">    <span class="c1"># Find valid positions (not padding)</span>
</span><span data-line="125">    <span class="n">batch_indices</span><span class="p">,</span> <span class="n">target_class_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">aranged_class_idx</span> <span class="o">&lt;</span> <span class="n">num_class_tokens</span><span class="p">)</span>
</span><span data-line="126">    <span class="n">_</span><span class="p">,</span> <span class="n">class_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">class_token_mask</span><span class="p">)</span>
</span><span data-line="127">
</span><span data-line="128">    <span class="c1"># Optionally shift to token after [ENT] (the entity type name)</span>
</span><span data-line="129">    <span class="k">if</span> <span class="ow">not</span> <span class="n">embed_ent_token</span><span class="p">:</span>
</span><span data-line="130">        <span class="n">class_indices</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span data-line="131">
</span><span data-line="132">    <span class="c1"># Initialize prompt embeddings tensor</span>
</span><span data-line="133">    <span class="n">prompts_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span data-line="134">        <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">token_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">token_embeds</span><span class="o">.</span><span class="n">device</span>
</span><span data-line="135">    <span class="p">)</span>
</span><span data-line="136">
</span><span data-line="137">    <span class="c1"># Create mask for valid (non-padded) positions</span>
</span><span data-line="138">    <span class="n">prompts_embedding_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">aranged_class_idx</span> <span class="o">&lt;</span> <span class="n">num_class_tokens</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span data-line="139">
</span><span data-line="140">    <span class="c1"># Extract embeddings at class token positions</span>
</span><span data-line="141">    <span class="n">prompts_embedding</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">target_class_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_embeds</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">class_indices</span><span class="p">]</span>
</span><span data-line="142">
</span><span data-line="143">    <span class="k">return</span> <span class="n">prompts_embedding</span><span class="p">,</span> <span class="n">prompts_embedding_mask</span></div>

</span><span data-line="144">
</span><span data-line="145">
<div class="viewcode-block" id="extract_prompt_features_and_word_embeddings">
<a class="viewcode-back" href="../../../api/gliner.modeling.utils.html#gliner.modeling.utils.extract_prompt_features_and_word_embeddings">[docs]</a>
</span><span data-line="146"><span class="k">def</span><span class="w"> </span><span class="nf">extract_prompt_features_and_word_embeddings</span><span class="p">(</span>
</span><span data-line="147">    <span class="n">class_token_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span data-line="148">    <span class="n">token_embeds</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="149">    <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="150">    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="151">    <span class="n">text_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="152">    <span class="n">words_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="153">    <span class="n">embed_ent_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span data-line="154">    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span data-line="155"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span data-line="156"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Extract both prompt embeddings and word embeddings in one call.</span>
</span><span data-line="157">
</span><span data-line="158"><span class="sd">    Convenience function that combines extract_prompt_features and</span>
</span><span data-line="159"><span class="sd">    extract_word_embeddings to get both prompt/entity type embeddings</span>
</span><span data-line="160"><span class="sd">    and word-level text embeddings from a single set of token embeddings.</span>
</span><span data-line="161">
</span><span data-line="162"><span class="sd">    This is the typical use case for prompt-based NER where you need both:</span>
</span><span data-line="163"><span class="sd">    1. Entity type embeddings (from prompt tokens like [ENT])</span>
</span><span data-line="164"><span class="sd">    2. Word-level text embeddings (from the actual text tokens)</span>
</span><span data-line="165">
</span><span data-line="166"><span class="sd">    Args:</span>
</span><span data-line="167"><span class="sd">        class_token_index: Token ID of the special class token (e.g., [ENT]).</span>
</span><span data-line="168"><span class="sd">        token_embeds: Token embeddings from transformer.</span>
</span><span data-line="169"><span class="sd">            Shape: (batch_size, seq_len, embed_dim)</span>
</span><span data-line="170"><span class="sd">        input_ids: Token IDs from tokenizer.</span>
</span><span data-line="171"><span class="sd">            Shape: (batch_size, seq_len)</span>
</span><span data-line="172"><span class="sd">        attention_mask: Standard attention mask from tokenizer.</span>
</span><span data-line="173"><span class="sd">            Shape: (batch_size, seq_len)</span>
</span><span data-line="174"><span class="sd">        text_lengths: Number of words in each example.</span>
</span><span data-line="175"><span class="sd">            Shape: (batch_size, 1) or (batch_size,)</span>
</span><span data-line="176"><span class="sd">        words_mask: Mask mapping subword positions to word indices.</span>
</span><span data-line="177"><span class="sd">            Shape: (batch_size, seq_len)</span>
</span><span data-line="178"><span class="sd">        embed_ent_token: If True, use [ENT] token embedding. If False,</span>
</span><span data-line="179"><span class="sd">            use the token after [ENT] (the entity type name). Default: True.</span>
</span><span data-line="180"><span class="sd">        **kwargs: Additional keyword arguments passed to extract_prompt_features.</span>
</span><span data-line="181">
</span><span data-line="182"><span class="sd">    Returns:</span>
</span><span data-line="183"><span class="sd">        Tuple containing:</span>
</span><span data-line="184"><span class="sd">            - prompts_embedding: Entity type embeddings.</span>
</span><span data-line="185"><span class="sd">              Shape: (batch_size, max_num_types, embed_dim)</span>
</span><span data-line="186"><span class="sd">            - prompts_embedding_mask: Mask for valid entity type positions.</span>
</span><span data-line="187"><span class="sd">              Shape: (batch_size, max_num_types)</span>
</span><span data-line="188"><span class="sd">            - words_embedding: Word-level text embeddings.</span>
</span><span data-line="189"><span class="sd">              Shape: (batch_size, max_text_length, embed_dim)</span>
</span><span data-line="190"><span class="sd">            - mask: Mask for valid word positions.</span>
</span><span data-line="191"><span class="sd">              Shape: (batch_size, max_text_length)</span>
</span><span data-line="192"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="193">    <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">token_embeds</span><span class="o">.</span><span class="n">shape</span>
</span><span data-line="194">    <span class="n">max_text_length</span> <span class="o">=</span> <span class="n">text_lengths</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</span><span data-line="195">
</span><span data-line="196">    <span class="c1"># Extract prompt/entity type embeddings</span>
</span><span data-line="197">    <span class="n">prompts_embedding</span><span class="p">,</span> <span class="n">prompts_embedding_mask</span> <span class="o">=</span> <span class="n">extract_prompt_features</span><span class="p">(</span>
</span><span data-line="198">        <span class="n">class_token_index</span><span class="p">,</span> <span class="n">token_embeds</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_ent_token</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
</span><span data-line="199">    <span class="p">)</span>
</span><span data-line="200">
</span><span data-line="201">    <span class="c1"># Extract word-level embeddings</span>
</span><span data-line="202">    <span class="n">words_embedding</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">extract_word_embeddings</span><span class="p">(</span>
</span><span data-line="203">        <span class="n">token_embeds</span><span class="p">,</span> <span class="n">words_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_text_length</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">text_lengths</span>
</span><span data-line="204">    <span class="p">)</span>
</span><span data-line="205">
</span><span data-line="206">    <span class="k">return</span> <span class="n">prompts_embedding</span><span class="p">,</span> <span class="n">prompts_embedding_mask</span><span class="p">,</span> <span class="n">words_embedding</span><span class="p">,</span> <span class="n">mask</span></div>

</span><span data-line="207">
</span><span data-line="208">
<div class="viewcode-block" id="build_entity_pairs">
<a class="viewcode-back" href="../../../api/gliner.modeling.utils.html#gliner.modeling.utils.build_entity_pairs">[docs]</a>
</span><span data-line="209"><span class="k">def</span><span class="w"> </span><span class="nf">build_entity_pairs</span><span class="p">(</span>
</span><span data-line="210">    <span class="n">adj</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="211">    <span class="n">span_rep</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span data-line="212">    <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
</span><span data-line="213"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span data-line="214"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Build entity pairs for relation extraction based on adjacency scores.</span>
</span><span data-line="215">
</span><span data-line="216"><span class="sd">    Extracts entity pairs (head, tail) where the adjacency score exceeds a</span>
</span><span data-line="217"><span class="sd">    threshold, and retrieves their corresponding embeddings. This is used in</span>
</span><span data-line="218"><span class="sd">    relation extraction to select which entity pairs should be classified for</span>
</span><span data-line="219"><span class="sd">    relation types.</span>
</span><span data-line="220">
</span><span data-line="221"><span class="sd">    The function considers ALL directed pairs (i,j) where iâ‰ j, not just the</span>
</span><span data-line="222"><span class="sd">    upper triangle, since relation direction matters (e.g., &quot;founded&quot; vs</span>
</span><span data-line="223"><span class="sd">    &quot;founded_by&quot; have opposite directions).</span>
</span><span data-line="224">
</span><span data-line="225"><span class="sd">    Args:</span>
</span><span data-line="226"><span class="sd">        adj: Adjacency matrix with scores or probabilities for entity pairs.</span>
</span><span data-line="227"><span class="sd">            Shape: (batch_size, num_entities, num_entities)</span>
</span><span data-line="228"><span class="sd">            The diagonal (self-pairs) is ignored. Values &gt; threshold indicate</span>
</span><span data-line="229"><span class="sd">            potential relations.</span>
</span><span data-line="230"><span class="sd">        span_rep: Entity/span embeddings for each entity in the batch.</span>
</span><span data-line="231"><span class="sd">            Shape: (batch_size, num_entities, embed_dim)</span>
</span><span data-line="232"><span class="sd">        threshold: Minimum adjacency score to consider a pair as a potential</span>
</span><span data-line="233"><span class="sd">            relation. Pairs with adj[i,j] &gt; threshold are kept. Default: 0.5.</span>
</span><span data-line="234">
</span><span data-line="235"><span class="sd">    Returns:</span>
</span><span data-line="236"><span class="sd">        Tuple containing:</span>
</span><span data-line="237"><span class="sd">            - pair_idx: Indices of (head, tail) entity pairs.</span>
</span><span data-line="238"><span class="sd">              Shape: (batch_size, max_pairs, 2)</span>
</span><span data-line="239"><span class="sd">              Values are entity indices, or -1 for padding positions.</span>
</span><span data-line="240"><span class="sd">            - pair_mask: Boolean mask indicating valid pairs (True) vs padding (False).</span>
</span><span data-line="241"><span class="sd">              Shape: (batch_size, max_pairs)</span>
</span><span data-line="242"><span class="sd">            - head_rep: Embeddings of head entities for each pair.</span>
</span><span data-line="243"><span class="sd">              Shape: (batch_size, max_pairs, embed_dim)</span>
</span><span data-line="244"><span class="sd">            - tail_rep: Embeddings of tail entities for each pair.</span>
</span><span data-line="245"><span class="sd">              Shape: (batch_size, max_pairs, embed_dim)</span>
</span><span data-line="246"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="247">    <span class="n">B</span><span class="p">,</span> <span class="n">E</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">adj</span><span class="o">.</span><span class="n">shape</span>
</span><span data-line="248">    <span class="n">device</span> <span class="o">=</span> <span class="n">adj</span><span class="o">.</span><span class="n">device</span>
</span><span data-line="249">    <span class="n">D</span> <span class="o">=</span> <span class="n">span_rep</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span data-line="250">
</span><span data-line="251">    <span class="c1"># Generate all possible (i, j) pairs where i != j</span>
</span><span data-line="252">    <span class="n">all_rows</span> <span class="o">=</span> <span class="p">[]</span>
</span><span data-line="253">    <span class="n">all_cols</span> <span class="o">=</span> <span class="p">[]</span>
</span><span data-line="254">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">E</span><span class="p">):</span>
</span><span data-line="255">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">E</span><span class="p">):</span>
</span><span data-line="256">            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
</span><span data-line="257">                <span class="n">all_rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</span><span data-line="258">                <span class="n">all_cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
</span><span data-line="259">
</span><span data-line="260">    <span class="n">rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">all_rows</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
</span><span data-line="261">    <span class="n">cols</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">all_cols</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
</span><span data-line="262">
</span><span data-line="263">    <span class="c1"># For each example in batch, find pairs exceeding threshold</span>
</span><span data-line="264">    <span class="n">batch_pair_lists</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span data-line="265">
</span><span data-line="266">    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
</span><span data-line="267">        <span class="n">sel</span> <span class="o">=</span> <span class="n">adj</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span>  <span class="c1"># Boolean mask for valid pairs</span>
</span><span data-line="268">        <span class="n">pairs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">rows</span><span class="p">[</span><span class="n">sel</span><span class="p">],</span> <span class="n">cols</span><span class="p">[</span><span class="n">sel</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (num_valid_pairs, 2)</span>
</span><span data-line="269">        <span class="n">batch_pair_lists</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
</span><span data-line="270">
</span><span data-line="271">    <span class="c1"># Find maximum number of pairs across batch (for padding)</span>
</span><span data-line="272">    <span class="n">N</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">batch_pair_lists</span><span class="p">)</span> <span class="k">if</span> <span class="n">batch_pair_lists</span> <span class="k">else</span> <span class="mi">0</span>
</span><span data-line="273">
</span><span data-line="274">    <span class="c1"># Handle case where no pairs exceed threshold</span>
</span><span data-line="275">    <span class="k">if</span> <span class="n">N</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span data-line="276">        <span class="n">pair_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="277">        <span class="n">pair_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="278">        <span class="n">head_rep</span> <span class="o">=</span> <span class="n">tail_rep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">span_rep</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="279">        <span class="k">return</span> <span class="n">pair_idx</span><span class="p">,</span> <span class="n">pair_mask</span><span class="p">,</span> <span class="n">head_rep</span><span class="p">,</span> <span class="n">tail_rep</span>
</span><span data-line="280">
</span><span data-line="281">    <span class="c1"># Initialize padded tensors</span>
</span><span data-line="282">    <span class="n">pair_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="283">    <span class="n">pair_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="284">
</span><span data-line="285">    <span class="c1"># Fill in valid pairs for each example</span>
</span><span data-line="286">    <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">pairs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch_pair_lists</span><span class="p">):</span>
</span><span data-line="287">        <span class="n">m</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span data-line="288">        <span class="n">pair_idx</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="p">:</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">pairs</span>
</span><span data-line="289">        <span class="n">pair_mask</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="p">:</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
</span><span data-line="290">
</span><span data-line="291">    <span class="c1"># Extract head and tail embeddings using advanced indexing</span>
</span><span data-line="292">    <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, 1)</span>
</span><span data-line="293">    <span class="n">head_rep</span> <span class="o">=</span> <span class="n">span_rep</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">pair_idx</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="mi">0</span><span class="p">)]</span>  <span class="c1"># (B, N, D)</span>
</span><span data-line="294">    <span class="n">tail_rep</span> <span class="o">=</span> <span class="n">span_rep</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">pair_idx</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="mi">0</span><span class="p">)]</span>  <span class="c1"># (B, N, D)</span>
</span><span data-line="295">
</span><span data-line="296">    <span class="k">return</span> <span class="n">pair_idx</span><span class="p">,</span> <span class="n">pair_mask</span><span class="p">,</span> <span class="n">head_rep</span><span class="p">,</span> <span class="n">tail_rep</span></div>

</span></pre></div>
        </article><button class="back-to-top" type="button">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
  </svg>
  <span>Back to top</span>
</button><div class="navigation flex print:hidden"></div></div>
    </div>
  </main>
</div>
<footer class="sy-foot">
  <div class="sy-foot-inner sy-container mx-auto">
    <div class="sy-foot-reserved md:flex justify-between items-center">
      <div class="sy-foot-copyright"><p>2025, GLiNER community</p>
  
  <p>
    Made with
    
    <a href="https://www.sphinx-doc.org/">Sphinx</a> and
    
    <a href="https://shibuya.lepture.com">Shibuya theme</a>.
  </p>
</div>
      <div class="sy-foot-socials"></div>
    </div>
  </div>
</footer>
      <script src="../../../_static/documentation_options.js?v=dc91f075"></script>
      <script src="../../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../_static/shibuya.js?v=9b0e4dde"></script></body>
</html>