<!DOCTYPE html>
<html lang="en" data-accent-color="violet" data-content_root="../../../">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>gliner.training.trainer - Home 0.2.24 documentation</title><link rel="index" title="Index" href="../../../genindex.html" /><link rel="search" title="Search" href="../../../search.html" /><script>
    function setColorMode(t){let e=document.documentElement;e.setAttribute("data-color-mode",t);let a=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,s=t;"auto"===t&&(s=a?"dark":"light"),"light"===s?(e.classList.remove("dark"),e.classList.add("light")):(e.classList.remove("light"),e.classList.add("dark"))}
    setColorMode(localStorage._theme||"auto");
  </script><link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=e1a1ceaf" />
    <link rel="stylesheet" type="text/css" href="../../../_static/shibuya.css?v=d140fbf8" />
    <link media="print" rel="stylesheet" type="text/css" href="../../../_static/print.css?v=20ff2c19" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
:root {
  --sy-f-text: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
  --sy-f-heading: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
}
</style>
    <meta property="og:type" content="website"/><meta property="og:title" content="gliner.training.trainer"/>
<meta name="twitter:card" content="summary"/>
  </head>
<body><div class="sy-head">
  <div class="sy-head-blur"></div>
  <div class="sy-head-inner sy-container mx-auto">
    <a class="sy-head-brand" href="../../../index.html">
      
      
      <strong>Home</strong>
    </a>
    <div class="sy-head-nav" id="head-nav">
      <nav class="sy-head-links"></nav>
      <div class="sy-head-extra flex items-center print:hidden"><form class="searchbox flex items-center" action="../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <kbd>/</kbd>
</form><div class="sy-head-socials"></div></div>
    </div>
    <div class="sy-head-actions flex items-center shrink-0 print:hidden"><button class="js-theme theme-switch flex items-center"
data-aria-auto="Switch to light color mode"
data-aria-light="Switch to dark color mode"
data-aria-dark="Switch to auto color mode">
<i class="i-lucide theme-icon"></i>
</button><button class="md:hidden flex items-center js-menu" aria-label="Menu" type="button" aria-controls="head-nav" aria-expanded="false">
        <div class="hamburger">
          <span class="hamburger_1"></span>
          <span class="hamburger_2"></span>
          <span class="hamburger_3"></span>
        </div>
      </button>
    </div>
  </div>
</div>
<div class="sy-page sy-container flex mx-auto">
  <aside id="lside" class="sy-lside md:w-72 md:shrink-0 print:hidden">
    <div class="sy-lside-inner md:sticky">
      <div class="sy-scrollbar p-6">
        <div class="globaltoc" data-expand-depth="0"><p class="caption" role="heading" aria-level="3"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../intro.html">Introduction to ðŸ‘‘ GLiNER</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../instalation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../usage.html">Advanced Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../configs.html">Components &amp; Configs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../training.html">Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architectures.html">Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../convert_to_onnx.html">ONNX Export &amp; Deployment</a></li>
</ul>
<p class="caption" role="heading" aria-level="3"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.model.html">gliner.model module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.config.html">gliner.config module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.training.html">gliner.training package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.training.trainer.html">gliner.training.trainer module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.modeling.html">gliner.modeling package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.multitask.html">gliner.modeling.multitask package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.modeling.multitask.relations_layers.html">gliner.modeling.multitask.relations_layers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.modeling.multitask.triples_layers.html">gliner.modeling.multitask.triples_layers module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.base.html">gliner.modeling.base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.decoder.html">gliner.modeling.decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.encoder.html">gliner.modeling.encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.layers.html">gliner.modeling.layers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.loss_functions.html">gliner.modeling.loss_functions module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.outputs.html">gliner.modeling.outputs module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.scorers.html">gliner.modeling.scorers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.span_rep.html">gliner.modeling.span_rep module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.utils.html">gliner.modeling.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.data_processing.html">gliner.data_processing package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.collator.html">gliner.data_processing.collator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.processor.html">gliner.data_processing.processor module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.tokenizer.html">gliner.data_processing.tokenizer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.utils.html">gliner.data_processing.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.evaluation.html">gliner.evaluation package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.evaluate_ner.html">gliner.evaluation.evaluate_ner module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.evaluator.html">gliner.evaluation.evaluator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.utils.html">gliner.evaluation.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.onnx.html">gliner.onnx package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.onnx.model.html">gliner.onnx.model module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.decoding.html">gliner.decoding package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.trie.html">gliner.decoding.trie package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.decoding.trie.labels_trie.html">gliner.decoding.trie.labels_trie module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.decoding.trie.python_labels_trie.html">gliner.decoding.trie.python_labels_trie module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.decoder.html">gliner.decoding.decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.utils.html">gliner.decoding.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.utils.html">gliner.utils module</a></li>
</ul>

        </div>
      </div>
    </div>
  </aside>
  <div class="lside-overlay js-menu" role="button" aria-label="Close left sidebar" aria-controls="lside" aria-expanded="false"></div>
  <aside id="rside" class="sy-rside pb-3 w-64 shrink-0 order-last">
    <button class="rside-close js-menu xl:hidden" aria-label="Close Table of Contents" type="button" aria-controls="rside" aria-expanded="false">
      <i class="i-lucide close"></i>
    </button>
    <div class="sy-scrollbar sy-rside-inner px-6 xl:top-16 xl:sticky xl:pl-0 pt-6 pb-4"><div id="ethical-ad-placement" data-ea-publisher="readthedocs"></div></div>
  </aside>
  <div class="rside-overlay js-menu" role="button" aria-label="Close Table of Contents" aria-controls="rside" aria-expanded="false"></div>
  <main class="sy-main w-full max-sm:max-w-full print:pt-6">
<div class="sy-breadcrumbs" role="navigation">
  <div class="sy-breadcrumbs-inner flex items-center">
    <div class="md:hidden mr-3">
      <button class="js-menu" aria-label="Menu" type="button" aria-controls="lside" aria-expanded="false">
        <i class="i-lucide menu"></i>
      </button>
    </div>
    <ol class="flex-1" itemscope itemtype="https://schema.org/BreadcrumbList"><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../../../index.html"><span itemprop="name">Home</span></a>
        <span>/</span>
        <meta itemprop="position" content="1" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../../index.html"><span itemprop="name">Module code</span></a>
        <span>/</span>
        <meta itemprop="position" content="2" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <strong itemprop="name">gliner.training.trainer</strong>
        <meta itemprop="position" content="3" />
      </li></ol>
    <div class="xl:hidden ml-1">
      <button class="js-menu" aria-label="Show table of contents" type="button" aria-controls="rside"
        aria-expanded="false">
        <i class="i-lucide outdent"></i>
      </button>
    </div>
  </div>
</div><div class="flex flex-col break-words justify-between">
      <div class="min-w-0 max-w-6xl px-6 pb-6 pt-8 xl:px-12">
        <article class="yue" role="main">
          <h1>Source code for gliner.training.trainer</h1><div class="highlight"><pre>
<span></span><span data-line="1"><span class="sd">&quot;&quot;&quot;Custom Trainer implementation with enhanced loss functions and optimizer configuration.</span>
</span><span data-line="2">
</span><span data-line="3"><span class="sd">This module extends the Hugging Face Transformers Trainer class to support</span>
</span><span data-line="4"><span class="sd">custom loss functions (focal loss, label smoothing), flexible learning rates</span>
</span><span data-line="5"><span class="sd">for different parameter groups, and robust error handling during training.</span>
</span><span data-line="6"><span class="sd">&quot;&quot;&quot;</span>
</span><span data-line="7">
</span><span data-line="8"><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
</span><span data-line="9"><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span>
</span><span data-line="10"><span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">field</span><span class="p">,</span> <span class="n">dataclass</span>
</span><span data-line="11">
</span><span data-line="12"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span data-line="13"><span class="kn">import</span><span class="w"> </span><span class="nn">transformers</span>
</span><span data-line="14"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span data-line="15"><span class="kn">from</span><span class="w"> </span><span class="nn">transformers.trainer</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
</span><span data-line="16">    <span class="n">get_parameter_names</span><span class="p">,</span>
</span><span data-line="17">    <span class="n">is_sagemaker_mp_enabled</span><span class="p">,</span>
</span><span data-line="18"><span class="p">)</span>
</span><span data-line="19"><span class="kn">from</span><span class="w"> </span><span class="nn">transformers.trainer_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">set_seed</span>
</span><span data-line="20">
</span><span data-line="21"><span class="k">if</span> <span class="n">is_sagemaker_mp_enabled</span><span class="p">():</span>
</span><span data-line="22">    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers.trainer_pt_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">smp_forward_backward</span>
</span><span data-line="23"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
</span><span data-line="24">
</span><span data-line="25"><span class="n">ALL_LAYERNORM_LAYERS</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">]</span>
</span><span data-line="26">
</span><span data-line="27"><span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</span><span data-line="28">
</span><span data-line="29">
<div class="viewcode-block" id="seed_worker">
<a class="viewcode-back" href="../../../api/gliner.training.trainer.html#gliner.training.trainer.seed_worker">[docs]</a>
</span><span data-line="30"><span class="k">def</span><span class="w"> </span><span class="nf">seed_worker</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
</span><span data-line="31"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Set worker seed during DataLoader initialization.</span>
</span><span data-line="32">
</span><span data-line="33"><span class="sd">    Helper function to ensure reproducibility by seeding each DataLoader worker</span>
</span><span data-line="34"><span class="sd">    process with a unique but deterministic seed based on PyTorch&#39;s initial seed.</span>
</span><span data-line="35">
</span><span data-line="36"><span class="sd">    Args:</span>
</span><span data-line="37"><span class="sd">        _: Worker ID (unused, but required by DataLoader worker_init_fn signature).</span>
</span><span data-line="38"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="39">    <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
</span><span data-line="40">    <span class="n">set_seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span></div>

</span><span data-line="41">
</span><span data-line="42">
<div class="viewcode-block" id="TrainingArguments">
<a class="viewcode-back" href="../../../api/gliner.training.trainer.html#gliner.training.trainer.TrainingArguments">[docs]</a>
</span><span data-line="43"><span class="nd">@dataclass</span>
</span><span data-line="44"><span class="k">class</span><span class="w"> </span><span class="nc">TrainingArguments</span><span class="p">(</span><span class="n">transformers</span><span class="o">.</span><span class="n">TrainingArguments</span><span class="p">):</span>
</span><span data-line="45"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Extended training arguments with custom loss and optimization parameters.</span>
</span><span data-line="46">
</span><span data-line="47"><span class="sd">    Extends the standard Hugging Face TrainingArguments with additional parameters</span>
</span><span data-line="48"><span class="sd">    for focal loss, label smoothing, differential learning rates, and custom</span>
</span><span data-line="49"><span class="sd">    negative sampling strategies.</span>
</span><span data-line="50">
</span><span data-line="51"><span class="sd">    Attributes:</span>
</span><span data-line="52"><span class="sd">        cache_dir: Directory to cache downloaded models and datasets.</span>
</span><span data-line="53"><span class="sd">        optim: Optimizer to use. Defaults to &quot;adamw_torch&quot;.</span>
</span><span data-line="54"><span class="sd">        others_lr: Optional separate learning rate for non-encoder parameters</span>
</span><span data-line="55"><span class="sd">            (e.g., classification heads). If None, uses the main learning rate.</span>
</span><span data-line="56"><span class="sd">        others_weight_decay: Weight decay for non-encoder parameters when</span>
</span><span data-line="57"><span class="sd">            using others_lr. Defaults to 0.0.</span>
</span><span data-line="58"><span class="sd">        focal_loss_alpha: Alpha parameter for focal loss. Values &lt; 0 disable</span>
</span><span data-line="59"><span class="sd">            focal loss weighting. Defaults to -1.</span>
</span><span data-line="60"><span class="sd">        focal_loss_gamma: Gamma (focusing parameter) for focal loss. Higher values</span>
</span><span data-line="61"><span class="sd">            increase focus on hard examples. Defaults to 0.</span>
</span><span data-line="62"><span class="sd">        focal_loss_prob_margin: Probability margin for focal loss computation.</span>
</span><span data-line="63"><span class="sd">            Defaults to 0.</span>
</span><span data-line="64"><span class="sd">        label_smoothing: Label smoothing factor. 0.0 means no smoothing.</span>
</span><span data-line="65"><span class="sd">            Defaults to 0.</span>
</span><span data-line="66"><span class="sd">        loss_reduction: Reduction method for loss (&#39;sum&#39;, &#39;mean&#39;, or &#39;none&#39;).</span>
</span><span data-line="67"><span class="sd">            Defaults to &#39;sum&#39;.</span>
</span><span data-line="68"><span class="sd">        negatives: Ratio of negative samples to use. Defaults to 1.0.</span>
</span><span data-line="69"><span class="sd">        masking: Masking strategy for training (&#39;global&#39; or other strategies).</span>
</span><span data-line="70"><span class="sd">            Defaults to &#39;global&#39;.</span>
</span><span data-line="71"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="72">
</span><span data-line="73">    <span class="n">cache_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span data-line="74">    <span class="n">optim</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">&quot;adamw_torch&quot;</span><span class="p">)</span>
</span><span data-line="75">    <span class="n">others_lr</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span><span data-line="76">    <span class="n">others_weight_decay</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
</span><span data-line="77">    <span class="n">focal_loss_alpha</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span><span data-line="78">    <span class="n">focal_loss_gamma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span data-line="79">    <span class="n">focal_loss_prob_margin</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span data-line="80">    <span class="n">label_smoothing</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span data-line="81">    <span class="n">loss_reduction</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span>
</span><span data-line="82">    <span class="n">negatives</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
</span><span data-line="83">    <span class="n">masking</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;global&quot;</span></div>

</span><span data-line="84">
</span><span data-line="85">
<div class="viewcode-block" id="Trainer">
<a class="viewcode-back" href="../../../api/gliner.training.trainer.html#gliner.training.trainer.Trainer">[docs]</a>
</span><span data-line="86"><span class="k">class</span><span class="w"> </span><span class="nc">Trainer</span><span class="p">(</span><span class="n">transformers</span><span class="o">.</span><span class="n">Trainer</span><span class="p">):</span>
</span><span data-line="87"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Custom Trainer with enhanced loss functions and error handling.</span>
</span><span data-line="88">
</span><span data-line="89"><span class="sd">    Extends the Hugging Face Trainer to support:</span>
</span><span data-line="90"><span class="sd">    - Custom loss functions (focal loss, label smoothing)</span>
</span><span data-line="91"><span class="sd">    - Differential learning rates for encoder vs. other parameters</span>
</span><span data-line="92"><span class="sd">    - Robust error handling with automatic recovery from failed batches</span>
</span><span data-line="93"><span class="sd">    - Custom negative sampling and masking strategies</span>
</span><span data-line="94"><span class="sd">    - Persistent worker support for data loading</span>
</span><span data-line="95">
</span><span data-line="96"><span class="sd">    The trainer automatically handles CUDA out-of-memory errors and other</span>
</span><span data-line="97"><span class="sd">    exceptions during training by skipping problematic batches and continuing.</span>
</span><span data-line="98"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="99">
<div class="viewcode-block" id="Trainer.training_step">
<a class="viewcode-back" href="../../../api/gliner.training.trainer.html#gliner.training.trainer.Trainer.training_step">[docs]</a>
</span><span data-line="100">    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span data-line="101"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform a training step on a batch of inputs.</span>
</span><span data-line="102">
</span><span data-line="103"><span class="sd">        Executes forward pass, loss computation, and backward pass for a single</span>
</span><span data-line="104"><span class="sd">        training batch. Includes automatic error handling to skip problematic</span>
</span><span data-line="105"><span class="sd">        batches without crashing the training run.</span>
</span><span data-line="106">
</span><span data-line="107"><span class="sd">        Args:</span>
</span><span data-line="108"><span class="sd">            model: The model to train.</span>
</span><span data-line="109"><span class="sd">            inputs: Dictionary of input tensors and targets for the model.</span>
</span><span data-line="110"><span class="sd">                The dictionary will be unpacked before being fed to the model.</span>
</span><span data-line="111"><span class="sd">                Most models expect targets under the &#39;labels&#39; key.</span>
</span><span data-line="112"><span class="sd">            *args: Additional positional arguments (unused, for compatibility).</span>
</span><span data-line="113"><span class="sd">            **kwargs: Additional keyword arguments (unused, for compatibility).</span>
</span><span data-line="114">
</span><span data-line="115"><span class="sd">        Returns:</span>
</span><span data-line="116"><span class="sd">            Training loss tensor for this batch, scaled by gradient accumulation</span>
</span><span data-line="117"><span class="sd">            steps. Returns a zero tensor with requires_grad=True if an error occurs.</span>
</span><span data-line="118">
</span><span data-line="119"><span class="sd">        Note:</span>
</span><span data-line="120"><span class="sd">            If an exception occurs during the training step, the method prints</span>
</span><span data-line="121"><span class="sd">            the error, zeros gradients, clears CUDA cache, and returns a zero</span>
</span><span data-line="122"><span class="sd">            loss to allow training to continue.</span>
</span><span data-line="123"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="124">        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span><span data-line="125">        <span class="k">try</span><span class="p">:</span>
</span><span data-line="126">            <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span data-line="127">            <span class="k">if</span> <span class="n">is_sagemaker_mp_enabled</span><span class="p">():</span>
</span><span data-line="128">                <span class="n">loss_mb</span> <span class="o">=</span> <span class="n">smp_forward_backward</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span><span class="p">)</span>
</span><span data-line="129">                <span class="k">return</span> <span class="n">loss_mb</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="130">
</span><span data-line="131">            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss_context_manager</span><span class="p">():</span>
</span><span data-line="132">                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
</span><span data-line="133">
</span><span data-line="134">            <span class="k">del</span> <span class="n">inputs</span>
</span><span data-line="135">            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</span><span data-line="136">
</span><span data-line="137">            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
</span><span data-line="138">
</span><span data-line="139">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span data-line="140">                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># Average on multi-gpu training</span>
</span><span data-line="141">
</span><span data-line="142">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_apex</span><span class="p">:</span>
</span><span data-line="143">                <span class="kn">from</span><span class="w"> </span><span class="nn">apex</span><span class="w"> </span><span class="kn">import</span> <span class="n">amp</span>
</span><span data-line="144">
</span><span data-line="145">                <span class="k">with</span> <span class="n">amp</span><span class="o">.</span><span class="n">scale_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span> <span class="k">as</span> <span class="n">scaled_loss</span><span class="p">:</span>
</span><span data-line="146">                    <span class="n">scaled_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span><span data-line="147">            <span class="k">else</span><span class="p">:</span>
</span><span data-line="148">                <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span data-line="149">
</span><span data-line="150">            <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>
</span><span data-line="151">
</span><span data-line="152">        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span><span data-line="153">            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Skipping iteration due to error: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</span><span data-line="154">            <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span data-line="155">            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</span><span data-line="156">            <span class="c1"># Safely get device for DataParallel or normal model</span>
</span><span data-line="157">            <span class="n">_model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;module&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</span><span data-line="158">            <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
</span><span data-line="159">            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span></div>

</span><span data-line="160">
<div class="viewcode-block" id="Trainer.save_model">
<a class="viewcode-back" href="../../../api/gliner.training.trainer.html#gliner.training.trainer.Trainer.save_model">[docs]</a>
</span><span data-line="161">    <span class="k">def</span><span class="w"> </span><span class="nf">save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">_internal_call</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span data-line="162"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Save the trained model to a directory.</span>
</span><span data-line="163">
</span><span data-line="164"><span class="sd">        Args:</span>
</span><span data-line="165"><span class="sd">            output_dir: Directory path where the model should be saved.</span>
</span><span data-line="166"><span class="sd">                If None, uses the default output directory from training arguments.</span>
</span><span data-line="167"><span class="sd">            _internal_call: Whether this is an internal call from the Trainer.</span>
</span><span data-line="168"><span class="sd">                Used for compatibility with the parent class.</span>
</span><span data-line="169"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="170">        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span></div>

</span><span data-line="171">
<div class="viewcode-block" id="Trainer.compute_loss">
<a class="viewcode-back" href="../../../api/gliner.training.trainer.html#gliner.training.trainer.Trainer.compute_loss">[docs]</a>
</span><span data-line="172">    <span class="k">def</span><span class="w"> </span><span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span data-line="173"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute loss using custom loss functions.</span>
</span><span data-line="174">
</span><span data-line="175"><span class="sd">        Performs forward pass with custom loss parameters including focal loss,</span>
</span><span data-line="176"><span class="sd">        label smoothing, and negative sampling configurations from training arguments.</span>
</span><span data-line="177">
</span><span data-line="178"><span class="sd">        Args:</span>
</span><span data-line="179"><span class="sd">            model: The model to compute loss for.</span>
</span><span data-line="180"><span class="sd">            inputs: Dictionary of input tensors including features and labels.</span>
</span><span data-line="181">
</span><span data-line="182"><span class="sd">        Returns:</span>
</span><span data-line="183"><span class="sd">            Computed loss tensor.</span>
</span><span data-line="184">
</span><span data-line="185"><span class="sd">        Note:</span>
</span><span data-line="186"><span class="sd">            The loss function parameters (alpha, gamma, label_smoothing, etc.)</span>
</span><span data-line="187"><span class="sd">            are passed to the model&#39;s forward method, so the model must support</span>
</span><span data-line="188"><span class="sd">            these keyword arguments.</span>
</span><span data-line="189"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="190">        <span class="c1"># Forward pass</span>
</span><span data-line="191">        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
</span><span data-line="192">            <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">focal_loss_alpha</span><span class="p">,</span>
</span><span data-line="193">            <span class="n">gamma</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">focal_loss_gamma</span><span class="p">,</span>
</span><span data-line="194">            <span class="n">prob_margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">focal_loss_prob_margin</span><span class="p">,</span>
</span><span data-line="195">            <span class="n">label_smoothing</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">label_smoothing</span><span class="p">,</span>
</span><span data-line="196">            <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">loss_reduction</span><span class="p">,</span>
</span><span data-line="197">            <span class="n">negatives</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">negatives</span><span class="p">,</span>
</span><span data-line="198">            <span class="n">masking</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">masking</span><span class="p">,</span>
</span><span data-line="199">            <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
</span><span data-line="200">        <span class="p">)</span>
</span><span data-line="201">        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
</span><span data-line="202">        <span class="k">return</span> <span class="n">loss</span></div>

</span><span data-line="203">
<div class="viewcode-block" id="Trainer.create_optimizer">
<a class="viewcode-back" href="../../../api/gliner.training.trainer.html#gliner.training.trainer.Trainer.create_optimizer">[docs]</a>
</span><span data-line="204">    <span class="k">def</span><span class="w"> </span><span class="nf">create_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span data-line="205"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Create and configure the optimizer with parameter groups.</span>
</span><span data-line="206">
</span><span data-line="207"><span class="sd">        Sets up the optimizer with support for:</span>
</span><span data-line="208"><span class="sd">        - Separate learning rates for encoder and non-encoder parameters</span>
</span><span data-line="209"><span class="sd">        - Weight decay only for non-bias and non-LayerNorm parameters</span>
</span><span data-line="210"><span class="sd">        - Custom weight decay values for different parameter groups</span>
</span><span data-line="211">
</span><span data-line="212"><span class="sd">        Returns:</span>
</span><span data-line="213"><span class="sd">            Configured optimizer instance.</span>
</span><span data-line="214">
</span><span data-line="215"><span class="sd">        Note:</span>
</span><span data-line="216"><span class="sd">            If self.args.others_lr is set, creates four parameter groups:</span>
</span><span data-line="217"><span class="sd">            1. Non-encoder parameters with weight decay</span>
</span><span data-line="218"><span class="sd">            2. Non-encoder parameters without weight decay</span>
</span><span data-line="219"><span class="sd">            3. Encoder parameters with weight decay</span>
</span><span data-line="220"><span class="sd">            4. Encoder parameters without weight decay</span>
</span><span data-line="221">
</span><span data-line="222"><span class="sd">            Otherwise, creates two standard parameter groups with and without</span>
</span><span data-line="223"><span class="sd">            weight decay.</span>
</span><span data-line="224"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="225">        <span class="k">if</span> <span class="n">is_sagemaker_mp_enabled</span><span class="p">():</span>
</span><span data-line="226">            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">create_optimizer</span><span class="p">()</span>
</span><span data-line="227">
</span><span data-line="228">        <span class="n">opt_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>
</span><span data-line="229">
</span><span data-line="230">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="231">            <span class="n">decay_parameters</span> <span class="o">=</span> <span class="n">get_parameter_names</span><span class="p">(</span><span class="n">opt_model</span><span class="p">,</span> <span class="n">ALL_LAYERNORM_LAYERS</span><span class="p">)</span>
</span><span data-line="232">            <span class="n">decay_parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
</span><span data-line="233">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">others_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="234">                <span class="n">encoder_parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">opt_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;token_rep_layer&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
</span><span data-line="235">                <span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="p">[</span>
</span><span data-line="236">                    <span class="p">{</span>
</span><span data-line="237">                        <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span data-line="238">                            <span class="n">p</span>
</span><span data-line="239">                            <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">opt_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
</span><span data-line="240">                            <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="ow">and</span> <span class="n">n</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">encoder_parameters</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</span><span data-line="241">                        <span class="p">],</span>
</span><span data-line="242">                        <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">others_weight_decay</span><span class="p">,</span>
</span><span data-line="243">                        <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">others_lr</span><span class="p">,</span>
</span><span data-line="244">                    <span class="p">},</span>
</span><span data-line="245">                    <span class="p">{</span>
</span><span data-line="246">                        <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span data-line="247">                            <span class="n">p</span>
</span><span data-line="248">                            <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">opt_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
</span><span data-line="249">                            <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="ow">and</span> <span class="n">n</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">encoder_parameters</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</span><span data-line="250">                        <span class="p">],</span>
</span><span data-line="251">                        <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span data-line="252">                        <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">others_lr</span><span class="p">,</span>
</span><span data-line="253">                    <span class="p">},</span>
</span><span data-line="254">                    <span class="p">{</span>
</span><span data-line="255">                        <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span data-line="256">                            <span class="n">p</span>
</span><span data-line="257">                            <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">opt_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
</span><span data-line="258">                            <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="ow">and</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">encoder_parameters</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</span><span data-line="259">                        <span class="p">],</span>
</span><span data-line="260">                        <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
</span><span data-line="261">                    <span class="p">},</span>
</span><span data-line="262">                    <span class="p">{</span>
</span><span data-line="263">                        <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span data-line="264">                            <span class="n">p</span>
</span><span data-line="265">                            <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">opt_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
</span><span data-line="266">                            <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="ow">and</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">encoder_parameters</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</span><span data-line="267">                        <span class="p">],</span>
</span><span data-line="268">                        <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span data-line="269">                    <span class="p">},</span>
</span><span data-line="270">                <span class="p">]</span>
</span><span data-line="271">            <span class="k">else</span><span class="p">:</span>
</span><span data-line="272">                <span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="p">[</span>
</span><span data-line="273">                    <span class="p">{</span>
</span><span data-line="274">                        <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span data-line="275">                            <span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">opt_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</span><span data-line="276">                        <span class="p">],</span>
</span><span data-line="277">                        <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
</span><span data-line="278">                    <span class="p">},</span>
</span><span data-line="279">                    <span class="p">{</span>
</span><span data-line="280">                        <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span data-line="281">                            <span class="n">p</span>
</span><span data-line="282">                            <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">opt_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
</span><span data-line="283">                            <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</span><span data-line="284">                        <span class="p">],</span>
</span><span data-line="285">                        <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span data-line="286">                    <span class="p">},</span>
</span><span data-line="287">                <span class="p">]</span>
</span><span data-line="288">
</span><span data-line="289">            <span class="n">optimizer_cls</span><span class="p">,</span> <span class="n">optimizer_kwargs</span> <span class="o">=</span> <span class="n">Trainer</span><span class="o">.</span><span class="n">get_optimizer_cls_and_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">)</span>
</span><span data-line="290">
</span><span data-line="291">            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="p">(</span><span class="n">optimizer_grouped_parameters</span><span class="p">,</span> <span class="o">**</span><span class="n">optimizer_kwargs</span><span class="p">)</span>
</span><span data-line="292">
</span><span data-line="293">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span></div>

</span><span data-line="294">
<div class="viewcode-block" id="Trainer.prediction_step">
<a class="viewcode-back" href="../../../api/gliner.training.trainer.html#gliner.training.trainer.Trainer.prediction_step">[docs]</a>
</span><span data-line="295">    <span class="k">def</span><span class="w"> </span><span class="nf">prediction_step</span><span class="p">(</span>
</span><span data-line="296">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="297">        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span><span data-line="298">        <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
</span><span data-line="299">        <span class="n">prediction_loss_only</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span data-line="300">        <span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="301">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
</span><span data-line="302"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform an evaluation step on the model using inputs.</span>
</span><span data-line="303">
</span><span data-line="304"><span class="sd">        Executes a single forward pass for evaluation without computing gradients.</span>
</span><span data-line="305">
</span><span data-line="306"><span class="sd">        Args:</span>
</span><span data-line="307"><span class="sd">            model: The model to evaluate.</span>
</span><span data-line="308"><span class="sd">            inputs: Dictionary of input tensors and targets for the model.</span>
</span><span data-line="309"><span class="sd">                The dictionary will be unpacked before being fed to the model.</span>
</span><span data-line="310"><span class="sd">                Most models expect targets under the &#39;labels&#39; key.</span>
</span><span data-line="311"><span class="sd">            prediction_loss_only: If True, only returns the loss and ignores</span>
</span><span data-line="312"><span class="sd">                logits and labels.</span>
</span><span data-line="313"><span class="sd">            ignore_keys: Optional list of keys in the model output dictionary</span>
</span><span data-line="314"><span class="sd">                that should be ignored when gathering predictions. Currently unused.</span>
</span><span data-line="315">
</span><span data-line="316"><span class="sd">        Returns:</span>
</span><span data-line="317"><span class="sd">            A tuple of (loss, logits, labels):</span>
</span><span data-line="318"><span class="sd">            - loss: Loss tensor if computed, None otherwise</span>
</span><span data-line="319"><span class="sd">            - logits: Model predictions if prediction_loss_only is False, None otherwise</span>
</span><span data-line="320"><span class="sd">            - labels: Ground truth labels if prediction_loss_only is False, None otherwise</span>
</span><span data-line="321"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="322">        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span><span data-line="323">            <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
</span><span data-line="324">            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss_context_manager</span><span class="p">():</span>
</span><span data-line="325">                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</span><span data-line="326">            <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
</span><span data-line="327">            <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
</span><span data-line="328">            <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span>
</span><span data-line="329">        <span class="k">if</span> <span class="n">prediction_loss_only</span><span class="p">:</span>
</span><span data-line="330">            <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span data-line="331">        <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span></div>

</span><span data-line="332">
<div class="viewcode-block" id="Trainer.get_train_dataloader">
<a class="viewcode-back" href="../../../api/gliner.training.trainer.html#gliner.training.trainer.Trainer.get_train_dataloader">[docs]</a>
</span><span data-line="333">    <span class="k">def</span><span class="w"> </span><span class="nf">get_train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">:</span>
</span><span data-line="334"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Create and return the training DataLoader.</span>
</span><span data-line="335">
</span><span data-line="336"><span class="sd">        Constructs a DataLoader with appropriate sampler, collation function,</span>
</span><span data-line="337"><span class="sd">        and worker configuration for the training dataset. Includes seeded</span>
</span><span data-line="338"><span class="sd">        worker initialization for reproducibility.</span>
</span><span data-line="339">
</span><span data-line="340"><span class="sd">        Returns:</span>
</span><span data-line="341"><span class="sd">            Configured and accelerator-prepared training DataLoader.</span>
</span><span data-line="342">
</span><span data-line="343"><span class="sd">        Raises:</span>
</span><span data-line="344"><span class="sd">            ValueError: If train_dataset is None.</span>
</span><span data-line="345">
</span><span data-line="346"><span class="sd">        Note:</span>
</span><span data-line="347"><span class="sd">            For IterableDataset, sampler and drop_last are not set.</span>
</span><span data-line="348"><span class="sd">            For regular datasets, uses the sampler from _get_train_sampler()</span>
</span><span data-line="349"><span class="sd">            and applies worker seeding via seed_worker function.</span>
</span><span data-line="350"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="351">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="352">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trainer: training requires a train_dataset.&quot;</span><span class="p">)</span>
</span><span data-line="353">
</span><span data-line="354">        <span class="n">train_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span>
</span><span data-line="355">        <span class="n">data_collator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_collator</span>
</span><span data-line="356">
</span><span data-line="357">        <span class="n">dataloader_params</span> <span class="o">=</span> <span class="p">{</span>
</span><span data-line="358">            <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span><span class="p">,</span>
</span><span data-line="359">            <span class="s2">&quot;collate_fn&quot;</span><span class="p">:</span> <span class="n">data_collator</span><span class="p">,</span>
</span><span data-line="360">            <span class="s2">&quot;num_workers&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_num_workers</span><span class="p">,</span>
</span><span data-line="361">            <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_pin_memory</span><span class="p">,</span>
</span><span data-line="362">            <span class="s2">&quot;persistent_workers&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_persistent_workers</span><span class="p">,</span>
</span><span data-line="363">        <span class="p">}</span>
</span><span data-line="364">
</span><span data-line="365">        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">IterableDataset</span><span class="p">):</span>
</span><span data-line="366">            <span class="n">dataloader_params</span><span class="p">[</span><span class="s2">&quot;sampler&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_train_sampler</span><span class="p">()</span>
</span><span data-line="367">            <span class="n">dataloader_params</span><span class="p">[</span><span class="s2">&quot;drop_last&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_drop_last</span>
</span><span data-line="368">            <span class="n">dataloader_params</span><span class="p">[</span><span class="s2">&quot;worker_init_fn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">seed_worker</span>
</span><span data-line="369">            <span class="n">dataloader_params</span><span class="p">[</span><span class="s2">&quot;prefetch_factor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_prefetch_factor</span>
</span><span data-line="370">
</span><span data-line="371">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_params</span><span class="p">))</span></div>

</span><span data-line="372">
<div class="viewcode-block" id="Trainer.get_eval_dataloader">
<a class="viewcode-back" href="../../../api/gliner.training.trainer.html#gliner.training.trainer.Trainer.get_eval_dataloader">[docs]</a>
</span><span data-line="373">    <span class="k">def</span><span class="w"> </span><span class="nf">get_eval_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">:</span>
</span><span data-line="374"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Create and return the evaluation DataLoader.</span>
</span><span data-line="375">
</span><span data-line="376"><span class="sd">        Constructs a DataLoader for evaluation with support for persistent workers</span>
</span><span data-line="377"><span class="sd">        and multiple evaluation datasets. Caches DataLoaders when persistent workers</span>
</span><span data-line="378"><span class="sd">        are enabled to avoid recreation overhead.</span>
</span><span data-line="379">
</span><span data-line="380"><span class="sd">        Args:</span>
</span><span data-line="381"><span class="sd">            eval_dataset: Evaluation dataset to use. Can be:</span>
</span><span data-line="382"><span class="sd">                - None: Uses self.eval_dataset</span>
</span><span data-line="383"><span class="sd">                - str: Uses self.eval_dataset[eval_dataset] (for named eval sets)</span>
</span><span data-line="384"><span class="sd">                - Dataset: Overrides self.eval_dataset directly</span>
</span><span data-line="385">
</span><span data-line="386"><span class="sd">        Returns:</span>
</span><span data-line="387"><span class="sd">            Configured and accelerator-prepared evaluation DataLoader.</span>
</span><span data-line="388">
</span><span data-line="389"><span class="sd">        Raises:</span>
</span><span data-line="390"><span class="sd">            ValueError: If both eval_dataset and self.eval_dataset are None.</span>
</span><span data-line="391">
</span><span data-line="392"><span class="sd">        Note:</span>
</span><span data-line="393"><span class="sd">            When persistent_workers is True, DataLoaders are cached in</span>
</span><span data-line="394"><span class="sd">            self._eval_dataloaders to avoid worker process recreation between</span>
</span><span data-line="395"><span class="sd">            evaluation calls. The cache key is the dataset name (if string)</span>
</span><span data-line="396"><span class="sd">            or &quot;eval&quot; for the default dataset.</span>
</span><span data-line="397"><span class="sd">        &quot;&quot;&quot;</span>
</span><span data-line="398">        <span class="k">if</span> <span class="n">eval_dataset</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_dataset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="399">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trainer: evaluation requires an eval_dataset.&quot;</span><span class="p">)</span>
</span><span data-line="400">
</span><span data-line="401">        <span class="n">dataloader_key</span> <span class="o">=</span> <span class="n">eval_dataset</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;eval&quot;</span>
</span><span data-line="402">        <span class="k">if</span> <span class="p">(</span>
</span><span data-line="403">            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_eval_dataloaders&quot;</span><span class="p">)</span>
</span><span data-line="404">            <span class="ow">and</span> <span class="n">dataloader_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eval_dataloaders</span>
</span><span data-line="405">            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_persistent_workers</span>
</span><span data-line="406">        <span class="p">):</span>
</span><span data-line="407">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_dataloaders</span><span class="p">[</span><span class="n">dataloader_key</span><span class="p">])</span>
</span><span data-line="408">
</span><span data-line="409">        <span class="n">eval_dataset</span> <span class="o">=</span> <span class="p">(</span>
</span><span data-line="410">            <span class="bp">self</span><span class="o">.</span><span class="n">eval_dataset</span><span class="p">[</span><span class="n">eval_dataset</span><span class="p">]</span>
</span><span data-line="411">            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
</span><span data-line="412">            <span class="k">else</span> <span class="n">eval_dataset</span>
</span><span data-line="413">            <span class="k">if</span> <span class="n">eval_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span data-line="414">            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_dataset</span>
</span><span data-line="415">        <span class="p">)</span>
</span><span data-line="416">        <span class="n">data_collator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_collator</span>
</span><span data-line="417">
</span><span data-line="418">        <span class="n">dataloader_params</span> <span class="o">=</span> <span class="p">{</span>
</span><span data-line="419">            <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">,</span>
</span><span data-line="420">            <span class="s2">&quot;collate_fn&quot;</span><span class="p">:</span> <span class="n">data_collator</span><span class="p">,</span>
</span><span data-line="421">            <span class="s2">&quot;num_workers&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_num_workers</span><span class="p">,</span>
</span><span data-line="422">            <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_pin_memory</span><span class="p">,</span>
</span><span data-line="423">            <span class="s2">&quot;persistent_workers&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_persistent_workers</span><span class="p">,</span>
</span><span data-line="424">        <span class="p">}</span>
</span><span data-line="425">
</span><span data-line="426">        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">IterableDataset</span><span class="p">):</span>
</span><span data-line="427">            <span class="n">dataloader_params</span><span class="p">[</span><span class="s2">&quot;sampler&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_eval_sampler</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">)</span>
</span><span data-line="428">            <span class="n">dataloader_params</span><span class="p">[</span><span class="s2">&quot;drop_last&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_drop_last</span>
</span><span data-line="429">            <span class="n">dataloader_params</span><span class="p">[</span><span class="s2">&quot;prefetch_factor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_prefetch_factor</span>
</span><span data-line="430">
</span><span data-line="431">        <span class="c1"># accelerator.free_memory() will destroy the references, so</span>
</span><span data-line="432">        <span class="c1"># we need to store the non-prepared version</span>
</span><span data-line="433">        <span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_params</span><span class="p">)</span>
</span><span data-line="434">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_persistent_workers</span><span class="p">:</span>
</span><span data-line="435">            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_eval_dataloaders&quot;</span><span class="p">):</span>
</span><span data-line="436">                <span class="bp">self</span><span class="o">.</span><span class="n">_eval_dataloaders</span><span class="p">[</span><span class="n">dataloader_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">eval_dataloader</span>
</span><span data-line="437">            <span class="k">else</span><span class="p">:</span>
</span><span data-line="438">                <span class="bp">self</span><span class="o">.</span><span class="n">_eval_dataloaders</span> <span class="o">=</span> <span class="p">{</span><span class="n">dataloader_key</span><span class="p">:</span> <span class="n">eval_dataloader</span><span class="p">}</span>
</span><span data-line="439">
</span><span data-line="440">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">eval_dataloader</span><span class="p">)</span></div>
</div>

</span></pre></div>
        </article><button class="back-to-top" type="button">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
  </svg>
  <span>Back to top</span>
</button><div class="navigation flex print:hidden"></div></div>
    </div>
  </main>
</div>
<footer class="sy-foot">
  <div class="sy-foot-inner sy-container mx-auto">
    <div class="sy-foot-reserved md:flex justify-between items-center">
      <div class="sy-foot-copyright"><p>2025, GLiNER community</p>
  
  <p>
    Made with
    
    <a href="https://www.sphinx-doc.org/">Sphinx</a> and
    
    <a href="https://shibuya.lepture.com">Shibuya theme</a>.
  </p>
</div>
      <div class="sy-foot-socials"></div>
    </div>
  </div>
</footer>
      <script src="../../../_static/documentation_options.js?v=dc91f075"></script>
      <script src="../../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../_static/shibuya.js?v=9b0e4dde"></script></body>
</html>