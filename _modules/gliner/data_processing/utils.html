<!DOCTYPE html>
<html lang="en" data-accent-color="violet" data-content_root="../../../">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>gliner.data_processing.utils - Home 0.2.24 documentation</title><link rel="index" title="Index" href="../../../genindex.html" /><link rel="search" title="Search" href="../../../search.html" /><script>
    function setColorMode(t){let e=document.documentElement;e.setAttribute("data-color-mode",t);let a=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,s=t;"auto"===t&&(s=a?"dark":"light"),"light"===s?(e.classList.remove("dark"),e.classList.add("light")):(e.classList.remove("light"),e.classList.add("dark"))}
    setColorMode(localStorage._theme||"auto");
  </script><link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=e1a1ceaf" />
    <link rel="stylesheet" type="text/css" href="../../../_static/shibuya.css?v=d140fbf8" />
    <link media="print" rel="stylesheet" type="text/css" href="../../../_static/print.css?v=20ff2c19" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
:root {
  --sy-f-text: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
  --sy-f-heading: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
}
</style>
    <meta property="og:type" content="website"/><meta property="og:title" content="gliner.data_processing.utils"/>
<meta name="twitter:card" content="summary"/>
  </head>
<body><div class="sy-head">
  <div class="sy-head-blur"></div>
  <div class="sy-head-inner sy-container mx-auto">
    <a class="sy-head-brand" href="../../../index.html">
      
      
      <strong>Home</strong>
    </a>
    <div class="sy-head-nav" id="head-nav">
      <nav class="sy-head-links"></nav>
      <div class="sy-head-extra flex items-center print:hidden"><form class="searchbox flex items-center" action="../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <kbd>/</kbd>
</form><div class="sy-head-socials"></div></div>
    </div>
    <div class="sy-head-actions flex items-center shrink-0 print:hidden"><button class="js-theme theme-switch flex items-center"
data-aria-auto="Switch to light color mode"
data-aria-light="Switch to dark color mode"
data-aria-dark="Switch to auto color mode">
<i class="i-lucide theme-icon"></i>
</button><button class="md:hidden flex items-center js-menu" aria-label="Menu" type="button" aria-controls="head-nav" aria-expanded="false">
        <div class="hamburger">
          <span class="hamburger_1"></span>
          <span class="hamburger_2"></span>
          <span class="hamburger_3"></span>
        </div>
      </button>
    </div>
  </div>
</div>
<div class="sy-page sy-container flex mx-auto">
  <aside id="lside" class="sy-lside md:w-72 md:shrink-0 print:hidden">
    <div class="sy-lside-inner md:sticky">
      <div class="sy-scrollbar p-6">
        <div class="globaltoc" data-expand-depth="0"><p class="caption" role="heading" aria-level="3"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../intro.html">Introduction to ðŸ‘‘ GLiNER</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../instalation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../usage.html">Advanced Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../configs.html">Components &amp; Configs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../training.html">Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architectures.html">Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../convert_to_onnx.html">ONNX Export &amp; Deployment</a></li>
</ul>
<p class="caption" role="heading" aria-level="3"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.model.html">gliner.model module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.config.html">gliner.config module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.training.html">gliner.training package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.training.trainer.html">gliner.training.trainer module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.modeling.html">gliner.modeling package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.multitask.html">gliner.modeling.multitask package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.modeling.multitask.relations_layers.html">gliner.modeling.multitask.relations_layers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.modeling.multitask.triples_layers.html">gliner.modeling.multitask.triples_layers module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.base.html">gliner.modeling.base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.decoder.html">gliner.modeling.decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.encoder.html">gliner.modeling.encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.layers.html">gliner.modeling.layers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.loss_functions.html">gliner.modeling.loss_functions module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.outputs.html">gliner.modeling.outputs module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.scorers.html">gliner.modeling.scorers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.span_rep.html">gliner.modeling.span_rep module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.modeling.utils.html">gliner.modeling.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.data_processing.html">gliner.data_processing package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.collator.html">gliner.data_processing.collator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.processor.html">gliner.data_processing.processor module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.tokenizer.html">gliner.data_processing.tokenizer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.data_processing.utils.html">gliner.data_processing.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.evaluation.html">gliner.evaluation package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.evaluate_ner.html">gliner.evaluation.evaluate_ner module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.evaluator.html">gliner.evaluation.evaluator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.evaluation.utils.html">gliner.evaluation.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.onnx.html">gliner.onnx package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.onnx.model.html">gliner.onnx.model module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.decoding.html">gliner.decoding package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.trie.html">gliner.decoding.trie package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.decoding.trie.labels_trie.html">gliner.decoding.trie.labels_trie module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gliner.decoding.trie.python_labels_trie.html">gliner.decoding.trie.python_labels_trie module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.decoder.html">gliner.decoding.decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gliner.decoding.utils.html">gliner.decoding.utils module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/gliner.utils.html">gliner.utils module</a></li>
</ul>

        </div>
      </div>
    </div>
  </aside>
  <div class="lside-overlay js-menu" role="button" aria-label="Close left sidebar" aria-controls="lside" aria-expanded="false"></div>
  <aside id="rside" class="sy-rside pb-3 w-64 shrink-0 order-last">
    <button class="rside-close js-menu xl:hidden" aria-label="Close Table of Contents" type="button" aria-controls="rside" aria-expanded="false">
      <i class="i-lucide close"></i>
    </button>
    <div class="sy-scrollbar sy-rside-inner px-6 xl:top-16 xl:sticky xl:pl-0 pt-6 pb-4"><div id="ethical-ad-placement" data-ea-publisher="readthedocs"></div></div>
  </aside>
  <div class="rside-overlay js-menu" role="button" aria-label="Close Table of Contents" aria-controls="rside" aria-expanded="false"></div>
  <main class="sy-main w-full max-sm:max-w-full print:pt-6">
<div class="sy-breadcrumbs" role="navigation">
  <div class="sy-breadcrumbs-inner flex items-center">
    <div class="md:hidden mr-3">
      <button class="js-menu" aria-label="Menu" type="button" aria-controls="lside" aria-expanded="false">
        <i class="i-lucide menu"></i>
      </button>
    </div>
    <ol class="flex-1" itemscope itemtype="https://schema.org/BreadcrumbList"><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../../../index.html"><span itemprop="name">Home</span></a>
        <span>/</span>
        <meta itemprop="position" content="1" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../../index.html"><span itemprop="name">Module code</span></a>
        <span>/</span>
        <meta itemprop="position" content="2" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <strong itemprop="name">gliner.data_processing.utils</strong>
        <meta itemprop="position" content="3" />
      </li></ol>
    <div class="xl:hidden ml-1">
      <button class="js-menu" aria-label="Show table of contents" type="button" aria-controls="rside"
        aria-expanded="false">
        <i class="i-lucide outdent"></i>
      </button>
    </div>
  </div>
</div><div class="flex flex-col break-words justify-between">
      <div class="min-w-0 max-w-6xl px-6 pb-6 pt-8 xl:px-12">
        <article class="yue" role="main">
          <h1>Source code for gliner.data_processing.utils</h1><div class="highlight"><pre>
<span></span><span data-line="1"><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
</span><span data-line="2"><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Sequence</span>
</span><span data-line="3">
</span><span data-line="4"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span data-line="5">
</span><span data-line="6">
<div class="viewcode-block" id="pad_2d_tensor">
<a class="viewcode-back" href="../../../api/gliner.data_processing.utils.html#gliner.data_processing.utils.pad_2d_tensor">[docs]</a>
</span><span data-line="7"><span class="k">def</span><span class="w"> </span><span class="nf">pad_2d_tensor</span><span class="p">(</span><span class="n">key_data</span><span class="p">):</span>
</span><span data-line="8"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Pad a list of 2D tensors to uniform dimensions.</span>
</span><span data-line="9">
</span><span data-line="10"><span class="sd">    Takes a list of 2D tensors with potentially different shapes and pads them</span>
</span><span data-line="11"><span class="sd">    to match the maximum dimensions across all tensors. All tensors are padded</span>
</span><span data-line="12"><span class="sd">    with zeros to create a uniform rectangular shape, then stacked into a single</span>
</span><span data-line="13"><span class="sd">    3D tensor with a batch dimension.</span>
</span><span data-line="14">
</span><span data-line="15"><span class="sd">    Args:</span>
</span><span data-line="16"><span class="sd">        key_data: List of 2D tensors to pad. Each tensor can have different</span>
</span><span data-line="17"><span class="sd">            dimensions, but all must be 2D.</span>
</span><span data-line="18">
</span><span data-line="19"><span class="sd">    Returns:</span>
</span><span data-line="20"><span class="sd">        A 3D tensor of shape (batch_size, max_rows, max_cols) containing all</span>
</span><span data-line="21"><span class="sd">        input tensors padded and stacked along the batch dimension.</span>
</span><span data-line="22">
</span><span data-line="23"><span class="sd">    Raises:</span>
</span><span data-line="24"><span class="sd">        ValueError: If the input list is empty.</span>
</span><span data-line="25">
</span><span data-line="26"><span class="sd">    Example:</span>
</span><span data-line="27"><span class="sd">        &gt;&gt;&gt; tensor1 = torch.tensor([[1, 2], [3, 4]])  # 2x2</span>
</span><span data-line="28"><span class="sd">        &gt;&gt;&gt; tensor2 = torch.tensor([[5, 6, 7]])  # 1x3</span>
</span><span data-line="29"><span class="sd">        &gt;&gt;&gt; result = pad_2d_tensor([tensor1, tensor2])</span>
</span><span data-line="30"><span class="sd">        &gt;&gt;&gt; result.shape</span>
</span><span data-line="31"><span class="sd">        torch.Size([2, 2, 3])</span>
</span><span data-line="32"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="33">    <span class="k">if</span> <span class="ow">not</span> <span class="n">key_data</span><span class="p">:</span>
</span><span data-line="34">        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The input list &#39;key_data&#39; should not be empty.&quot;</span><span class="p">)</span>
</span><span data-line="35">
</span><span data-line="36">    <span class="c1"># Determine the maximum size along both dimensions</span>
</span><span data-line="37">    <span class="n">max_rows</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">key_data</span><span class="p">)</span>
</span><span data-line="38">    <span class="n">max_cols</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">key_data</span><span class="p">)</span>
</span><span data-line="39">
</span><span data-line="40">    <span class="n">tensors</span> <span class="o">=</span> <span class="p">[]</span>
</span><span data-line="41">    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">key_data</span><span class="p">:</span>
</span><span data-line="42">        <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>
</span><span data-line="43">        <span class="n">row_padding</span> <span class="o">=</span> <span class="n">max_rows</span> <span class="o">-</span> <span class="n">rows</span>
</span><span data-line="44">        <span class="n">col_padding</span> <span class="o">=</span> <span class="n">max_cols</span> <span class="o">-</span> <span class="n">cols</span>
</span><span data-line="45">
</span><span data-line="46">        <span class="c1"># Pad the tensor along both dimensions</span>
</span><span data-line="47">        <span class="n">padded_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">col_padding</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">row_padding</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span data-line="48">        <span class="n">tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">padded_tensor</span><span class="p">)</span>
</span><span data-line="49">
</span><span data-line="50">    <span class="c1"># Stack the tensors into a single tensor along a new batch dimension</span>
</span><span data-line="51">    <span class="n">padded_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
</span><span data-line="52">    <span class="k">return</span> <span class="n">padded_tensors</span></div>

</span><span data-line="53">
</span><span data-line="54">
<div class="viewcode-block" id="get_negatives">
<a class="viewcode-back" href="../../../api/gliner.data_processing.utils.html#gliner.data_processing.utils.get_negatives">[docs]</a>
</span><span data-line="55"><span class="k">def</span><span class="w"> </span><span class="nf">get_negatives</span><span class="p">(</span><span class="n">batch_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span> <span class="n">sampled_neg</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s2">&quot;ner&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
</span><span data-line="56"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample negative entity or relation types from a batch.</span>
</span><span data-line="57">
</span><span data-line="58"><span class="sd">    Extracts all unique entity/relation types from a batch of examples and</span>
</span><span data-line="59"><span class="sd">    randomly samples a subset to use as negative types for contrastive learning.</span>
</span><span data-line="60"><span class="sd">    This helps the model learn to distinguish between similar but incorrect types.</span>
</span><span data-line="61">
</span><span data-line="62"><span class="sd">    Args:</span>
</span><span data-line="63"><span class="sd">        batch_list: List of example dictionaries. Each dictionary should contain</span>
</span><span data-line="64"><span class="sd">            the specified key with annotations in the format where the last element</span>
</span><span data-line="65"><span class="sd">            of each annotation tuple is the type label.</span>
</span><span data-line="66"><span class="sd">        sampled_neg: Maximum number of negative types to sample (default: 5).</span>
</span><span data-line="67"><span class="sd">            If fewer unique types exist, all will be returned.</span>
</span><span data-line="68"><span class="sd">        key: Dictionary key to access annotations (default: &quot;ner&quot;). Common values</span>
</span><span data-line="69"><span class="sd">            are &quot;ner&quot; for entities or &quot;relations&quot; for relation types.</span>
</span><span data-line="70">
</span><span data-line="71"><span class="sd">    Returns:</span>
</span><span data-line="72"><span class="sd">        List of randomly sampled type strings. Length will be min(sampled_neg,</span>
</span><span data-line="73"><span class="sd">        number of unique types in batch).</span>
</span><span data-line="74">
</span><span data-line="75"><span class="sd">    Example:</span>
</span><span data-line="76"><span class="sd">        &gt;&gt;&gt; batch = [{&quot;ner&quot;: [(0, 1, &quot;PERSON&quot;), (2, 3, &quot;ORG&quot;)]}, {&quot;ner&quot;: [(0, 1, &quot;LOC&quot;), (3, 4, &quot;PERSON&quot;)]}]</span>
</span><span data-line="77"><span class="sd">        &gt;&gt;&gt; negatives = get_negatives(batch, sampled_neg=2, key=&quot;ner&quot;)</span>
</span><span data-line="78"><span class="sd">        &gt;&gt;&gt; len(negatives) &lt;= 2</span>
</span><span data-line="79"><span class="sd">        True</span>
</span><span data-line="80"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="81">    <span class="n">element_types</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span><span data-line="82">    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch_list</span><span class="p">:</span>
</span><span data-line="83">        <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
</span><span data-line="84">            <span class="n">types</span> <span class="o">=</span> <span class="p">{</span><span class="n">el</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">b</span><span class="p">[</span><span class="n">key</span><span class="p">]}</span>
</span><span data-line="85">            <span class="n">element_types</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">types</span><span class="p">)</span>
</span><span data-line="86">
</span><span data-line="87">    <span class="n">element_types</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">element_types</span><span class="p">)</span>
</span><span data-line="88">    <span class="n">selected_elements</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">element_types</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="n">sampled_neg</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">element_types</span><span class="p">)))</span>
</span><span data-line="89">    <span class="k">return</span> <span class="n">selected_elements</span></div>

</span><span data-line="90">
<div class="viewcode-block" id="prepare_word_mask">
<a class="viewcode-back" href="../../../api/gliner.data_processing.utils.html#gliner.data_processing.utils.prepare_word_mask">[docs]</a>
</span><span data-line="91"><span class="k">def</span><span class="w"> </span><span class="nf">prepare_word_mask</span><span class="p">(</span>
</span><span data-line="92">    <span class="n">texts</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
</span><span data-line="93">    <span class="n">tokenized_inputs</span><span class="p">,</span>
</span><span data-line="94">    <span class="o">*</span><span class="p">,</span>
</span><span data-line="95">    <span class="n">skip_first_words</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span data-line="96">    <span class="n">token_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span data-line="97"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
</span><span data-line="98"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Create word-level masks for subword tokenized sequences.</span>
</span><span data-line="99">
</span><span data-line="100"><span class="sd">    Maps subword tokens back to their original word positions, enabling span</span>
</span><span data-line="101"><span class="sd">    extraction at the word level. Each subword token is assigned an integer</span>
</span><span data-line="102"><span class="sd">    indicating which word it belongs to (1-indexed), with special tokens and</span>
</span><span data-line="103"><span class="sd">    continuation subwords optionally masked as 0.</span>
</span><span data-line="104">
</span><span data-line="105"><span class="sd">    This is essential for span-based NER where predictions are made at the word</span>
</span><span data-line="106"><span class="sd">    level but the model processes subword tokens. The mask allows the model to</span>
</span><span data-line="107"><span class="sd">    aggregate subword representations into word-level representations.</span>
</span><span data-line="108">
</span><span data-line="109"><span class="sd">    Args:</span>
</span><span data-line="110"><span class="sd">        texts: Original text sequences as lists of words, one sequence per example.</span>
</span><span data-line="111"><span class="sd">        tokenized_inputs: Tokenized output from a transformer tokenizer with a</span>
</span><span data-line="112"><span class="sd">            word_ids() method (e.g., from HuggingFace tokenizers).</span>
</span><span data-line="113"><span class="sd">        skip_first_words: Optional number of words to skip at the beginning of</span>
</span><span data-line="114"><span class="sd">            each sequence (e.g., prompt words). Must have the same length as texts</span>
</span><span data-line="115"><span class="sd">            if provided. Skipped words are masked as 0.</span>
</span><span data-line="116"><span class="sd">        token_level: If True, assign a unique mask value to every token of a word</span>
</span><span data-line="117"><span class="sd">            (enabling token-level granularity). If False, only the first subword</span>
</span><span data-line="118"><span class="sd">            token of each word gets a mask value; continuation tokens are masked</span>
</span><span data-line="119"><span class="sd">            as 0 (default: False).</span>
</span><span data-line="120">
</span><span data-line="121"><span class="sd">    Returns:</span>
</span><span data-line="122"><span class="sd">        List of word mask lists, one per input sequence. Each mask list has the</span>
</span><span data-line="123"><span class="sd">        same length as the corresponding tokenized sequence. Values are:</span>
</span><span data-line="124"><span class="sd">            - 0: Special tokens, skipped words, or continuation subwords</span>
</span><span data-line="125"><span class="sd">            - 1, 2, 3, ...: Word indices (1-indexed) after skipping</span>
</span><span data-line="126">
</span><span data-line="127"><span class="sd">    Raises:</span>
</span><span data-line="128"><span class="sd">        ValueError: If skip_first_words length doesn&#39;t match texts length.</span>
</span><span data-line="129">
</span><span data-line="130"><span class="sd">    Example:</span>
</span><span data-line="131"><span class="sd">        &gt;&gt;&gt; texts = [[&quot;Hello&quot;, &quot;world&quot;]]</span>
</span><span data-line="132"><span class="sd">        &gt;&gt;&gt; # Assuming tokenizer splits &quot;Hello&quot; -&gt; [&quot;Hel&quot;, &quot;##lo&quot;]</span>
</span><span data-line="133"><span class="sd">        &gt;&gt;&gt; # and &quot;world&quot; -&gt; [&quot;world&quot;]</span>
</span><span data-line="134"><span class="sd">        &gt;&gt;&gt; mask = prepare_word_mask(texts, tokenized_inputs)</span>
</span><span data-line="135"><span class="sd">        &gt;&gt;&gt; # Result might be: [[0, 1, 0, 2, 0]]</span>
</span><span data-line="136"><span class="sd">        &gt;&gt;&gt; #                   [CLS, Hel, ##lo, world, SEP]</span>
</span><span data-line="137"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="138">    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
</span><span data-line="139">
</span><span data-line="140">    <span class="k">if</span> <span class="n">skip_first_words</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="141">        <span class="n">skip_first_words</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span>
</span><span data-line="142">    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">skip_first_words</span><span class="p">)</span> <span class="o">!=</span> <span class="n">n</span><span class="p">:</span>
</span><span data-line="143">        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;skip_first_words must have same length as texts&quot;</span><span class="p">)</span>
</span><span data-line="144">
</span><span data-line="145">    <span class="n">words_masks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span data-line="146">
</span><span data-line="147">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span><span data-line="148">        <span class="n">mask</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span data-line="149">        <span class="n">prev_word_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span><span data-line="150">        <span class="n">seen_words</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># counts distinct word_ids we&#39;ve traversed in this sequence</span>
</span><span data-line="151">
</span><span data-line="152">        <span class="k">for</span> <span class="n">wid</span> <span class="ow">in</span> <span class="n">tokenized_inputs</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
</span><span data-line="153">            <span class="k">if</span> <span class="n">wid</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span data-line="154">                <span class="c1"># Special tokens (CLS, SEP, PAD, etc.)</span>
</span><span data-line="155">                <span class="n">mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span data-line="156">            <span class="k">elif</span> <span class="n">wid</span> <span class="o">!=</span> <span class="n">prev_word_id</span> <span class="ow">or</span> <span class="n">token_level</span><span class="p">:</span>
</span><span data-line="157">                <span class="c1"># If we just moved to a new word, update seen_words</span>
</span><span data-line="158">                <span class="k">if</span> <span class="n">wid</span> <span class="o">!=</span> <span class="n">prev_word_id</span><span class="p">:</span>
</span><span data-line="159">                    <span class="n">seen_words</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span data-line="160">
</span><span data-line="161">                <span class="k">if</span> <span class="n">seen_words</span> <span class="o">&lt;=</span> <span class="n">skip_first_words</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
</span><span data-line="162">                    <span class="c1"># This word is in the skip range (e.g., prompt tokens)</span>
</span><span data-line="163">                    <span class="n">mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span data-line="164">                <span class="k">else</span><span class="p">:</span>
</span><span data-line="165">                    <span class="c1"># 1-based word index after skipping</span>
</span><span data-line="166">                    <span class="n">mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seen_words</span> <span class="o">-</span> <span class="n">skip_first_words</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span><span data-line="167">            <span class="k">else</span><span class="p">:</span>
</span><span data-line="168">                <span class="c1"># same word continuation and token_level=False -&gt; mask as 0</span>
</span><span data-line="169">                <span class="n">mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span data-line="170">
</span><span data-line="171">            <span class="n">prev_word_id</span> <span class="o">=</span> <span class="n">wid</span>
</span><span data-line="172">
</span><span data-line="173">        <span class="n">words_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
</span><span data-line="174">
</span><span data-line="175">    <span class="k">return</span> <span class="n">words_masks</span></div>

</span><span data-line="176">
</span><span data-line="177">
<div class="viewcode-block" id="make_mapping">
<a class="viewcode-back" href="../../../api/gliner.data_processing.utils.html#gliner.data_processing.utils.make_mapping">[docs]</a>
</span><span data-line="178"><span class="k">def</span><span class="w"> </span><span class="nf">make_mapping</span><span class="p">(</span><span class="n">types</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
</span><span data-line="179"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Create bidirectional mappings between type labels and integer IDs.</span>
</span><span data-line="180">
</span><span data-line="181"><span class="sd">    Generates forward and reverse dictionaries for converting between string</span>
</span><span data-line="182"><span class="sd">    labels (e.g., entity or relation types) and integer IDs used in model training.</span>
</span><span data-line="183"><span class="sd">    Duplicate types are removed while preserving the order of first occurrence.</span>
</span><span data-line="184"><span class="sd">    IDs start from 1 (reserving 0 for no-label/padding).</span>
</span><span data-line="185">
</span><span data-line="186"><span class="sd">    Args:</span>
</span><span data-line="187"><span class="sd">        types: List of type label strings. May contain duplicates, which will</span>
</span><span data-line="188"><span class="sd">            be removed while preserving order.</span>
</span><span data-line="189">
</span><span data-line="190"><span class="sd">    Returns:</span>
</span><span data-line="191"><span class="sd">        Tuple containing:</span>
</span><span data-line="192"><span class="sd">            - Forward mapping (Dict[str, int]): Maps type labels to integer IDs</span>
</span><span data-line="193"><span class="sd">              starting from 1</span>
</span><span data-line="194"><span class="sd">            - Reverse mapping (Dict[int, str]): Maps integer IDs back to type labels</span>
</span><span data-line="195">
</span><span data-line="196"><span class="sd">    Example:</span>
</span><span data-line="197"><span class="sd">        &gt;&gt;&gt; types = [&quot;PERSON&quot;, &quot;ORG&quot;, &quot;LOC&quot;, &quot;PERSON&quot;]  # &quot;PERSON&quot; duplicated</span>
</span><span data-line="198"><span class="sd">        &gt;&gt;&gt; fwd, rev = make_mapping(types)</span>
</span><span data-line="199"><span class="sd">        &gt;&gt;&gt; fwd</span>
</span><span data-line="200"><span class="sd">        {&#39;PERSON&#39;: 1, &#39;ORG&#39;: 2, &#39;LOC&#39;: 3}</span>
</span><span data-line="201"><span class="sd">        &gt;&gt;&gt; rev</span>
</span><span data-line="202"><span class="sd">        {1: &#39;PERSON&#39;, 2: &#39;ORG&#39;, 3: &#39;LOC&#39;}</span>
</span><span data-line="203"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="204">    <span class="c1"># de-duplicate while preserving order</span>
</span><span data-line="205">    <span class="n">uniq</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="n">types</span><span class="p">))</span>
</span><span data-line="206">    <span class="n">fwd</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">uniq</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">)}</span>
</span><span data-line="207">    <span class="n">rev</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">fwd</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</span><span data-line="208">    <span class="k">return</span> <span class="n">fwd</span><span class="p">,</span> <span class="n">rev</span></div>

</span><span data-line="209">
</span><span data-line="210">
<div class="viewcode-block" id="prepare_span_idx">
<a class="viewcode-back" href="../../../api/gliner.data_processing.utils.html#gliner.data_processing.utils.prepare_span_idx">[docs]</a>
</span><span data-line="211"><span class="k">def</span><span class="w"> </span><span class="nf">prepare_span_idx</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">,</span> <span class="n">max_width</span><span class="p">):</span>
</span><span data-line="212"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate all possible span indices for a sequence.</span>
</span><span data-line="213">
</span><span data-line="214"><span class="sd">    Creates a list of all possible (start, end) span pairs for a sequence,</span>
</span><span data-line="215"><span class="sd">    where each span has a width (end - start) less than max_width. This is used</span>
</span><span data-line="216"><span class="sd">    in span-based NER models that enumerate and classify all possible spans.</span>
</span><span data-line="217">
</span><span data-line="218"><span class="sd">    The spans follow these conventions:</span>
</span><span data-line="219"><span class="sd">    - Start index is inclusive</span>
</span><span data-line="220"><span class="sd">    - End index is inclusive (so span (i, i) is a single token)</span>
</span><span data-line="221"><span class="sd">    - Spans are generated in left-to-right order, with shorter spans first</span>
</span><span data-line="222"><span class="sd">      for each starting position</span>
</span><span data-line="223">
</span><span data-line="224"><span class="sd">    Args:</span>
</span><span data-line="225"><span class="sd">        num_tokens: Length of the sequence (number of tokens).</span>
</span><span data-line="226"><span class="sd">        max_width: Maximum span width to generate. A span of width w covers</span>
</span><span data-line="227"><span class="sd">            w+1 tokens (e.g., width 0 is a single token).</span>
</span><span data-line="228">
</span><span data-line="229"><span class="sd">    Returns:</span>
</span><span data-line="230"><span class="sd">        List of (start, end) tuples representing all valid spans. Each tuple</span>
</span><span data-line="231"><span class="sd">        contains:</span>
</span><span data-line="232"><span class="sd">            - start: Starting token index (0-indexed, inclusive)</span>
</span><span data-line="233"><span class="sd">            - end: Ending token index (0-indexed, inclusive)</span>
</span><span data-line="234">
</span><span data-line="235"><span class="sd">    Example:</span>
</span><span data-line="236"><span class="sd">        &gt;&gt;&gt; spans = prepare_span_idx(num_tokens=3, max_width=2)</span>
</span><span data-line="237"><span class="sd">        &gt;&gt;&gt; spans</span>
</span><span data-line="238"><span class="sd">        [(0, 0), (0, 1), (1, 1), (1, 2), (2, 2), (2, 3)]</span>
</span><span data-line="239"><span class="sd">        &gt;&gt;&gt; # For sequence [&quot;The&quot;, &quot;cat&quot;, &quot;sat&quot;]:</span>
</span><span data-line="240"><span class="sd">        &gt;&gt;&gt; # (0, 0) = &quot;The&quot;</span>
</span><span data-line="241"><span class="sd">        &gt;&gt;&gt; # (0, 1) = &quot;The cat&quot;</span>
</span><span data-line="242"><span class="sd">        &gt;&gt;&gt; # (1, 1) = &quot;cat&quot;</span>
</span><span data-line="243"><span class="sd">        &gt;&gt;&gt; # (1, 2) = &quot;cat sat&quot;</span>
</span><span data-line="244"><span class="sd">        &gt;&gt;&gt; # (2, 2) = &quot;sat&quot;</span>
</span><span data-line="245"><span class="sd">        &gt;&gt;&gt; # (2, 3) would be invalid (beyond sequence length)</span>
</span><span data-line="246"><span class="sd">    &quot;&quot;&quot;</span>
</span><span data-line="247">    <span class="n">span_idx</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_width</span><span class="p">)]</span>
</span><span data-line="248">    <span class="k">return</span> <span class="n">span_idx</span></div>

</span></pre></div>
        </article><button class="back-to-top" type="button">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
  </svg>
  <span>Back to top</span>
</button><div class="navigation flex print:hidden"></div></div>
    </div>
  </main>
</div>
<footer class="sy-foot">
  <div class="sy-foot-inner sy-container mx-auto">
    <div class="sy-foot-reserved md:flex justify-between items-center">
      <div class="sy-foot-copyright"><p>2025, GLiNER community</p>
  
  <p>
    Made with
    
    <a href="https://www.sphinx-doc.org/">Sphinx</a> and
    
    <a href="https://shibuya.lepture.com">Shibuya theme</a>.
  </p>
</div>
      <div class="sy-foot-socials"></div>
    </div>
  </div>
</footer>
      <script src="../../../_static/documentation_options.js?v=dc91f075"></script>
      <script src="../../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../_static/shibuya.js?v=9b0e4dde"></script></body>
</html>